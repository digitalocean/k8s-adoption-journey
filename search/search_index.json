{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hands-on tutorial for going from day-1 to production on DigitalOcean Kubernetes . Abstract The decision to adopt Kubernetes shouldn\u2019t be taken lightly. It\u2019s a complex system that requires hard work and dedication. In fact, when it comes to using Kubernetes, the best value comes from adopting a DevOps culture. Kubernetes isn\u2019t just installing something on a virtual machine (VM), it\u2019s a transformative way of viewing IT and enabling your teams. Automation is a direct path to becoming more productive. Kubernetes can make every aspect of your applications and tools accessible to automation, including role-based access controls, ports, databases, storage, networking, container builds, security scanning, and more. Kubernetes gives teams a well-documented and proven system that they can use to automate so that they can focus on outcomes. Startups and small-to-medium-sized businesses (SMBs) are uniquely positioned in their journeys. Unlike enterprises, they typically don\u2019t have dedicated departments for operations or security, and they usually run lean development teams. At the same time, SMBs are ultra-efficient, sometimes going from discovery phases to production in a matter of two or three weeks. When startups and SMBs consider adopting Kubernetes, they need to retain their ability to be flexible and pivot quickly while benefiting from the scalability and automation provided by Kubernetes. To achieve both without large and ongoing maintenance overhead, SMBs may consider the hands-free operations provided by Managed Kubernetes offerings. In this guide, you\u2019ll find recommendations for SMBs at any stage of the Kubernetes adoption journey. From the incipient stages of discovery and development through staging, production, and ultimately scaling applications, we\u2019ll offer a simple yet comprehensive approach to getting the most out of Kubernetes. Guide Overview This guide will teach you how to: Install and configure required tools to build applications running on DigitalOcean Kubernetes . Set up a DigitalOcean Container Registry for storing application images. Deploy the online boutique sample application consisting of several microservices, used as a demonstration through this guide. Perform local development for the online boutique sample application using Tilt . Set up a DigitalOcean Kubernetes cluster to perform remote development using Tilt. Set up staging and production environments to promote the online boutique sample application to upper environments. Set up CI/CD workflows using GitHub actions to build and deploy the online boutique sample application to each environment after a code change (or PR merge). Set up observability for each environment - logging, monitoring, alerting, etc. Automatically scale application workloads on demand. Please proceed with the Getting started -> Installing required tools section.","title":"Intro"},{"location":"#abstract","text":"The decision to adopt Kubernetes shouldn\u2019t be taken lightly. It\u2019s a complex system that requires hard work and dedication. In fact, when it comes to using Kubernetes, the best value comes from adopting a DevOps culture. Kubernetes isn\u2019t just installing something on a virtual machine (VM), it\u2019s a transformative way of viewing IT and enabling your teams. Automation is a direct path to becoming more productive. Kubernetes can make every aspect of your applications and tools accessible to automation, including role-based access controls, ports, databases, storage, networking, container builds, security scanning, and more. Kubernetes gives teams a well-documented and proven system that they can use to automate so that they can focus on outcomes. Startups and small-to-medium-sized businesses (SMBs) are uniquely positioned in their journeys. Unlike enterprises, they typically don\u2019t have dedicated departments for operations or security, and they usually run lean development teams. At the same time, SMBs are ultra-efficient, sometimes going from discovery phases to production in a matter of two or three weeks. When startups and SMBs consider adopting Kubernetes, they need to retain their ability to be flexible and pivot quickly while benefiting from the scalability and automation provided by Kubernetes. To achieve both without large and ongoing maintenance overhead, SMBs may consider the hands-free operations provided by Managed Kubernetes offerings. In this guide, you\u2019ll find recommendations for SMBs at any stage of the Kubernetes adoption journey. From the incipient stages of discovery and development through staging, production, and ultimately scaling applications, we\u2019ll offer a simple yet comprehensive approach to getting the most out of Kubernetes.","title":"Abstract"},{"location":"#guide-overview","text":"This guide will teach you how to: Install and configure required tools to build applications running on DigitalOcean Kubernetes . Set up a DigitalOcean Container Registry for storing application images. Deploy the online boutique sample application consisting of several microservices, used as a demonstration through this guide. Perform local development for the online boutique sample application using Tilt . Set up a DigitalOcean Kubernetes cluster to perform remote development using Tilt. Set up staging and production environments to promote the online boutique sample application to upper environments. Set up CI/CD workflows using GitHub actions to build and deploy the online boutique sample application to each environment after a code change (or PR merge). Set up observability for each environment - logging, monitoring, alerting, etc. Automatically scale application workloads on demand. Please proceed with the Getting started -> Installing required tools section.","title":"Guide Overview"},{"location":"01-getting-started/do-api-auth/","text":"Introduction This section will show you how to authenticate with the DigitalOcean API . To use the API, you\u2019ll first generate a personal access token. A personal access token allows you to use automation tools such as doctl to create and manage various DigitalOcean cloud resources, such as Kubernetes Clusters , Droplets , Container Registries , etc. Prerequisites To complete this section, you will need: A DigitalOcean account for accessing the DigitalOcean platform. A DigitalOcean personal access token for using the DigitalOcean API. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Authenticating with the DigitalOcean API First you will need to initialze doctl by running the following command: doctl auth init After entering your token doctl should be able to validate your token. Test to ensure that your account is configured for doctl to use: doctl auth list You should see a line containing your account and the \"current\" string next to it. For more info on this topic please see this Kubernetes Starter Kit Authentication Section . Next, you will learn how to create a DigitalOcean Container Registry to store all microservices Docker images used in this guide for demonstration.","title":"Authenticating with the DigitalOcean API"},{"location":"01-getting-started/do-api-auth/#introduction","text":"This section will show you how to authenticate with the DigitalOcean API . To use the API, you\u2019ll first generate a personal access token. A personal access token allows you to use automation tools such as doctl to create and manage various DigitalOcean cloud resources, such as Kubernetes Clusters , Droplets , Container Registries , etc.","title":"Introduction"},{"location":"01-getting-started/do-api-auth/#prerequisites","text":"To complete this section, you will need: A DigitalOcean account for accessing the DigitalOcean platform. A DigitalOcean personal access token for using the DigitalOcean API. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section.","title":"Prerequisites"},{"location":"01-getting-started/do-api-auth/#authenticating-with-the-digitalocean-api","text":"First you will need to initialze doctl by running the following command: doctl auth init After entering your token doctl should be able to validate your token. Test to ensure that your account is configured for doctl to use: doctl auth list You should see a line containing your account and the \"current\" string next to it. For more info on this topic please see this Kubernetes Starter Kit Authentication Section . Next, you will learn how to create a DigitalOcean Container Registry to store all microservices Docker images used in this guide for demonstration.","title":"Authenticating with the DigitalOcean API"},{"location":"01-getting-started/installing-required-tools/","text":"Introduction This section will show you how to install the required tools needed to complete this guide. You will focus on most popular tools used in the Kubernetes world. You will also install additional programs used for interacting with the DigitalOcean API, and local development enablement. Below is a complete list of the tools used in this guide: Kubectl - this the official Kubernetes client. Allows you to interact with the Kubernetes API, and to run commands against Kubernetes clusters. Helm - this is the package manager for Kubernetes. Behaves the same way as package managers used in Linux distributions, but for Kubernetes. Gained a lot of popularity, and it is a widely adopted solution for managing software packages installation and upgrade in Kubernetes. Doctl - allows you to interact with the DigitalOcean API via the command line. It supports most functionality found in the control panel. You can create, configure, and destroy DigitalOcean resources like Droplets, Kubernetes clusters, firewalls, load balancers, database clusters, domains, and more. Docker Desktop - enables you to build and share containerized applications and microservices using Docker. It has a GUI interface, and bundles a ready to run Kubernetes cluster to use for local development. Kustomize - Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. Tilt - eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date. Prerequisites To complete this section, you will need: Homebrew if you are using a macOS system. Curl package installed on your system. Installing Docker Desktop Depending on your operating system, you can install docker-desktop in the following ways: MacOS Linux - Ubuntu Install docker-desktop using Homebrew: brew install --cask docker Test to ensure you installed the latest version: docker version Update apt package index: sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg Set up the repository: echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install docker desktop: sudo apt install docker-desktop Test to ensure you installed the latest version: docker version Installing Doctl Depending on your operating system, you can install doctl in the following ways: MacOS Linux Install doctl using Homebrew: brew install doctl Test to ensure you installed the latest version: doctl version Download the latest doctl package (check releases page): curl -LO https://github.com/digitalocean/doctl/releases/download/v1.79.0/doctl-1.79.0-linux-amd64.tar.gz Extract the doctl package: tar xf doctl-1.79.0-linux-amd64.tar.gz Set the executable flag, and make the doctl binary available in your path: chmod +x doctl sudo mv doctl /usr/local/bin Test to ensure you installed the latest version: doctl version Installing Kubectl Depending on your operating system, you can install kubectl in the following ways: MacOS Linux Install kubectl using Homebrew: brew install kubectl Test to ensure you installed the latest version: kubectl version --client Download the latest kubectl release: curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" Set the executable flag, and make the kubectl binary available in your path: chmod +x kubectl sudo mv kubectl /usr/local/bin Test to ensure you installed the latest version: kubectl version --client Installing Helm Depending on your operating system, you can install helm in the following ways: MacOS Linux Install helm using Homebrew: brew install helm Test to ensure you installed the latest version: helm version Download the latest helm release: curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Test to ensure you installed the latest version: helm version Installing Kustomize Depending on your operating system, you can install kustomize in the following ways: MacOS Linux Install kustomize using Homebrew: brew install kustomize Test to ensure you installed the latest version: kustomize version Install kustomize by running the following script: curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash Test to ensure you installed the latest version: kustomize version Installing Tilt Depending on your operating system, you can install Tilt in the following ways: MacOS Linux Install tilt using curl: brew install tilt Test to ensure you installed the latest version: tilt version Install tilt using curl: curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh | bash Test to ensure you installed the latest version: tilt version Installing Cloud Native Buildpacks's CLI (Optional) Note Installing the pack CLI is only needed if you wish to build and push the docker images of the microservices-demo app using Cloud Native Buildpacks as explained in preparing the demo application section. Depending on your operating system, you can install pack in the following ways: MacOS Linux Install pack using homebrew: brew install buildpacks/tap/pack Test to ensure you installed the latest version: pack version Install pack using curl: sudo add-apt-repository ppa:cncf-buildpacks/pack-cli sudo apt-get update sudo apt-get install pack-cli Test to ensure you installed the latest version: pack version Next, you will learn how to authenticate with the DigitalOcean API to get the most out of the tools used in this guide to provision required cloud resources.","title":"Installing required tools"},{"location":"01-getting-started/installing-required-tools/#introduction","text":"This section will show you how to install the required tools needed to complete this guide. You will focus on most popular tools used in the Kubernetes world. You will also install additional programs used for interacting with the DigitalOcean API, and local development enablement. Below is a complete list of the tools used in this guide: Kubectl - this the official Kubernetes client. Allows you to interact with the Kubernetes API, and to run commands against Kubernetes clusters. Helm - this is the package manager for Kubernetes. Behaves the same way as package managers used in Linux distributions, but for Kubernetes. Gained a lot of popularity, and it is a widely adopted solution for managing software packages installation and upgrade in Kubernetes. Doctl - allows you to interact with the DigitalOcean API via the command line. It supports most functionality found in the control panel. You can create, configure, and destroy DigitalOcean resources like Droplets, Kubernetes clusters, firewalls, load balancers, database clusters, domains, and more. Docker Desktop - enables you to build and share containerized applications and microservices using Docker. It has a GUI interface, and bundles a ready to run Kubernetes cluster to use for local development. Kustomize - Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is. Tilt - eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date.","title":"Introduction"},{"location":"01-getting-started/installing-required-tools/#prerequisites","text":"To complete this section, you will need: Homebrew if you are using a macOS system. Curl package installed on your system.","title":"Prerequisites"},{"location":"01-getting-started/installing-required-tools/#installing-docker-desktop","text":"Depending on your operating system, you can install docker-desktop in the following ways: MacOS Linux - Ubuntu Install docker-desktop using Homebrew: brew install --cask docker Test to ensure you installed the latest version: docker version Update apt package index: sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg Set up the repository: echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install docker desktop: sudo apt install docker-desktop Test to ensure you installed the latest version: docker version","title":"Installing Docker Desktop"},{"location":"01-getting-started/installing-required-tools/#installing-doctl","text":"Depending on your operating system, you can install doctl in the following ways: MacOS Linux Install doctl using Homebrew: brew install doctl Test to ensure you installed the latest version: doctl version Download the latest doctl package (check releases page): curl -LO https://github.com/digitalocean/doctl/releases/download/v1.79.0/doctl-1.79.0-linux-amd64.tar.gz Extract the doctl package: tar xf doctl-1.79.0-linux-amd64.tar.gz Set the executable flag, and make the doctl binary available in your path: chmod +x doctl sudo mv doctl /usr/local/bin Test to ensure you installed the latest version: doctl version","title":"Installing Doctl"},{"location":"01-getting-started/installing-required-tools/#installing-kubectl","text":"Depending on your operating system, you can install kubectl in the following ways: MacOS Linux Install kubectl using Homebrew: brew install kubectl Test to ensure you installed the latest version: kubectl version --client Download the latest kubectl release: curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" Set the executable flag, and make the kubectl binary available in your path: chmod +x kubectl sudo mv kubectl /usr/local/bin Test to ensure you installed the latest version: kubectl version --client","title":"Installing Kubectl"},{"location":"01-getting-started/installing-required-tools/#installing-helm","text":"Depending on your operating system, you can install helm in the following ways: MacOS Linux Install helm using Homebrew: brew install helm Test to ensure you installed the latest version: helm version Download the latest helm release: curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Test to ensure you installed the latest version: helm version","title":"Installing Helm"},{"location":"01-getting-started/installing-required-tools/#installing-kustomize","text":"Depending on your operating system, you can install kustomize in the following ways: MacOS Linux Install kustomize using Homebrew: brew install kustomize Test to ensure you installed the latest version: kustomize version Install kustomize by running the following script: curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | bash Test to ensure you installed the latest version: kustomize version","title":"Installing Kustomize"},{"location":"01-getting-started/installing-required-tools/#installing-tilt","text":"Depending on your operating system, you can install Tilt in the following ways: MacOS Linux Install tilt using curl: brew install tilt Test to ensure you installed the latest version: tilt version Install tilt using curl: curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh | bash Test to ensure you installed the latest version: tilt version","title":"Installing Tilt"},{"location":"01-getting-started/installing-required-tools/#installing-cloud-native-buildpackss-cli-optional","text":"Note Installing the pack CLI is only needed if you wish to build and push the docker images of the microservices-demo app using Cloud Native Buildpacks as explained in preparing the demo application section. Depending on your operating system, you can install pack in the following ways: MacOS Linux Install pack using homebrew: brew install buildpacks/tap/pack Test to ensure you installed the latest version: pack version Install pack using curl: sudo add-apt-repository ppa:cncf-buildpacks/pack-cli sudo apt-get update sudo apt-get install pack-cli Test to ensure you installed the latest version: pack version Next, you will learn how to authenticate with the DigitalOcean API to get the most out of the tools used in this guide to provision required cloud resources.","title":"Installing Cloud Native Buildpacks's CLI (Optional)"},{"location":"01-getting-started/setup-docr/","text":"Introduction In this section you will learn you how to create a DigitalOcean Container Registry ( DOCR ) registry using doctl . A docker container registry is required to store all microservices-demo app images used in this guide. For each microservice you need a separate repository in the registry. The microservices-demo app consists of nine microservice , hence a total of nine Docker repositories will be created in the registry. Prerequisites To complete this section you will need: Doctl utility already installed, as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section. Provisioning a DigitalOcean Container Registry for Microservices Development Following command will create a professional tier Docker registry in the nyc3 region, named microservices-demo : doctl registry create microservices-demo \\ --subscription-tier professional \\ --region nyc3 Note The professional tier is required to store all docker images used in this guide which costs 20$/month . Above example is using microservices-demo as the registry name. Don't forget to adjust according to your setup. It is recommended to use a region for your registry that is closest to you for faster image download/upload operations. Run the following command - doctl registry options available-regions to check available regions. Next, you will prepare a personal GitHub repository to store all required assets for the microservices-demo application, as well as the initial configuration to make it work.","title":"Set up a Digital Ocean container registry"},{"location":"01-getting-started/setup-docr/#introduction","text":"In this section you will learn you how to create a DigitalOcean Container Registry ( DOCR ) registry using doctl . A docker container registry is required to store all microservices-demo app images used in this guide. For each microservice you need a separate repository in the registry. The microservices-demo app consists of nine microservice , hence a total of nine Docker repositories will be created in the registry.","title":"Introduction"},{"location":"01-getting-started/setup-docr/#prerequisites","text":"To complete this section you will need: Doctl utility already installed, as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section.","title":"Prerequisites"},{"location":"01-getting-started/setup-docr/#provisioning-a-digitalocean-container-registry-for-microservices-development","text":"Following command will create a professional tier Docker registry in the nyc3 region, named microservices-demo : doctl registry create microservices-demo \\ --subscription-tier professional \\ --region nyc3 Note The professional tier is required to store all docker images used in this guide which costs 20$/month . Above example is using microservices-demo as the registry name. Don't forget to adjust according to your setup. It is recommended to use a region for your registry that is closest to you for faster image download/upload operations. Run the following command - doctl registry options available-regions to check available regions. Next, you will prepare a personal GitHub repository to store all required assets for the microservices-demo application, as well as the initial configuration to make it work.","title":"Provisioning a DigitalOcean Container Registry for Microservices Development"},{"location":"01-getting-started/building-and-pushing-docker-images/building-and-pushing-images-using-cnb/","text":"Introduction Cloud Native Buildpacks transform your application source code into images that can run on any cloud. In this section you'll learn the basics of using buildpacks and create the images required for the microservices-demo project. Buildpacks allow you to convert your source code into a secure, efficient, production ready container image without the need to write a Dockerfile so you can focus all your attention on writing the application code. They provide framework and runtime support for your applications. Buildpacks examine your apps to determine all dependencies it needs and configures them appropriately to run on any cloud. Each buildpack is comprised of two phases: The detect phase which runs against your source code to determine if the buildpack is applicable or not. Once a buildpack is detected to be applicable, it proceeds to the build stage. Detection criteria is specific to each buildpack \u2013 for instance, an NPM buildpack might look for a package.json, and a Go buildpack might look for Go source files. The build phase runs against your source code to set up the build-time and run-time environment, download dependencies and compile your source code (if needed) and set appropriate entry point and startup scripts. Builders are an ordered combination of buildpacks with a base build image, a lifecycle, and reference to a run image. They take in your app source code and build the output app image. The build image provides the base environment for the builder (for eg. an Ubuntu Bionic OS image with build tooling) and a run image provides the base environment for the app image during runtime. A combination of a build image and a run image is called a stack . Build and Push Online Boutique Application Images In this section you will build and push required images for the online boutique demo project using pack CLI, a tool maintained by the Cloud Native Buildpacks project to support the use of buildpacks. A helper script is used to ease the process, named make-cnb-docker-images.sh . Note Cloud Native Buildpacks images work best when run on an x86 architecture workstation. ARM is not 100% supported at this time. To be able to use pack you will need to install it as explained in the Installing Pack . Follow below steps to build and push online boutique demo application images using pack CLI: Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git From the command line, change directory to the microservices-demo folder (if not there already): cd microservices-demo Login to DOCR: doctl registry login Run the make-cnb-docker-images.sh script after setting required environment variables first: export REPO_PREFIX = \"registry.digitalocean.com/microservices-demo\" export TAG = \"v1.0.0\" ./release-scripts/cnb-docker-images.sh Info This script will go through each of the microservices and perform a pack build and publish , tagging each image with the service name and the TAG environment variable exported above. Using Cloud Native Buildpacks means that even if there are no Dockerfiles present, pack will be able to detect which buildpack to use and proceed to the build step. You will be pushing an initial release first to DOCR - v1.0.0 , and use that to deploy to the staging and production environments in the upcoming sections. Later on, GitHub Actions will take care of building, tagging and pushing images to DOCR . Tip If you need to run one of the Python microservices (recommendationservice or emailservice) with a different runtime than what the builder is offering you can change the python runtime in the Python version file. The entrypoint needs to be explicitly set for Python microservices when using Cloud Native Buildpacks. This is set in the Procfile . Any changes to the entrypoints of Python microservices need to be set in the Procfile . Next, you will learn how to setup DOKS and deploy the microservices-demo application to your development environment, as well as configuring ingress and monitoring .","title":"Building and pushing images with Cloud Native Buildpacks"},{"location":"01-getting-started/building-and-pushing-docker-images/building-and-pushing-images-using-cnb/#introduction","text":"Cloud Native Buildpacks transform your application source code into images that can run on any cloud. In this section you'll learn the basics of using buildpacks and create the images required for the microservices-demo project. Buildpacks allow you to convert your source code into a secure, efficient, production ready container image without the need to write a Dockerfile so you can focus all your attention on writing the application code. They provide framework and runtime support for your applications. Buildpacks examine your apps to determine all dependencies it needs and configures them appropriately to run on any cloud. Each buildpack is comprised of two phases: The detect phase which runs against your source code to determine if the buildpack is applicable or not. Once a buildpack is detected to be applicable, it proceeds to the build stage. Detection criteria is specific to each buildpack \u2013 for instance, an NPM buildpack might look for a package.json, and a Go buildpack might look for Go source files. The build phase runs against your source code to set up the build-time and run-time environment, download dependencies and compile your source code (if needed) and set appropriate entry point and startup scripts. Builders are an ordered combination of buildpacks with a base build image, a lifecycle, and reference to a run image. They take in your app source code and build the output app image. The build image provides the base environment for the builder (for eg. an Ubuntu Bionic OS image with build tooling) and a run image provides the base environment for the app image during runtime. A combination of a build image and a run image is called a stack .","title":"Introduction"},{"location":"01-getting-started/building-and-pushing-docker-images/building-and-pushing-images-using-cnb/#build-and-push-online-boutique-application-images","text":"In this section you will build and push required images for the online boutique demo project using pack CLI, a tool maintained by the Cloud Native Buildpacks project to support the use of buildpacks. A helper script is used to ease the process, named make-cnb-docker-images.sh . Note Cloud Native Buildpacks images work best when run on an x86 architecture workstation. ARM is not 100% supported at this time. To be able to use pack you will need to install it as explained in the Installing Pack . Follow below steps to build and push online boutique demo application images using pack CLI: Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git From the command line, change directory to the microservices-demo folder (if not there already): cd microservices-demo Login to DOCR: doctl registry login Run the make-cnb-docker-images.sh script after setting required environment variables first: export REPO_PREFIX = \"registry.digitalocean.com/microservices-demo\" export TAG = \"v1.0.0\" ./release-scripts/cnb-docker-images.sh Info This script will go through each of the microservices and perform a pack build and publish , tagging each image with the service name and the TAG environment variable exported above. Using Cloud Native Buildpacks means that even if there are no Dockerfiles present, pack will be able to detect which buildpack to use and proceed to the build step. You will be pushing an initial release first to DOCR - v1.0.0 , and use that to deploy to the staging and production environments in the upcoming sections. Later on, GitHub Actions will take care of building, tagging and pushing images to DOCR . Tip If you need to run one of the Python microservices (recommendationservice or emailservice) with a different runtime than what the builder is offering you can change the python runtime in the Python version file. The entrypoint needs to be explicitly set for Python microservices when using Cloud Native Buildpacks. This is set in the Procfile . Any changes to the entrypoints of Python microservices need to be set in the Procfile . Next, you will learn how to setup DOKS and deploy the microservices-demo application to your development environment, as well as configuring ingress and monitoring .","title":"Build and Push Online Boutique Application Images"},{"location":"01-getting-started/building-and-pushing-docker-images/building-and-pushing-images-using-docker/","text":"Introduction Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you manage your infrastructure in the same way you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you significantly reduce the delay between writing code and running in production. Build and Push Online Boutique Application Images In this section you will build and push to DOCR all required images for the online boutique demo project using docker CLI. A helper script is used to ease the process, named make-docker-images.sh . Follow below steps to build and push online boutique demo application images using docker CLI: Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git From the command line, change directory to the microservices-demo folder (if not there already): cd microservices-demo Login to DOCR: doctl registry login Run the make-docker-images.sh script after setting required environment variables first: export REPO_PREFIX = \"registry.digitalocean.com/microservices-demo\" export TAG = \"v1.0.0\" ./release-scripts/make-docker-images.sh Info This script will go through each of the microservices and perform a docker build and a docker push tagging each image with the service name and the TAG environment variable exported above. You will be pushing an initial release first to DOCR - v1.0.0 , and use that to deploy to the staging and production environments in the upcoming sections. Later on, GitHub Actions will take care of building, tagging and pushing images to DOCR . Next, you have the option to study Cloud Native Buildpacks project to build and push docker images without having to write a single Dockerfile. If not, skip to the Development Environment section where you will learn how to setup DOKS and deploy the microservices-demo application to your development environment, as well as configuring ingress and monitoring .","title":"Building and pushing images with Docker"},{"location":"01-getting-started/building-and-pushing-docker-images/building-and-pushing-images-using-docker/#introduction","text":"Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you manage your infrastructure in the same way you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you significantly reduce the delay between writing code and running in production.","title":"Introduction"},{"location":"01-getting-started/building-and-pushing-docker-images/building-and-pushing-images-using-docker/#build-and-push-online-boutique-application-images","text":"In this section you will build and push to DOCR all required images for the online boutique demo project using docker CLI. A helper script is used to ease the process, named make-docker-images.sh . Follow below steps to build and push online boutique demo application images using docker CLI: Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git From the command line, change directory to the microservices-demo folder (if not there already): cd microservices-demo Login to DOCR: doctl registry login Run the make-docker-images.sh script after setting required environment variables first: export REPO_PREFIX = \"registry.digitalocean.com/microservices-demo\" export TAG = \"v1.0.0\" ./release-scripts/make-docker-images.sh Info This script will go through each of the microservices and perform a docker build and a docker push tagging each image with the service name and the TAG environment variable exported above. You will be pushing an initial release first to DOCR - v1.0.0 , and use that to deploy to the staging and production environments in the upcoming sections. Later on, GitHub Actions will take care of building, tagging and pushing images to DOCR . Next, you have the option to study Cloud Native Buildpacks project to build and push docker images without having to write a single Dockerfile. If not, skip to the Development Environment section where you will learn how to setup DOKS and deploy the microservices-demo application to your development environment, as well as configuring ingress and monitoring .","title":"Build and Push Online Boutique Application Images"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/","text":"Introduction In this section you will create the GitHub repository hosting the sample application used through this guide. It's a web-based e-commerce app (online boutique) consisting of 11 microservices of which only 9 are used by this guide. In terms of functionality, users can browse items, add them to the cart, and purchase them. The online boutique application is a clone of the original GoogleCloudPlatform project. It will be used as a demo for the Kubernetes adoption journey. The microservices-demo project from the DigitalOcean kubernetes-sample-apps repository has been stripped down to focus only on the major parts required by the adoption journey. For more information and architecture details, please visit the GoogleCloudPlatform GitHub repository. Why not create a fork? There are several reasons why a fork is not appropriate to complete all steps from this guide: Each PR opened on your fork will point to the upstream repository by default. You will want to have PRs opened against your repo when testing each section of the adoption journey guide, especially in the continuous integration chapter. Full history of the upstream repo will be present in your fork as well. This can create lot of noise. All projects from the kubernetes-sample-apps upstream are pulled in your fork as well. You only care about microservices-demo. Next, you will start by creating your own GitHub repository hosting the online boutique demo application source code and configuration files. Prerequisites To complete this section you will need: A GitHub account you own. You can create one for free here . A Git client to perform operations on your GitHub repository. Usually bundled with your Linux distribution. Wget and unzip utilities. Usually bundled with your Linux distribution. A container registry already set up as explained in the Set up DOCR section. Set Up GitHub Repository for the Online Boutique Application Navigate to Github website and log in using your GitHub account. In the upper-right corner of any page, use the + drop-down menu, and select New repository . Set the Repository name to microservices-demo . Click on the Create repository button. From the command line clone the newly created repository to your local machine (make sure to replace the <> placeholders accordingly): git clone git@github.com:<YOUR_GITHUB_USERNAME>/microservices-demo.git Change directory to your local clone: cd microservices-demo Run the following command to download a zip file of the entire kubernetes-sample-apps repo: wget https://github.com/digitalocean/kubernetes-sample-apps/archive/refs/heads/master.zip -O kubernetes-sample-apps.zip Unzip the microservices-demo project folder from the kubernetes-sample-apps.zip file: unzip kubernetes-sample-apps.zip 'kubernetes-sample-apps-master/microservices-demo/*' Info This will result in a kubernetes-sample-apps-master folder being created from the unzip process. Copy the content of the microservices-demo from the kubernetes-sample-apps-master to the current working directory: cp -r kubernetes-sample-apps-master/microservices-demo/* . Remove the kubernetes-sample-apps-master and kubernetes-sample-apps.zip : rm -rf kubernetes-sample-apps-master kubernetes-sample-apps.zip Click to expand microservices-demo repository structure \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 dev \u2502 \u251c\u2500\u2500 prod \u2502 \u251c\u2500\u2500 staging \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 license_header.txt \u2502 \u251c\u2500\u2500 make-docker-images.sh \u2502 \u251c\u2500\u2500 make-release-artifacts.sh \u2502 \u2514\u2500\u2500 make-release.sh \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 CODE_OF_CONDUCT.md \u251c\u2500\u2500 CONTRIBUTING.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Commit and push all changes to main branch: git add . git commit -m \"Initial repository layout\" git push origin At this point your online boutique GitHub repository is prepared and ready to use through this guide. Next, a quick introduction is given for main project layout and important assets. Understanding Microservices Demo Project Structure It's important to familiarize with the folder structure of the e-commerce web application used in this guide. The microservices-demo repository folder layout is listed below: Click to expand microservices-demo project folder layout . \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u251c\u2500\u2500 cartservice.yaml \u2502 \u2502 \u251c\u2500\u2500 checkoutservice.yaml \u2502 \u2502 \u251c\u2500\u2500 currencyservice.yaml \u2502 \u2502 \u251c\u2500\u2500 emailservice.yaml \u2502 \u2502 \u251c\u2500\u2500 frontend.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u251c\u2500\u2500 paymentservice.yaml \u2502 \u2502 \u251c\u2500\u2500 productcatalogservice.yaml \u2502 \u2502 \u251c\u2500\u2500 recommendationservice.yaml \u2502 \u2502 \u251c\u2500\u2500 redis.yaml \u2502 \u2502 \u2514\u2500\u2500 shippingservice.yaml \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 local \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 loadgenerator \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Explanations for the above layout: kustomize - main folder containing project kustomizations. Project deployment is managed via Kustomize . Each environment is represented and managed via a Kustomize overlay folder - dev , staging , prod , etc. Overlays contain environment specific configuration over the base folder. The base contains common configuration across all environments. release-scripts - contains utility shell scripts used to create, build, tag and push project docker images. src - this is the main project folder containing source code for all application microservices. It also contains required Dockerfiles to build each component image. It is a standardized layout (except for cartservice component). You will find here each project unit tests as well (not all are implemented yet though). tilt-resources - Tilt configuration profiles for each environment supported by the sample application. Tiltfile - main project Tilt logic. You will learn more about Tilt in the development section . Configuring DOCR Endpoint for Kustomize Overlays Each overlay corresponds to an environment and it is defined by the kustomization.yaml manifest file present in the corresponding folder, such as dev , staging , prod . Base overlay contains common stuff across all environments. The following listing shows the Kustomize folder layout used in this guide: . \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u251c\u2500\u2500 cartservice.yaml \u2502 \u2502 \u251c\u2500\u2500 checkoutservice.yaml \u2502 \u2502 \u251c\u2500\u2500 currencyservice.yaml \u2502 \u2502 \u251c\u2500\u2500 emailservice.yaml \u2502 \u2502 \u251c\u2500\u2500 frontend.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u251c\u2500\u2500 paymentservice.yaml \u2502 \u2502 \u251c\u2500\u2500 productcatalogservice.yaml \u2502 \u2502 \u251c\u2500\u2500 recommendationservice.yaml \u2502 \u2502 \u251c\u2500\u2500 redis.yaml \u2502 \u2502 \u2514\u2500\u2500 shippingservice.yaml \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 local \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml ... Above listing shows all kustomization.yaml manifests present in each environment subfolder (or overlay). The kustomization.yaml manifest file is important because it defines the differences across environments. Basically, it overrides or adds new settings over the existing ones present in the base subfolder. There's a kustomization.yaml manifest file located in the base subfolder as well which defines default or common settings for all project microservices. Going forward, every environment overrides each microservice Docker image used in the project to include registry information. Based on your current setup, this setting needs to be changed accordingly. Follow below steps to change DOCR settings for each environment: Development Environment DOCR Overlay Open and edit the kustomize/dev/kustomization.yaml file using an editor of your choice (preferably with YAML lint suppprt). For example, you can use VS Code : code kustomize/dev/kustomization.yaml For each application image, replace the microservices-demo string within each newName line with your own registry name: Click to expand the kustomize/dev/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.0 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.0 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.0 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.0 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.0 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.0 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.0 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.0 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.0 ... Staging Environment DOCR Overlay Open and edit the kustomize/staging/kustomization.yaml file using an editor of your choice (preferably with YAML lint suppprt). For example, you can use VS Code : code kustomize/staging/kustomization.yaml For each application image, replace the microservices-demo string within each newName line with your own registry name: Click to expand the kustomize/staging/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.0 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.0 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.0 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.0 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.0 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.0 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.0 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.0 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.0 ... Production Environment DOCR Overlay Open and edit the kustomize/production/kustomization.yaml file using an editor of your choice (preferably with YAML lint suppprt). For example, you can use VS Code : code kustomize/production/kustomization.yaml For each application image, replace the microservices-demo string within each newName line with your own registry name: Click to expand the kustomize/production/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.0 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.0 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.0 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.0 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.0 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.0 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.0 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.0 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.0 ... Next, it is important to set a few protection rules to avoid pushing directly to main branch, as well as how to enforce a set of policies related to how code is merged. Set Up Main Branch Protection You should define branch protection rules to disable force pushing, prevent branches from being deleted, and optionally require status checks before merging. From GitHub , navigate to the main page of your repository. Under your repository name, click Settings . In the Code and automation section of the sidebar, click Branches . Next to Branch protection rules , click Add rule. Set the Branch name pattern to master : Tick the following options: Require a pull request before merging. Dismiss stale pull request approvals when new commits are pushed. The online boutique demo application uses Docker technology to distribute and run all application components. Next, you have the option to use a classic docker build to distribute the application, or use Cloud Native Buildpacks . Cloud Native Buildpacks simplify the process of distributing Docker applications without having to write a single Dockerfile.","title":"Repository setup"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#introduction","text":"In this section you will create the GitHub repository hosting the sample application used through this guide. It's a web-based e-commerce app (online boutique) consisting of 11 microservices of which only 9 are used by this guide. In terms of functionality, users can browse items, add them to the cart, and purchase them. The online boutique application is a clone of the original GoogleCloudPlatform project. It will be used as a demo for the Kubernetes adoption journey. The microservices-demo project from the DigitalOcean kubernetes-sample-apps repository has been stripped down to focus only on the major parts required by the adoption journey. For more information and architecture details, please visit the GoogleCloudPlatform GitHub repository. Why not create a fork? There are several reasons why a fork is not appropriate to complete all steps from this guide: Each PR opened on your fork will point to the upstream repository by default. You will want to have PRs opened against your repo when testing each section of the adoption journey guide, especially in the continuous integration chapter. Full history of the upstream repo will be present in your fork as well. This can create lot of noise. All projects from the kubernetes-sample-apps upstream are pulled in your fork as well. You only care about microservices-demo. Next, you will start by creating your own GitHub repository hosting the online boutique demo application source code and configuration files.","title":"Introduction"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#prerequisites","text":"To complete this section you will need: A GitHub account you own. You can create one for free here . A Git client to perform operations on your GitHub repository. Usually bundled with your Linux distribution. Wget and unzip utilities. Usually bundled with your Linux distribution. A container registry already set up as explained in the Set up DOCR section.","title":"Prerequisites"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#set-up-github-repository-for-the-online-boutique-application","text":"Navigate to Github website and log in using your GitHub account. In the upper-right corner of any page, use the + drop-down menu, and select New repository . Set the Repository name to microservices-demo . Click on the Create repository button. From the command line clone the newly created repository to your local machine (make sure to replace the <> placeholders accordingly): git clone git@github.com:<YOUR_GITHUB_USERNAME>/microservices-demo.git Change directory to your local clone: cd microservices-demo Run the following command to download a zip file of the entire kubernetes-sample-apps repo: wget https://github.com/digitalocean/kubernetes-sample-apps/archive/refs/heads/master.zip -O kubernetes-sample-apps.zip Unzip the microservices-demo project folder from the kubernetes-sample-apps.zip file: unzip kubernetes-sample-apps.zip 'kubernetes-sample-apps-master/microservices-demo/*' Info This will result in a kubernetes-sample-apps-master folder being created from the unzip process. Copy the content of the microservices-demo from the kubernetes-sample-apps-master to the current working directory: cp -r kubernetes-sample-apps-master/microservices-demo/* . Remove the kubernetes-sample-apps-master and kubernetes-sample-apps.zip : rm -rf kubernetes-sample-apps-master kubernetes-sample-apps.zip Click to expand microservices-demo repository structure \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 dev \u2502 \u251c\u2500\u2500 prod \u2502 \u251c\u2500\u2500 staging \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 license_header.txt \u2502 \u251c\u2500\u2500 make-docker-images.sh \u2502 \u251c\u2500\u2500 make-release-artifacts.sh \u2502 \u2514\u2500\u2500 make-release.sh \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 CODE_OF_CONDUCT.md \u251c\u2500\u2500 CONTRIBUTING.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Commit and push all changes to main branch: git add . git commit -m \"Initial repository layout\" git push origin At this point your online boutique GitHub repository is prepared and ready to use through this guide. Next, a quick introduction is given for main project layout and important assets.","title":"Set Up GitHub Repository for the Online Boutique Application"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#understanding-microservices-demo-project-structure","text":"It's important to familiarize with the folder structure of the e-commerce web application used in this guide. The microservices-demo repository folder layout is listed below: Click to expand microservices-demo project folder layout . \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u251c\u2500\u2500 cartservice.yaml \u2502 \u2502 \u251c\u2500\u2500 checkoutservice.yaml \u2502 \u2502 \u251c\u2500\u2500 currencyservice.yaml \u2502 \u2502 \u251c\u2500\u2500 emailservice.yaml \u2502 \u2502 \u251c\u2500\u2500 frontend.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u251c\u2500\u2500 paymentservice.yaml \u2502 \u2502 \u251c\u2500\u2500 productcatalogservice.yaml \u2502 \u2502 \u251c\u2500\u2500 recommendationservice.yaml \u2502 \u2502 \u251c\u2500\u2500 redis.yaml \u2502 \u2502 \u2514\u2500\u2500 shippingservice.yaml \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 local \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 release-scripts \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 cartservice \u2502 \u251c\u2500\u2500 checkoutservice \u2502 \u251c\u2500\u2500 currencyservice \u2502 \u251c\u2500\u2500 emailservice \u2502 \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 loadgenerator \u2502 \u251c\u2500\u2500 paymentservice \u2502 \u251c\u2500\u2500 productcatalogservice \u2502 \u251c\u2500\u2500 recommendationservice \u2502 \u2514\u2500\u2500 shippingservice \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 README.md \u2514\u2500\u2500 Tiltfile Explanations for the above layout: kustomize - main folder containing project kustomizations. Project deployment is managed via Kustomize . Each environment is represented and managed via a Kustomize overlay folder - dev , staging , prod , etc. Overlays contain environment specific configuration over the base folder. The base contains common configuration across all environments. release-scripts - contains utility shell scripts used to create, build, tag and push project docker images. src - this is the main project folder containing source code for all application microservices. It also contains required Dockerfiles to build each component image. It is a standardized layout (except for cartservice component). You will find here each project unit tests as well (not all are implemented yet though). tilt-resources - Tilt configuration profiles for each environment supported by the sample application. Tiltfile - main project Tilt logic. You will learn more about Tilt in the development section .","title":"Understanding Microservices Demo Project Structure"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#configuring-docr-endpoint-for-kustomize-overlays","text":"Each overlay corresponds to an environment and it is defined by the kustomization.yaml manifest file present in the corresponding folder, such as dev , staging , prod . Base overlay contains common stuff across all environments. The following listing shows the Kustomize folder layout used in this guide: . \u251c\u2500\u2500 kustomize \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u251c\u2500\u2500 cartservice.yaml \u2502 \u2502 \u251c\u2500\u2500 checkoutservice.yaml \u2502 \u2502 \u251c\u2500\u2500 currencyservice.yaml \u2502 \u2502 \u251c\u2500\u2500 emailservice.yaml \u2502 \u2502 \u251c\u2500\u2500 frontend.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 namespace.yaml \u2502 \u2502 \u251c\u2500\u2500 paymentservice.yaml \u2502 \u2502 \u251c\u2500\u2500 productcatalogservice.yaml \u2502 \u2502 \u251c\u2500\u2500 recommendationservice.yaml \u2502 \u2502 \u251c\u2500\u2500 redis.yaml \u2502 \u2502 \u2514\u2500\u2500 shippingservice.yaml \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 local \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 prod \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 staging \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 kustomization.yaml ... Above listing shows all kustomization.yaml manifests present in each environment subfolder (or overlay). The kustomization.yaml manifest file is important because it defines the differences across environments. Basically, it overrides or adds new settings over the existing ones present in the base subfolder. There's a kustomization.yaml manifest file located in the base subfolder as well which defines default or common settings for all project microservices. Going forward, every environment overrides each microservice Docker image used in the project to include registry information. Based on your current setup, this setting needs to be changed accordingly. Follow below steps to change DOCR settings for each environment:","title":"Configuring DOCR Endpoint for Kustomize Overlays"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#development-environment-docr-overlay","text":"Open and edit the kustomize/dev/kustomization.yaml file using an editor of your choice (preferably with YAML lint suppprt). For example, you can use VS Code : code kustomize/dev/kustomization.yaml For each application image, replace the microservices-demo string within each newName line with your own registry name: Click to expand the kustomize/dev/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.0 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.0 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.0 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.0 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.0 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.0 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.0 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.0 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.0 ...","title":"Development Environment DOCR Overlay"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#staging-environment-docr-overlay","text":"Open and edit the kustomize/staging/kustomization.yaml file using an editor of your choice (preferably with YAML lint suppprt). For example, you can use VS Code : code kustomize/staging/kustomization.yaml For each application image, replace the microservices-demo string within each newName line with your own registry name: Click to expand the kustomize/staging/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.0 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.0 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.0 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.0 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.0 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.0 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.0 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.0 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.0 ...","title":"Staging Environment DOCR Overlay"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#production-environment-docr-overlay","text":"Open and edit the kustomize/production/kustomization.yaml file using an editor of your choice (preferably with YAML lint suppprt). For example, you can use VS Code : code kustomize/production/kustomization.yaml For each application image, replace the microservices-demo string within each newName line with your own registry name: Click to expand the kustomize/production/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.0 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.0 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.0 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.0 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.0 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.0 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.0 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.0 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.0 ... Next, it is important to set a few protection rules to avoid pushing directly to main branch, as well as how to enforce a set of policies related to how code is merged.","title":"Production Environment DOCR Overlay"},{"location":"01-getting-started/building-and-pushing-docker-images/introduction-and-repository-setup/#set-up-main-branch-protection","text":"You should define branch protection rules to disable force pushing, prevent branches from being deleted, and optionally require status checks before merging. From GitHub , navigate to the main page of your repository. Under your repository name, click Settings . In the Code and automation section of the sidebar, click Branches . Next to Branch protection rules , click Add rule. Set the Branch name pattern to master : Tick the following options: Require a pull request before merging. Dismiss stale pull request approvals when new commits are pushed. The online boutique demo application uses Docker technology to distribute and run all application components. Next, you have the option to use a classic docker build to distribute the application, or use Cloud Native Buildpacks . Cloud Native Buildpacks simplify the process of distributing Docker applications without having to write a single Dockerfile.","title":"Set Up Main Branch Protection"},{"location":"02-development/observability-dev/","text":"Introduction Note You will not install a full blown monitoring and logging solution on the development cluster such as Prometheus and Loki , but will do so for the staging and production clusters. On the development environment you will be configuring the Kubernetes Metrics Server and Kubernetes Dashboard tools. This way your cluster size can be small but sufficient that you can do your local development and deploy that to a Kubernetes cluster and test it. At the most basic level you will need to be able to troubleshoot your aplication if things go wrong. In this section, you will learn about the Kubernetes Metrics Server and the Kubernetes Dashboard . Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API . Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. JQ command-line JSON processor installed on your machine. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section. Installing the Kubernetes Metrics Server In this section you will install the community maintained Kubernetes Metrics Server. Please follow below steps to install it using Helm: Add the Metrics Server Helm repository: helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ Install the Kubernetes Metrics Server using Helm : helm upgrade --install metrics-server metrics-server/metrics-server \\ --namespace metrics-server \\ --create-namespace Note To check if the installation was successful, run the helm ls -n metrics-server command, and confirm the deployment status. Collect resource metrics from Kubernetes objects Resource metrics track the utilization and availability of critical resources such as CPU, memory, and storage. Kubernetes provides a Metrics API and a number of command line queries that allow you to retrieve snapshots of resource utilization. Query the Metrics API to retrieve current metrics from any node or pod (you can find your desired node or pod by running kubectl get nodes or kubectl get pods ): kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<NODE_NAME> | jq kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/<NAMESPACE>/pods/<POD_NAME> | jq Info The Metrics API returns a JSON object, so (optionally) piping the response through jq displays the JSON in a more human-readable format. Retrieve compact metric snapshots from the Metrics API using kubectl top. The kubectl top command returns current CPU and memory usage for a cluster\u2019s pods or nodes, or for a particular pod or node if specified. kubectl top node The output should look like the following: NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% basicnp-766qo 280m 7% 1662Mi 24% basicnp-766qr 252m 6% 1561Mi 23% Or you can query resource utilization by pod in a particular namespace (you can use the microservices-demo-dev namespace you deployed in the Tilt Remote section): kubectl top pod --namespace microservices-demo-dev The output should look like the following: NAME CPU(cores) MEMORY(bytes) cartservice-84758f76f-cl9vm 8m 29Mi checkoutservice-5d8d8cfd5f-8hbjn 3m 16Mi currencyservice-5d5f698f87-kh4z8 4m 30Mi emailservice-f8795cc94-r7hwp 30m 40Mi frontend-6d45d8cc5d-59fmw 1m 20Mi paymentservice-995d69494-kcqn2 5m 30Mi productcatalogservice-556d4f9446-7sqp9 7m 16Mi recommendationservice-59f78c445b-5487v 40m 40Mi redis-cart-596c7658c4-lwf8g 3m 7Mi shippingservice-bfc488696-dkcpz 3m 15Mi See details about the resources that have been allocated to your nodes, rather than the current resource usage. The kubectl describe command provides a detailed breakdown of a specified pod or node. kubectl describe node <NODE_NAME> The output is quite verbose containing a full breakdown of the node\u2019s workloads, system info, and metadata such as labels and annotations. Click to expand node describe command output Name: basicnp-766qo Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=s-4vcpu-8gb-amd beta.kubernetes.io/os=linux doks.digitalocean.com/node-id=018db650-0dd3-4e23-a387-d386a193456e doks.digitalocean.com/node-pool=basicnp doks.digitalocean.com/node-pool-id=45a15812-c08d-48f0-ae7d-61eb0ddc3e7c doks.digitalocean.com/version=1.24.4-do.0 failure-domain.beta.kubernetes.io/region=nyc1 kubernetes.io/arch=amd64 kubernetes.io/hostname=basicnp-766qo kubernetes.io/os=linux node.kubernetes.io/instance-type=s-4vcpu-8gb-amd region=nyc1 topology.kubernetes.io/region=nyc1 type=basic Annotations: alpha.kubernetes.io/provided-node-ip: 10.116.0.4 csi.volume.kubernetes.io/nodeid: {\"dobs.csi.digitalocean.com\":\"318312990\"} io.cilium.network.ipv4-cilium-host: 10.244.1.102 io.cilium.network.ipv4-health-ip: 10.244.1.76 io.cilium.network.ipv4-pod-cidr: 10.244.1.0/25 node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 26 Sep 2022 11:48:39 +0300 Taints: <none> Unschedulable: false Lease: HolderIdentity: basicnp-766qo AcquireTime: <unset> RenewTime: Wed, 28 Sep 2022 11:31:13 +0300 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Mon, 26 Sep 2022 11:50:10 +0300 Mon, 26 Sep 2022 11:50:10 +0300 CiliumIsUp Cilium is running on this node MemoryPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:49:10 +0300 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 10.116.0.4 Hostname: basicnp-766qo ExternalIP: 159.223.163.15 Capacity: cpu: 4 ephemeral-storage: 165089200Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 8149728Ki pods: 110 Allocatable: cpu: 3900m ephemeral-storage: 152146206469 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 6694Mi pods: 110 System Info: Machine ID: a6b0d5360452428382d5b0c516caa546 System UUID: a6b0d536-0452-4283-82d5-b0c516caa546 Boot ID: 3345884c-f9af-4716-af86-17814d28ef96 Kernel Version: 5.10.0-0.bpo.15-amd64 OS Image: Debian GNU/Linux 10 (buster) Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.4.13 Kubelet Version: v1.24.4 Kube-Proxy Version: v1.24.4 ProviderID: digitalocean://318312990 Non-terminated Pods: (15 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- cert-manager cert-manager-ddd4d6ddf-zmpk4 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h ingress-nginx ingress-nginx-controller-778667bc4b-7lj4c 100m (2%) 0 (0%) 90Mi (1%) 0 (0%) 47h kube-system cilium-bv8x9 310m (7%) 100m (2%) 310Mi (4%) 75Mi (1%) 47h kube-system cilium-operator-6d485f4f69-fsv7x 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system coredns-9c8d9dc8c-9mzcd 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system cpc-bridge-proxy-dktvq 100m (2%) 0 (0%) 75Mi (1%) 0 (0%) 47h kube-system csi-do-node-trfdm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system do-node-agent-vt9kn 102m (2%) 102m (2%) 80Mi (1%) 300Mi (4%) 47h kube-system konnectivity-agent-fnvp9 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system kube-proxy-9ff7c 0 (0%) 0 (0%) 125Mi (1%) 0 (0%) 47h microservices-demo-dev cartservice-84758f76f-cl9vm 200m (5%) 300m (7%) 128Mi (1%) 256Mi (3%) 45h microservices-demo-dev frontend-6d45d8cc5d-59fmw 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev productcatalogservice-556d4f9446-7sqp9 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev recommendationservice-59f78c445b-5487v 100m (2%) 200m (5%) 220Mi (3%) 450Mi (6%) 45h microservices-demo-dev redis-cart-596c7658c4-lwf8g 70m (1%) 125m (3%) 200Mi (2%) 256Mi (3%) 45h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1382m (35%) 1227m (31%) memory 1721869056 (24%) 1970381568 (28%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Installing the Kubernetes Dashboard Note The Kubernetes Dashboard is already available as a managed solution for DigitalOcean customers after creating a Kubernetes Cluster. You are installing it separately due to the lack of CPU and memory usage metrics not displayed in the managed solution and those metrics are very important to monitor. Please see this support case for more details. In this section you will install the community maintained Kubernetes Dashboard . Please follow below steps to install it using kubectl: Install the Kubernetes Dashboard using kubectl : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml Note To check if the installation was successful, run the kubectl get pods -n kubernetes-dashboard command, and confirm that the pods are running. In a new terminal window start the kubectl proxy : kubectl proxy Launch a web browser and open the Kubernetes Dashboard Login page. Then, choose the Kubeconfig option, and provide your cluster's config file to log in. Note To get the config file navigate to your DigitalOcean cloud console, go to Kubernetes , select your cluster and from the Configuration section, click on Download Config File . After successfully logging in, you should be presented with the main dashboard landing page: Next, you can check metric summaries for each pod, node, and namespace in your cluster. Editing Kubernetes objects is also possible, such as scaling up/down deployments, change image version for pods, etc. Going further it is also possible to inspect log streams for pods. From the navigation bar at the top of the Pods view, click on the Logs tab to access a pod log stream directly in your web browser. In case of pods comprised of multiple containers, you have the option to inspect each container logs. Finally, you can Exec into a pod container from the same page. Kuberentes events are also viewable from the Kubernetes Dashboard . From the left menu click on the Events view. Events will be displayed and stored for 1 hour. Next, you will learn how to provision and configure the staging environment for the online boutique sample application. Besides DOKS setup and the sample app deployment, you will also configure a full observability stack comprised of logging, monitoring and alerting via Slack. Usually, a staging environment should be pretty close (if not similar) to a production environment.","title":"Observability"},{"location":"02-development/observability-dev/#introduction","text":"Note You will not install a full blown monitoring and logging solution on the development cluster such as Prometheus and Loki , but will do so for the staging and production clusters. On the development environment you will be configuring the Kubernetes Metrics Server and Kubernetes Dashboard tools. This way your cluster size can be small but sufficient that you can do your local development and deploy that to a Kubernetes cluster and test it. At the most basic level you will need to be able to troubleshoot your aplication if things go wrong. In this section, you will learn about the Kubernetes Metrics Server and the Kubernetes Dashboard . Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API . Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.","title":"Introduction"},{"location":"02-development/observability-dev/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. JQ command-line JSON processor installed on your machine. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section.","title":"Prerequisites"},{"location":"02-development/observability-dev/#installing-the-kubernetes-metrics-server","text":"In this section you will install the community maintained Kubernetes Metrics Server. Please follow below steps to install it using Helm: Add the Metrics Server Helm repository: helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ Install the Kubernetes Metrics Server using Helm : helm upgrade --install metrics-server metrics-server/metrics-server \\ --namespace metrics-server \\ --create-namespace Note To check if the installation was successful, run the helm ls -n metrics-server command, and confirm the deployment status.","title":"Installing the Kubernetes Metrics Server"},{"location":"02-development/observability-dev/#collect-resource-metrics-from-kubernetes-objects","text":"Resource metrics track the utilization and availability of critical resources such as CPU, memory, and storage. Kubernetes provides a Metrics API and a number of command line queries that allow you to retrieve snapshots of resource utilization. Query the Metrics API to retrieve current metrics from any node or pod (you can find your desired node or pod by running kubectl get nodes or kubectl get pods ): kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<NODE_NAME> | jq kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/<NAMESPACE>/pods/<POD_NAME> | jq Info The Metrics API returns a JSON object, so (optionally) piping the response through jq displays the JSON in a more human-readable format. Retrieve compact metric snapshots from the Metrics API using kubectl top. The kubectl top command returns current CPU and memory usage for a cluster\u2019s pods or nodes, or for a particular pod or node if specified. kubectl top node The output should look like the following: NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% basicnp-766qo 280m 7% 1662Mi 24% basicnp-766qr 252m 6% 1561Mi 23% Or you can query resource utilization by pod in a particular namespace (you can use the microservices-demo-dev namespace you deployed in the Tilt Remote section): kubectl top pod --namespace microservices-demo-dev The output should look like the following: NAME CPU(cores) MEMORY(bytes) cartservice-84758f76f-cl9vm 8m 29Mi checkoutservice-5d8d8cfd5f-8hbjn 3m 16Mi currencyservice-5d5f698f87-kh4z8 4m 30Mi emailservice-f8795cc94-r7hwp 30m 40Mi frontend-6d45d8cc5d-59fmw 1m 20Mi paymentservice-995d69494-kcqn2 5m 30Mi productcatalogservice-556d4f9446-7sqp9 7m 16Mi recommendationservice-59f78c445b-5487v 40m 40Mi redis-cart-596c7658c4-lwf8g 3m 7Mi shippingservice-bfc488696-dkcpz 3m 15Mi See details about the resources that have been allocated to your nodes, rather than the current resource usage. The kubectl describe command provides a detailed breakdown of a specified pod or node. kubectl describe node <NODE_NAME> The output is quite verbose containing a full breakdown of the node\u2019s workloads, system info, and metadata such as labels and annotations. Click to expand node describe command output Name: basicnp-766qo Roles: <none> Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=s-4vcpu-8gb-amd beta.kubernetes.io/os=linux doks.digitalocean.com/node-id=018db650-0dd3-4e23-a387-d386a193456e doks.digitalocean.com/node-pool=basicnp doks.digitalocean.com/node-pool-id=45a15812-c08d-48f0-ae7d-61eb0ddc3e7c doks.digitalocean.com/version=1.24.4-do.0 failure-domain.beta.kubernetes.io/region=nyc1 kubernetes.io/arch=amd64 kubernetes.io/hostname=basicnp-766qo kubernetes.io/os=linux node.kubernetes.io/instance-type=s-4vcpu-8gb-amd region=nyc1 topology.kubernetes.io/region=nyc1 type=basic Annotations: alpha.kubernetes.io/provided-node-ip: 10.116.0.4 csi.volume.kubernetes.io/nodeid: {\"dobs.csi.digitalocean.com\":\"318312990\"} io.cilium.network.ipv4-cilium-host: 10.244.1.102 io.cilium.network.ipv4-health-ip: 10.244.1.76 io.cilium.network.ipv4-pod-cidr: 10.244.1.0/25 node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 26 Sep 2022 11:48:39 +0300 Taints: <none> Unschedulable: false Lease: HolderIdentity: basicnp-766qo AcquireTime: <unset> RenewTime: Wed, 28 Sep 2022 11:31:13 +0300 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Mon, 26 Sep 2022 11:50:10 +0300 Mon, 26 Sep 2022 11:50:10 +0300 CiliumIsUp Cilium is running on this node MemoryPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:48:39 +0300 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Wed, 28 Sep 2022 11:26:35 +0300 Mon, 26 Sep 2022 11:49:10 +0300 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 10.116.0.4 Hostname: basicnp-766qo ExternalIP: 159.223.163.15 Capacity: cpu: 4 ephemeral-storage: 165089200Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 8149728Ki pods: 110 Allocatable: cpu: 3900m ephemeral-storage: 152146206469 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 6694Mi pods: 110 System Info: Machine ID: a6b0d5360452428382d5b0c516caa546 System UUID: a6b0d536-0452-4283-82d5-b0c516caa546 Boot ID: 3345884c-f9af-4716-af86-17814d28ef96 Kernel Version: 5.10.0-0.bpo.15-amd64 OS Image: Debian GNU/Linux 10 (buster) Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.4.13 Kubelet Version: v1.24.4 Kube-Proxy Version: v1.24.4 ProviderID: digitalocean://318312990 Non-terminated Pods: (15 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- cert-manager cert-manager-ddd4d6ddf-zmpk4 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h ingress-nginx ingress-nginx-controller-778667bc4b-7lj4c 100m (2%) 0 (0%) 90Mi (1%) 0 (0%) 47h kube-system cilium-bv8x9 310m (7%) 100m (2%) 310Mi (4%) 75Mi (1%) 47h kube-system cilium-operator-6d485f4f69-fsv7x 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system coredns-9c8d9dc8c-9mzcd 100m (2%) 0 (0%) 150M (2%) 150M (2%) 47h kube-system cpc-bridge-proxy-dktvq 100m (2%) 0 (0%) 75Mi (1%) 0 (0%) 47h kube-system csi-do-node-trfdm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system do-node-agent-vt9kn 102m (2%) 102m (2%) 80Mi (1%) 300Mi (4%) 47h kube-system konnectivity-agent-fnvp9 0 (0%) 0 (0%) 0 (0%) 0 (0%) 47h kube-system kube-proxy-9ff7c 0 (0%) 0 (0%) 125Mi (1%) 0 (0%) 47h microservices-demo-dev cartservice-84758f76f-cl9vm 200m (5%) 300m (7%) 128Mi (1%) 256Mi (3%) 45h microservices-demo-dev frontend-6d45d8cc5d-59fmw 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev productcatalogservice-556d4f9446-7sqp9 100m (2%) 200m (5%) 64Mi (0%) 128Mi (1%) 45h microservices-demo-dev recommendationservice-59f78c445b-5487v 100m (2%) 200m (5%) 220Mi (3%) 450Mi (6%) 45h microservices-demo-dev redis-cart-596c7658c4-lwf8g 70m (1%) 125m (3%) 200Mi (2%) 256Mi (3%) 45h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1382m (35%) 1227m (31%) memory 1721869056 (24%) 1970381568 (28%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%)","title":"Collect resource metrics from Kubernetes objects"},{"location":"02-development/observability-dev/#installing-the-kubernetes-dashboard","text":"Note The Kubernetes Dashboard is already available as a managed solution for DigitalOcean customers after creating a Kubernetes Cluster. You are installing it separately due to the lack of CPU and memory usage metrics not displayed in the managed solution and those metrics are very important to monitor. Please see this support case for more details. In this section you will install the community maintained Kubernetes Dashboard . Please follow below steps to install it using kubectl: Install the Kubernetes Dashboard using kubectl : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml Note To check if the installation was successful, run the kubectl get pods -n kubernetes-dashboard command, and confirm that the pods are running. In a new terminal window start the kubectl proxy : kubectl proxy Launch a web browser and open the Kubernetes Dashboard Login page. Then, choose the Kubeconfig option, and provide your cluster's config file to log in. Note To get the config file navigate to your DigitalOcean cloud console, go to Kubernetes , select your cluster and from the Configuration section, click on Download Config File . After successfully logging in, you should be presented with the main dashboard landing page: Next, you can check metric summaries for each pod, node, and namespace in your cluster. Editing Kubernetes objects is also possible, such as scaling up/down deployments, change image version for pods, etc. Going further it is also possible to inspect log streams for pods. From the navigation bar at the top of the Pods view, click on the Logs tab to access a pod log stream directly in your web browser. In case of pods comprised of multiple containers, you have the option to inspect each container logs. Finally, you can Exec into a pod container from the same page. Kuberentes events are also viewable from the Kubernetes Dashboard . From the left menu click on the Events view. Events will be displayed and stored for 1 hour. Next, you will learn how to provision and configure the staging environment for the online boutique sample application. Besides DOKS setup and the sample app deployment, you will also configure a full observability stack comprised of logging, monitoring and alerting via Slack. Usually, a staging environment should be pretty close (if not similar) to a production environment.","title":"Installing the Kubernetes Dashboard"},{"location":"02-development/setup-doks-dev/","text":"Introduction This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster that can be used for remote development, targeting the online boutique sample application used as a reference in this guide. Prerequisites To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Doctl client already set up to work with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API chapter. Provisioning a Development DOKS Cluster for Microservices In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-dev , with a pool size of 3 nodes , each having 2 vCPUs and 4GB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-dev \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb;count=3;tag=adoption-journey;label=type=basic\" \\ --region nyc1 Note The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 72$/month . For simplicity and consistency through all the guide, the microservices-demo-dev name was picked for the dev cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation . Configuring DOKS for Private Registries From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to perform local microservices development using Tilt .","title":"Set up Development DOKS"},{"location":"02-development/setup-doks-dev/#introduction","text":"This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster that can be used for remote development, targeting the online boutique sample application used as a reference in this guide.","title":"Introduction"},{"location":"02-development/setup-doks-dev/#prerequisites","text":"To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Doctl client already set up to work with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API chapter.","title":"Prerequisites"},{"location":"02-development/setup-doks-dev/#provisioning-a-development-doks-cluster-for-microservices","text":"In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-dev , with a pool size of 3 nodes , each having 2 vCPUs and 4GB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-dev \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb;count=3;tag=adoption-journey;label=type=basic\" \\ --region nyc1 Note The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 72$/month . For simplicity and consistency through all the guide, the microservices-demo-dev name was picked for the dev cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation .","title":"Provisioning a Development DOKS Cluster for Microservices"},{"location":"02-development/setup-doks-dev/#configuring-doks-for-private-registries","text":"From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to perform local microservices development using Tilt .","title":"Configuring DOKS for Private Registries"},{"location":"02-development/setup-ingress-dev/","text":"Introduction In this section, you will learn how to install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to discover how to automatically issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy. Installing the Nginx Ingress Controller In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. Create an A record for your host: LOAD_BALANCER_IP = $( doctl compute load-balancer list --format IP --no-header ) doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \" $LOAD_BALANCER_IP \" \\ --record-ttl \"30\" Info The upper mentioned command works if you have only one LB in your DO account. If you have multiple LBs you will need to add the its IP in the command. Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-dev \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservices-demo-dev namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-dev spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-dev should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-dev to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-dev spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-dev Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-dev namespace : microservices-demo-dev spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next, you will deploy the Kubernetes Dashboard and Kubernetes Metrics Server to your cluster in order to visualize application and cluster related metrics, as well as corresponding logs and events.","title":"Set up ingress"},{"location":"02-development/setup-ingress-dev/#introduction","text":"In this section, you will learn how to install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to discover how to automatically issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications.","title":"Introduction"},{"location":"02-development/setup-ingress-dev/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Tilt remote development section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy.","title":"Prerequisites"},{"location":"02-development/setup-ingress-dev/#installing-the-nginx-ingress-controller","text":"In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. Create an A record for your host: LOAD_BALANCER_IP = $( doctl compute load-balancer list --format IP --no-header ) doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \" $LOAD_BALANCER_IP \" \\ --record-ttl \"30\" Info The upper mentioned command works if you have only one LB in your DO account. If you have multiple LBs you will need to add the its IP in the command. Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-dev \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservices-demo-dev namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-dev spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-dev should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-dev to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-dev spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-dev Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-dev namespace : microservices-demo-dev spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/02-development/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next, you will deploy the Kubernetes Dashboard and Kubernetes Metrics Server to your cluster in order to visualize application and cluster related metrics, as well as corresponding logs and events.","title":"Installing the Nginx Ingress Controller"},{"location":"02-development/tilt-local/","text":"Introduction This section will show you how to do local development using Tilt . Tilt eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date. The way Tilt works is based on a single Tiltfile present in your root project directory containing environment setup logic. You will install the online boutique sample application on your local environment using Docker Desktop and Tilt. The way you will use Tilt in this guide is based on configuration profiles . This approach has a major benefit - application logic is decoupled from configuration data. It means, you don't have to modify the base Tiltfile only if really required, or to add new functionality. Tilt automatically loads (if present) the configuration file ( tilt_config.json ), and sets up the environment for you. Based on your current requirements or needs, you just pick the appropriate tilt_config.json file from the tilt-resources directory, and copy it alongside main Tiltfile. Below is an example of such setup: . . . \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 Tiltfile \u2514\u2500\u2500 tilt_config.json You need to copy only once the environment specific configuration file, and then run tilt up from the root directory of your project: If you need a local environment setup - cp tilt-resources/local/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action. If you need a remote development setup - cp tilt-resources/dev/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action. Prerequisites To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools -> Tilt section. Docker desktop already installed and running as explained in the Installing Required Tools -> Docker Desktop section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Local development with Tilt Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch your current Kubernetes config to docker-desktop : kubectl config use-context docker-desktop Note This is required for local development as Tilt can be ran against a production Kubernetes cluster for example, and you can accidentally perform unwanted changes. Within the current directory, copy the local profile configuration for Tilt: cp tilt-resources/local/tilt_config.json . Note Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo local environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: You should see the following: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page: Live Updates with Tilt Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... < div class = \"col-12\" > < h3 > On Sale Now </ h3 > </ div > }); Navigate to Tilt's detailed view using the web interface. You should see that the frontend resource is being rebuilt. Finally, open a web browser and point to localhost:9090 . You should see the updated online boutique welcome page with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to perform remote development for the same set of microservices, using the Kubernetes environment created in the Set up Development DOKS section.","title":"Tilt for local development"},{"location":"02-development/tilt-local/#introduction","text":"This section will show you how to do local development using Tilt . Tilt eases local development by taking away the pain of time consuming Docker builds, watching files, and bringing environments up to date. The way Tilt works is based on a single Tiltfile present in your root project directory containing environment setup logic. You will install the online boutique sample application on your local environment using Docker Desktop and Tilt. The way you will use Tilt in this guide is based on configuration profiles . This approach has a major benefit - application logic is decoupled from configuration data. It means, you don't have to modify the base Tiltfile only if really required, or to add new functionality. Tilt automatically loads (if present) the configuration file ( tilt_config.json ), and sets up the environment for you. Based on your current requirements or needs, you just pick the appropriate tilt_config.json file from the tilt-resources directory, and copy it alongside main Tiltfile. Below is an example of such setup: . . . \u251c\u2500\u2500 tilt-resources \u2502 \u251c\u2500\u2500 dev \u2502 \u2502 \u2514\u2500\u2500 tilt_config.json \u2502 \u2514\u2500\u2500 local \u2502 \u2514\u2500\u2500 tilt_config.json \u251c\u2500\u2500 Tiltfile \u2514\u2500\u2500 tilt_config.json You need to copy only once the environment specific configuration file, and then run tilt up from the root directory of your project: If you need a local environment setup - cp tilt-resources/local/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action. If you need a remote development setup - cp tilt-resources/dev/tilt_config.json <Tiltfile_directory> . Then, run tilt up , and see it in action.","title":"Introduction"},{"location":"02-development/tilt-local/#prerequisites","text":"To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools -> Tilt section. Docker desktop already installed and running as explained in the Installing Required Tools -> Docker Desktop section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section.","title":"Prerequisites"},{"location":"02-development/tilt-local/#local-development-with-tilt","text":"Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch your current Kubernetes config to docker-desktop : kubectl config use-context docker-desktop Note This is required for local development as Tilt can be ran against a production Kubernetes cluster for example, and you can accidentally perform unwanted changes. Within the current directory, copy the local profile configuration for Tilt: cp tilt-resources/local/tilt_config.json . Note Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo local environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: You should see the following: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page:","title":"Local development with Tilt"},{"location":"02-development/tilt-local/#live-updates-with-tilt","text":"Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... < div class = \"col-12\" > < h3 > On Sale Now </ h3 > </ div > }); Navigate to Tilt's detailed view using the web interface. You should see that the frontend resource is being rebuilt. Finally, open a web browser and point to localhost:9090 . You should see the updated online boutique welcome page with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to perform remote development for the same set of microservices, using the Kubernetes environment created in the Set up Development DOKS section.","title":"Live Updates with Tilt"},{"location":"02-development/tilt-remote/","text":"Introduction This section will show you how to do remote development using Tilt . It is very similar to the local development guide, the only difference being you will work directly on the remote Kubernetes cluster created in the Set up Development DOKS section. Application changes and reloading will happen on the fly on the remote development cluster as well. Next, you will use Tilt to deploy the online boutique sample application to your development DOKS cluster. The same approach based on Tilt configuration profiles is being used here as well. You only need to copy the environment specific configuration file ( tilt_config.json ) to your root project directory, and then run tilt up . Prerequisites To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools section. A container registry already set up as explained in the Set up DOCR section. A development Kubernetes cluster (DOKS) up and running as explained in the Set up Development DOKS section. You need to have proper permissions to create namespaces and deploy resources. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Remote development with Tilt Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch current Kubernetes config to your microservices-demo-dev cluster. Bear in mind that the context is prefixed using the do-<region_id>- string (e.g. do-nyc1- ): kubectl config use-context do -nyc1-microservices-demo-dev All microservices Docker images are built on your local machine, and then pushed to your DOCR registry. A registry login is required first using doctl : doctl registry login Within the current directory, copy the dev profile configuration for Tilt: cp tilt-resources/dev/tilt_config.json . Warn Make sure to give a unique value for the namespace property in the tilt_config.json file to avoid overriding existing applications on the remote development cluster. Make sure to change the default_registry value from the tilt_config.json file to point to your DOCR registry name as configured in the DOCR setup chapter from this guide. Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo dev environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. This may take a few minutes. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page: Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote development cluster by Tilt. Live Updates with Tilt Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... <div class=\"col-12\"> <h3>On Sale Now</h3> </div> }); Navigate to Tilt 's detailed view using the web interface. You should see the frontend resource being rebuilt. The updated docker image will be pushed to your DOCR. Finally, open a web browser and point to localhost:9090 . You should see the online boutique welcome page updated with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to deploy and configure the Nginx ingress controller for your development cluster (DOKS) to expose microservices to the outside world. You will also learn how to set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Tilt for remote development"},{"location":"02-development/tilt-remote/#introduction","text":"This section will show you how to do remote development using Tilt . It is very similar to the local development guide, the only difference being you will work directly on the remote Kubernetes cluster created in the Set up Development DOKS section. Application changes and reloading will happen on the fly on the remote development cluster as well. Next, you will use Tilt to deploy the online boutique sample application to your development DOKS cluster. The same approach based on Tilt configuration profiles is being used here as well. You only need to copy the environment specific configuration file ( tilt_config.json ) to your root project directory, and then run tilt up .","title":"Introduction"},{"location":"02-development/tilt-remote/#prerequisites","text":"To complete this section you will need: Tilt already installed and working as explained in the Installing Required Tools section. A container registry already set up as explained in the Set up DOCR section. A development Kubernetes cluster (DOKS) up and running as explained in the Set up Development DOKS section. You need to have proper permissions to create namespaces and deploy resources. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section.","title":"Prerequisites"},{"location":"02-development/tilt-remote/#remote-development-with-tilt","text":"Clone your microservices-demo repository if you haven't already (make sure to replace the <> placeholders first): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to microservices-demo folder: cd microservices-demo Switch current Kubernetes config to your microservices-demo-dev cluster. Bear in mind that the context is prefixed using the do-<region_id>- string (e.g. do-nyc1- ): kubectl config use-context do -nyc1-microservices-demo-dev All microservices Docker images are built on your local machine, and then pushed to your DOCR registry. A registry login is required first using doctl : doctl registry login Within the current directory, copy the dev profile configuration for Tilt: cp tilt-resources/dev/tilt_config.json . Warn Make sure to give a unique value for the namespace property in the tilt_config.json file to avoid overriding existing applications on the remote development cluster. Make sure to change the default_registry value from the tilt_config.json file to point to your DOCR registry name as configured in the DOCR setup chapter from this guide. Tilt configuration files will be excluded from git commits by .gitignore settings. Each developer customizes his profile locally as desired, and should not impact other team members. Bring the microservices-demo dev environment up using Tilt: tilt up You should see the following output: Tilt started on http://localhost:10350/ v0.30.7, built 2022-08-12 (space) to open the browser (s) to stream logs (--stream=true) (t) to open legacy terminal mode (--legacy=true) (ctrl-c) to exit Press the Space bar to open Tilt's UI: Note Please note that from the top left you can switch between Table and Detail view. Detail view offers a lot more information on what Tilt is doing such as logs from all Kubernetes resources. This may take a few minutes. Open a web browser and point to localhost:9090 . You should see the online boutique welcome page: Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote development cluster by Tilt.","title":"Remote development with Tilt"},{"location":"02-development/tilt-remote/#live-updates-with-tilt","text":"Tilt has the ability to reload and rebuild resources at the right time. Every code change triggers Tilt to automatically rebuild local docker images, and roll out new versions of your application pods. Follow steps below to watch Tilt doing live updates for your application: Change directory to your local clone of the microservices-demo project directory (if not already). Then, open the src/frontend/templates/home.html file using a text editor of your choice (preferably with HTML lint support). For example, you can use VS Code : code src/frontend/templates/home.html Next, change one of the h3 tags to something different, such as: ... <div class=\"col-12\"> <h3>On Sale Now</h3> </div> }); Navigate to Tilt 's detailed view using the web interface. You should see the frontend resource being rebuilt. The updated docker image will be pushed to your DOCR. Finally, open a web browser and point to localhost:9090 . You should see the online boutique welcome page updated with your changes: Info Due to browser cache the changes might not appear immediately and for this reason you can hard refresh your browser to see the changes. On modern browsers this can be achieved by pressing Command + Shift + R on macOS, and Ctrl + Shift + R for Linux systems. Next, you will learn how to deploy and configure the Nginx ingress controller for your development cluster (DOKS) to expose microservices to the outside world. You will also learn how to set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Live Updates with Tilt"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/","text":"Introduction Note On the development cluster you used Tilt to deploy the sample application. On the staging environment, kustomize will be used to initially deploy the application and then ArgoCD will do the heavy lifting for future deployments. In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize . Prerequisites To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Bootstrap the online boutique application using Kustomize Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/staging Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-staging command. The application is deployed to the staging environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-staging 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote staging cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your staging cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Deploying the online boutique sample application"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/#introduction","text":"Note On the development cluster you used Tilt to deploy the sample application. On the staging environment, kustomize will be used to initially deploy the application and then ArgoCD will do the heavy lifting for future deployments. In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize .","title":"Introduction"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section.","title":"Prerequisites"},{"location":"03-staging/deploying-the-online-boutique-sample-application-staging/#bootstrap-the-online-boutique-application-using-kustomize","text":"Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/staging Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-staging command. The application is deployed to the staging environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-staging 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote staging cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your staging cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Bootstrap the online boutique application using Kustomize"},{"location":"03-staging/observability-staging/","text":"Introduction The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons. Installing the Prometheus Monitoring Stack Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods. Configuring Persistent Storage for Prometheus In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Configuring Persistent Storage for Grafana In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Installing the Loki Stack In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status. Configuring Grafana with Loki In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-staging namespace you can run: {namespace=\"microservices-demo-staging\"} . Configuring Persistent Storage for Loki In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/03-staging/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode). Setting up a retention policy In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one. Setting up Alert Manager Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-staging namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-staging\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list. Configuring Alertmanager to Send Notifications to Slack To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article . Setting up Event Exporter for events retention A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/03-staging/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/03-staging/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/03-staging/assets/manifests/event-exporter-deployment.yaml Viewing events in Grafana If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial .","title":"Observability"},{"location":"03-staging/observability-staging/#introduction","text":"The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators.","title":"Introduction"},{"location":"03-staging/observability-staging/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons.","title":"Prerequisites"},{"location":"03-staging/observability-staging/#installing-the-prometheus-monitoring-stack","text":"Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods.","title":"Installing the Prometheus Monitoring Stack"},{"location":"03-staging/observability-staging/#configuring-persistent-storage-for-prometheus","text":"In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Prometheus"},{"location":"03-staging/observability-staging/#configuring-persistent-storage-for-grafana","text":"In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the staging environment, you will define a 5 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 5Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Grafana"},{"location":"03-staging/observability-staging/#installing-the-loki-stack","text":"In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status.","title":"Installing the Loki Stack"},{"location":"03-staging/observability-staging/#configuring-grafana-with-loki","text":"In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-staging namespace you can run: {namespace=\"microservices-demo-staging\"} .","title":"Configuring Grafana with Loki"},{"location":"03-staging/observability-staging/#configuring-persistent-storage-for-loki","text":"In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/03-staging/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/03-staging/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode).","title":"Configuring Persistent Storage for Loki"},{"location":"03-staging/observability-staging/#setting-up-a-retention-policy","text":"In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one.","title":"Setting up a retention policy"},{"location":"03-staging/observability-staging/#setting-up-alert-manager","text":"Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-staging namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-staging\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list.","title":"Setting up Alert Manager"},{"location":"03-staging/observability-staging/#configuring-alertmanager-to-send-notifications-to-slack","text":"To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/03-staging/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/03-staging/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article .","title":"Configuring Alertmanager to Send Notifications to Slack"},{"location":"03-staging/observability-staging/#setting-up-event-exporter-for-events-retention","text":"A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/03-staging/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/03-staging/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/03-staging/assets/manifests/event-exporter-deployment.yaml","title":"Setting up Event Exporter for events retention"},{"location":"03-staging/observability-staging/#viewing-events-in-grafana","text":"If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial .","title":"Viewing events in Grafana"},{"location":"03-staging/setup-doks-staging/","text":"Introduction This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as a staging environment, targeting the online boutique sample application used as a reference in this guide. Note A staging environment should be pretty close (if not similar) to a production environment hence you will be creating a bigger cluster, resource wise, to be able to handle the workload you would normally have on your production environment. Prerequisites To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section. Provisioning a Staging DOKS Cluster for Microservices In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-staging , with a pool size of 3 nodes , auto-scale to 2-4 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-staging \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=3;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=2;max-nodes=4\" \\ --region nyc1 Note The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 84$/month . For simplicity and consistency through all the guide, the microservices-demo-staging name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation . Configuring DOKS for Private Registries From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your staging cluster using Kustomize .","title":"Set up Staging DOKS"},{"location":"03-staging/setup-doks-staging/#introduction","text":"This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as a staging environment, targeting the online boutique sample application used as a reference in this guide. Note A staging environment should be pretty close (if not similar) to a production environment hence you will be creating a bigger cluster, resource wise, to be able to handle the workload you would normally have on your production environment.","title":"Introduction"},{"location":"03-staging/setup-doks-staging/#prerequisites","text":"To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section.","title":"Prerequisites"},{"location":"03-staging/setup-doks-staging/#provisioning-a-staging-doks-cluster-for-microservices","text":"In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-staging , with a pool size of 3 nodes , auto-scale to 2-4 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-staging \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=3;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=2;max-nodes=4\" \\ --region nyc1 Note The example cluster created above is using 3 nodes, each having 2vCPU/4GB size, which amounts to 84$/month . For simplicity and consistency through all the guide, the microservices-demo-staging name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation .","title":"Provisioning a Staging DOKS Cluster for Microservices"},{"location":"03-staging/setup-doks-staging/#configuring-doks-for-private-registries","text":"From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your staging cluster using Kustomize .","title":"Configuring DOKS for Private Registries"},{"location":"03-staging/setup-ingress-staging/","text":"Introduction In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy. Installing the Nginx Ingress Controller In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_STAGING_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-staging \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-staging namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-staging spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-staging should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-staging to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-staging spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-staging Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-staging namespace : microservices-demo-staging spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Set up ingress"},{"location":"03-staging/setup-ingress-staging/#introduction","text":"In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications.","title":"Introduction"},{"location":"03-staging/setup-ingress-staging/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy.","title":"Prerequisites"},{"location":"03-staging/setup-ingress-staging/#installing-the-nginx-ingress-controller","text":"In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_STAGING_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-staging \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-staging namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-staging spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-staging should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-staging to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-staging spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-staging Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-staging namespace : microservices-demo-staging spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/03-staging/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Installing the Nginx Ingress Controller"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/","text":"Introduction In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize . Prerequisites To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Bootstrap the online boutique application using Kustomize Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/prod Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-prod command. The application is deployed to the production environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-prod 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote production cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your production cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Deploying the online boutique sample application"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/#introduction","text":"In this section you will learn how to deploy the online boutique sample application using Kustomize . It is tool for customizing Kubernetes configurations. It has the following features to manage application configuration files: generating resources from other sources setting cross-cutting fields for resources composing and customizing collections of resources A more common use case of Kustomize is that you\u2019ll need multiple variants of a common set of resources, e.g., a development , staging and production variant. For this purpose, kustomize supports the idea of an overlay and a base . Both are represented by a kustomization file. The base declares things that the variants share in common (both resources and a common customization of those resources), and the overlays declare the differences. This is well represented in the structure of the online boutique sample application repository structure . Info You will be using the kubectl built-in version of Kustomize .","title":"Introduction"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Set up DOCR section. The microservices-demo images build and pushed to DOCR as explained in the Set up DOCR --> Building and pushing docker images to DOCR A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. Doctl utility already installed as explained in the Installing Required Tools -> Doctl section.","title":"Prerequisites"},{"location":"04-production/deploying-the-online-boutique-sample-application-production/#bootstrap-the-online-boutique-application-using-kustomize","text":"Clone your fork of the kubernetes-sample-apps if you haven't already (make sure to replace the <> placeholders). git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/kubernetes-sample-apps.git Info The kubernetes-sample-apps repository was forked initially in the Setup DOCR section. Change directory to the microservices-demo folder: cd kubernetes-sample-apps/microservices-demo Deploy the Kustomization to your cluster using kubectl : kubectl apply -k kustomize/prod Note To verify that the deployment was succesful run the kubectl get all -n microservices-demo-prod command. The application is deployed to the production environment using the images built and pushed in the Setup DOCR section. Access the web interface by port-forwarding the frontend service: kubectl port-forward service/frontend -n microservices-demo-prod 9090 :80 Open a web browser and point to localhost:9090 . You should see the online boutique welcome page. Note Although you open a connection to localhost in your web browser, traffic is forwarded to the remote production cluster by kubectl . Next, you will deploy and configure the Nginx ingress controller for your production cluster (DOKS) to expose microservices to the outside world. You will also set up cert-manager to automatically issue valid TLS certificates for your applications.","title":"Bootstrap the online boutique application using Kustomize"},{"location":"04-production/observability-production/","text":"Introduction The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons. Installing the Prometheus Monitoring Stack Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods. Configuring Persistent Storage for Prometheus In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : replicas : 2 retention : 20 storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 10Gi Note The default retention time for metrics is set to 10d by default in the kube-prometheus-stack helm chart. In production the retention time will be set to 20d . After 20 days the metrics will be deleted from the Volume . Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Configuring Persistent Storage for Grafana In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 10Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel. Installing the Loki Stack In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status. Configuring Grafana with Loki In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-prod namespace you can run: {namespace=\"microservices-demo-prod\"} . Configuring Persistent Storage for Loki In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/04-production/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode). Setting up a retention policy In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one. Setting up Alert Manager Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-prod namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-prod\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list. Configuring Alertmanager to Send Notifications to Slack To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmaanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article . Setting up Event Exporter for events retention A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/04-production/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/04-production/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/04-production/assets/manifests/event-exporter-deployment.yaml Viewing events in Grafana If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial . Next, you will learn how to configure the CI/CD process and associated GitHub workflows for all project components used in this guide.","title":"Observability"},{"location":"04-production/observability-production/#introduction","text":"The goal of observability is to understand what\u2019s happening across all of your environments and among the technologies, so you can detect and resolve issues to keep your systems efficient and reliable. Observability is a measure of how well the system\u2019s internal states can be inferred from knowledge of its external outputs. It uses the data and insights that monitoring produces to provide a holistic understanding of your system, including its health and performance. The observability of your system, then, depends partly on how well your monitoring metrics can interpret your system's performance indicators.","title":"Introduction"},{"location":"04-production/observability-production/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A DO Spaces bucket for Loki storage. Please follow the official DigitalOcean tutorial to create one . Make sure that it is set to restrict file listing for security reasons.","title":"Prerequisites"},{"location":"04-production/observability-production/#installing-the-prometheus-monitoring-stack","text":"Add the Helm repository and list the available charts: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update prometheus-community Install the kube-prometheus-stack , using Helm : HELM_CHART_VERSION = \"35.5.1\" helm install kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note A specific version for the Helm chart is used. In this case 35.5.1 was picked, which maps to the 0.56.3 version of the application. To check if the installation was successful, run the helm ls -n monitoring command, and confirm the deployment status. Connect to Grafana (using default credentials: admin/prom-operator ) - by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Info You should NOT expose Grafana to public network (eg. you should create an ingress mapping or LB service). Open a web browser and point to localhost:3000 . You should see the Grafna login page. Info Grafana installation comes with a number of dashboards. Open a web browser on localhost:3000 . Once in, you can go to Dashboards -> Browse , and choose different dashboards. As an example, you can open the General / Kubernetes / Compute Resources / Node (Pods) and view the resource metrics for a node and its related pods.","title":"Installing the Prometheus Monitoring Stack"},{"location":"04-production/observability-production/#configuring-persistent-storage-for-prometheus","text":"In this section, you will learn how to enable persistent storage for Prometheus , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the \"docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: prometheusSpec : replicas : 2 retention : 20 storageSpec : volumeClaimTemplate : spec : storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 10Gi Note The default retention time for metrics is set to 10d by default in the kube-prometheus-stack helm chart. In production the retention time will be set to 20d . After 20 days the metrics will be deleted from the Volume . Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Prometheus"},{"location":"04-production/observability-production/#configuring-persistent-storage-for-grafana","text":"In this section, you will learn how to enable persistent storage for Grafana , so that metrics data is persisted across server restarts , or in case of cluster failures . For the production environment, you will define a 10 Gi Persistent Volume Claim (PVC), using the DigitalOcean Block Storage . Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the storageSpec section. The definition should look like this: grafana : ... persistence : enabled : true storageClassName : do-block-storage accessModes : [ \"ReadWriteOnce\" ] size : 10Gi Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check the PVC status by running kubectl get pvc -n monitoring . A new Volume should appear in the Volumes web page, from your DigitalOcean account panel.","title":"Configuring Persistent Storage for Grafana"},{"location":"04-production/observability-production/#installing-the-loki-stack","text":"In this section you will learn about Loki , which is a log aggregation system inspired by Prometheus . Loki uses Promtail to fetch logs from all Pods running in your cluster. Then, logs are aggregated and compressed , and sent to the configured storage . Next, you can connect Loki data source to Grafana and view the logs. Add the Grafana Helm repository and list the available charts: helm repo add grafana https://grafana.github.io/helm-charts helm repo update grafana Intall the Loki stack using Helm : HELM_CHART_VERSION = \"2.6.4\" helm install loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ --create-namespace \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note The above values file, enables Loki and Promtail for you, so no other input is required. Prometheus and Grafana installation is disabled, because Installing the Prometheus Monitoring Stack took care of it already. The 2.6.4 Helm chart version is picked for loki-stack , which maps to application version 2.4.2 . To check if the installation was successful, run the helm ls -n loki-stack command, and confirm the deployment status.","title":"Installing the Loki Stack"},{"location":"04-production/observability-production/#configuring-grafana-with-loki","text":"In this section, you will add the Loki data source to Grafana . First, you need to expose the Grafana web interface on your local machine (default credentials: admin/prom-operator ): kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Next, open a web browser on localhost:3000 , and follow below steps: Click the Configuration gear from the left panel. Select Data sources . Click the Add data source blue button. Select Loki from the list and add Loki url http://loki.loki-stack:3100 . Save and test. Info If everything goes well, a green label message will appear, saying Data source connected and labels found. You can access logs from the Explore tab of Grafana . Make sure to select Loki as the data source. Use the Help button for log search cheat sheet. !!! info As an example query, to retrieve all the logs for the microservices-demo-prod namespace you can run: {namespace=\"microservices-demo-prod\"} .","title":"Configuring Grafana with Loki"},{"location":"04-production/observability-production/#configuring-persistent-storage-for-loki","text":"In this step, you will learn how to enable persistent storage for Loki . You're going to use the DO Spaces bucket created in the Prerequisites section. Open the docs/04-production/assets/manifests/loki-stack-values-v35.5.1.yaml file provided and remove the comments surrounding the schema_config and storage_config keys. The definition should look like this: Click to expand the loki config loki : enabled : true config : schema_config : configs : - from : \"2020-10-24\" store : boltdb-shipper object_store : aws schema : v11 index : prefix : index_ period : 24h storage_config : boltdb_shipper : active_index_directory : /data/loki/boltdb-shipper-active cache_location : /data/loki/boltdb-shipper-cache cache_ttl : 24h shared_store : aws aws : bucketnames : <YOUR_DO_SPACES_BUCKET_NAME_HERE> endpoint : <YOUR_DO_SPACES_BUCKET_ENDPOINT_HERE> # in the following format: <region>.digitaloceanspaces.com region : <YOUR_DO_SPACES_BUCKET_REGION_HERE> # short region name (e.g.: fra1) access_key_id : <YOUR_DO_SPACES_ACCESS_KEY_HERE> secret_access_key : <YOURDO_SPACES_SECRET_KEY_HERE> s3forcepathstyle : true Apply the new settings using Helm : HELM_CHART_VERSION = \"2.6.4\" helm upgrade loki grafana/loki-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace = loki-stack \\ -f \"docs/04-production/assets/manifests/loki-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Note Check if the main Loki application pod is up and running by runnging the following kubectl command: kubectl get pods -n loki-stack -l app=loki . If everything goes well, you should see the DO Spaces bucket containing the index and chunks folders (the chunks folder is called fake , which is a strange name - this is by design, when not running in multi-tenant mode).","title":"Configuring Persistent Storage for Loki"},{"location":"04-production/observability-production/#setting-up-a-retention-policy","text":"In this step you will set up a retention policy for your DO Spaces bucket. S3CMD is a good utility to have in order to set up retention policies. Please follow the DigitalOcean guide for installing and setting up s3cmd . Configure the Loki bucket lifecycle, using s3cmd : Click to expand the Loki bucket lifecycle <LifecycleConfiguration xmlns= \"http://s3.amazonaws.com/doc/2006-03-01/\" > <Rule> <ID> Expire old fake data </ID> <Prefix> fake/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> <Rule> <ID> Expire old index data </ID> <Prefix> index/ </Prefix> <Status> Enabled </Status> <Expiration> <Days> 10 </Days> </Expiration> </Rule> </LifecycleConfiguration> s3cmd setlifecycle 04 -setup-observability/assets/manifests/loki_do_spaces_lifecycle.xml s3://<LOKI_STORAGE_BUCKET_NAME> Check that the policy was set (please replace the <> placeholders accordingly): s3cmd getlifecycle s3://<LOKI_STORAGE_BUCKET_NAME> Note The DO Spaces backend implementation will clean the objects for you automatically , based on the expiration date. You can always go back and edit the policy if needed later on, by uploading a new one.","title":"Setting up a retention policy"},{"location":"04-production/observability-production/#setting-up-alert-manager","text":"Alertmanager is deployed alongside Prometheus and forms the alerting layer of the kube-prom-stack . It handles alerts generated by Prometheus by deduplicating, grouping, and routing them to various integrations such as email, Slack or PagerDuty. Alerts and notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing, etc.), you will want to get notifications in real time to handle critical situations as soon as possible. To create a new alert, you need to add a new definition in the additionalPrometheusRule s section from the kube-prom-stack Helm values file. You will be creating a sample alert that will trigger if the microservices-demo-prod namespace does not have an expected number of instances. The expected number of pods for the online boutique application is 10. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the additionalPrometheusRulesMap block. The definition should look like this: additionalPrometheusRulesMap : rule-name : groups : - name : online-boutique-instance-down rules : - alert : OnlineBoutiqueInstanceDown expr : sum(kube_pod_owner{namespace=\"microservices-demo-prod\"}) by (namespace) < 10 for : 1m labels : severity : 'critical' annotations : description : ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 10. ' summary : 'Pod {{ $labels.pod }} down' Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" Info To check that the alert has been created successfully, first port-forward to your local machine by running this command: kubectl --namespace monitoring port-forward service/kube-prom-stack-kube-prome-prometheus 9090:9090 . Navigate to the Promethes Console click on the Alerts menu item and identify the OnlineBoutiqueInstanceDown alert. It should be visible at the bottom of the list.","title":"Setting up Alert Manager"},{"location":"04-production/observability-production/#configuring-alertmanager-to-send-notifications-to-slack","text":"To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from AlertManager . Steps to follow: Open a web browser and navigate to https://api.slack.com/apps and click on the Create New App button. In the Create an app window select the From scratch option. Then, give your application a name and select the appropriate workspace. From the Basic Information page click on the Incoming Webhooks option, turn it on and click on the Add New Webhook to Workspace button at the bottom. On the next page, use the Search for a channel... drop-down list to select the desired channel where you want to send notifications. When ready, click on the Allow button. Take note of the Webhook URL value displayed on the page. You will be using it in the next section. Next you will tell Alertmanager how to send Slack notifications. Open the docs/04-production/assets/manifests/prom-stack-values-v35.5.1.yaml file provided and uncomment the alertmaanager.config block. Make sure to update the <> placeholders accordingly. The definition should look like: Click to expand the alertmanager config alertmanager : enabled : true config : global : resolve_timeout : 5m slack_api_url : \"<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>\" route : receiver : \"null\" repeat_interval : 12h routes : - receiver : \"slack-notifications\" matchers : - alertname=\"OnlineBoutiqueInstanceDown\" continue : false receivers : - name : \"null\" - name : \"slack-notifications\" slack_configs : - channel : \"#<YOUR_SLACK_CHANNEL_NAME_HERE>\" send_resolved : true title : \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ end }}\" text : \"{{ range .Alerts }}{{ .Annotations.description }}\\n{{ end }}\" Apply the new settings using Helm : HELM_CHART_VERSION = \"35.5.1\" helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version \" ${ HELM_CHART_VERSION } \" \\ --namespace monitoring \\ -f \"docs/04-production/assets/manifests/prom-stack-values-v ${ HELM_CHART_VERSION } .yaml\" !!! info At this point you should only receieve alerts from the matching OnlinqBoutiqueInstanceDown alertname. Since the continue is set to false Alertmanager will only send notifications from this alert and stop sending for others. Clicking on the notification name in Slack will open a web browser to an unreachable web page with the internal Kubernetes DNS of the Alertmanager pod. This is expected. For more information you can check out this article . For additional information about the configuration parameters for Alertmanager you can check out this doc . You can also at some notification examples in this article .","title":"Configuring Alertmanager to Send Notifications to Slack"},{"location":"04-production/observability-production/#setting-up-event-exporter-for-events-retention","text":"A Kubernetes event is an object that shows what\u2019s happening inside a cluster, node, pod, or container. These objects are usually generated in response to changes that occur inside your K8s system. The Kubernetes API Server enables all core components to create these events. Generally, each event is accompanied by a log message as well. Event objects are not regular log events, therefore the Kubernetes logs do not include them. Kubernetes has no builtin support to store or forward these events on the long term, and they are cleaned up after a short retention time defaulting to just 1 hour. In this section you will learn how to configure the Kubernetes Events Exporter and collect and perist those events using Loki . Create the ServiceAccount , ClusterRole and ClusterRoleBinding using kubectl : The manifest file looks like the following: Click to expand the manifest file apiVersion : v1 kind : Namespace metadata : name : event-exporter --- apiVersion : v1 kind : ServiceAccount metadata : namespace : event-exporter name : event-exporter --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : event-exporter roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount namespace : event-exporter name : event-exporter kubectl apply -f docs/04-production/assets/manifests/event-exporter-roles.yaml Create the event exporter config using kubectl : The manifest file for the config looks like the following: Click to expand the config manifest file apiVersion : v1 kind : ConfigMap metadata : name : event-exporter-cfg namespace : event-exporter data : config.yaml : | logLevel : error logFormat : json route : routes : - match : - receiver : \"dump\" receivers : - name : \"dump\" stdout : {} kubectl apply -f docs/04-production/assets/manifests/event-exporter-config.yaml Finally, create the event exporter deployment using kubectl: The manifest file for the deployment looks like the following: Click to expand the deployment manifest file apiVersion : apps/v1 kind : Deployment metadata : name : event-exporter namespace : event-exporter spec : replicas : 1 template : metadata : labels : app : event-exporter version : v1 spec : serviceAccountName : event-exporter containers : - name : event-exporter image : ghcr.io/resmoio/kubernetes-event-exporter:latest imagePullPolicy : IfNotPresent args : - -conf=/data/config.yaml volumeMounts : - mountPath : /data name : cfg volumes : - name : cfg configMap : name : event-exporter-cfg selector : matchLabels : app : event-exporter version : v1 kubectl apply -f docs/04-production/assets/manifests/event-exporter-deployment.yaml","title":"Setting up Event Exporter for events retention"},{"location":"04-production/observability-production/#viewing-events-in-grafana","text":"If the installation went well and no errors were reported you should start seeing event start to flow in Grafana. Connect to Grafana (using default credentials: admin/prom-operator ) by port forwarding to local machine: kubectl --namespace monitoring port-forward svc/kube-prom-stack-grafana 3000 :80 Open a web browser on localhost:3000 . Once in, you can go to Explore menu and select the Loki as a datasource. In the Log browser input enter the following: { app= \"event-exporter\" } Info You should see events in the Logs section. Any of the fields in the Detected fields section of a log detail view can be used to query. For example you can perform a query using the pod name and view specific logs for a certain pod: {app=\"event-exporter\"} |= \"shippingservice-79bdd5f858-gqm6g\" . For a more in depth explanation on the Obervability topic you can check out the Kubernetes Starter Kit Tutorial . Next, you will learn how to configure the CI/CD process and associated GitHub workflows for all project components used in this guide.","title":"Viewing events in Grafana"},{"location":"04-production/setup-doks-production/","text":"Introduction This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as the production environment, targeting the online boutique sample application used as a reference in this guide. A Kubernetes environment is referred to as production-ready when it has everything needed to serve traffic to real end users and has the resources to adapt to changing demands. A production environment should be secure, scalable, highly available and reliable, and must provide logging and monitoring capabilities that meet organizational requirements. Prerequisites To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section. Provisioning a Production DOKS Cluster for Microservices In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-production , with a pool size of 4 nodes , auto-scale to 3-5 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-production \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=4;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=3;max-nodes=5\" \\ --region nyc1 Note The example cluster created above is using 4 nodes, each having 2vCPU/4GB size, which amounts to 94$/month . For simplicity and consistency through all the guide, the microservices-demo-production name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation . Configuring DOKS for Private Registries From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your production cluster using Kustomize .","title":"Set up Production DOKS"},{"location":"04-production/setup-doks-production/#introduction","text":"This section will show you how to create a DigitalOcean Kubernetes Cluster ( DOKS ) cluster which will be used as the production environment, targeting the online boutique sample application used as a reference in this guide. A Kubernetes environment is referred to as production-ready when it has everything needed to serve traffic to real end users and has the resources to adapt to changing demands. A production environment should be secure, scalable, highly available and reliable, and must provide logging and monitoring capabilities that meet organizational requirements.","title":"Introduction"},{"location":"04-production/setup-doks-production/#prerequisites","text":"To complete this section you will need: Doctl utility already installed as explained in the Installing Required Tools -> Doctl section. Make sure that you're authenticated with the DigitalOcean API as explained in the Authenticating with the DigitalOcean API section.","title":"Prerequisites"},{"location":"04-production/setup-doks-production/#provisioning-a-production-doks-cluster-for-microservices","text":"In this step, you will create a new Kubernetes cluster running on the DigitalOcean platform, using the doctl utility. Following command will create a DigitalOcean Kubernetes cluster named microservices-demo-production , with a pool size of 4 nodes , auto-scale to 3-5 each having 2 vCPUs and 4gbGB of RAM, in the nyc1 region: doctl k8s cluster create microservices-demo-production \\ --auto-upgrade = true \\ --maintenance-window \"saturday=21:00\" \\ --node-pool \"name=basicnp;size=s-2vcpu-4gb-amd;count=4;tag=cluster2;label=type=basic;auto-scale=true;min-nodes=3;max-nodes=5\" \\ --region nyc1 Note The example cluster created above is using 4 nodes, each having 2vCPU/4GB size, which amounts to 94$/month . For simplicity and consistency through all the guide, the microservices-demo-production name was picked for the example cluster. You can choose any name you like, but you need to make sure the naming convention stays consistent. It is recommended to use a region for your cluster that is closest to you for faster interaction. Run the following command - doctl k8s options regions to check available regions. Cluster auto upgrade is enabled ( --auto-upgrade=true ). Kubernetes clusters should be auto-upgraded to ensure that they always contain the latest security patches. Next, you can verify the cluster details. First, fetch your DOKS cluster ID : doctl k8s cluster list Finally, check if the kubectl context was set to point to your DOKS cluster. The doctl utility should do this automatically: kubectl config current-context For more info on this topic please see this Kubernetes Starter Kit DOKS Creation .","title":"Provisioning a Production DOKS Cluster for Microservices"},{"location":"04-production/setup-doks-production/#configuring-doks-for-private-registries","text":"From the command line run the following: doctl registry kubernetes-manifest | kubectl apply -f - This will configure your DOKS cluster to fetch images from your DOCR created in the Set up a DigitalOcean container registry section This step can also be achieved via the DigitalOcean cloud console. Please follow this guide . Next, you will learn how to deploy the online boutique sample application to your production cluster using Kustomize .","title":"Configuring DOKS for Private Registries"},{"location":"04-production/setup-ingress-production/","text":"Introduction In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications. Prerequisites To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy. Installing the Nginx Ingress Controller In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_PRODUCTION_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-prod \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-prod namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-prod spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-prod should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-prod to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-prod spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-prod Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-prod namespace : microservices-demo-prod spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Set up ingress"},{"location":"04-production/setup-ingress-production/#introduction","text":"In this section, you will install and configure the Kubernetes-maintained version of the Nginx Ingress Controller. Then, you're going to issue TLS certificates for your hosts (thus enabling TLS termination), and route traffic to your backend applications.","title":"Introduction"},{"location":"04-production/setup-ingress-production/#prerequisites","text":"To complete this section you will need: Helm installed as explained in the Installing required tools section. A Kubernetes cluster (DOKS) up and running as explained in the Set up DOKS section. The online boutique sample application deployed to your cluster as explained in the Deploying the app section. A valid domain available and configured to point to DigitalOcean name servers. More information is available in this article . Digital Ocean is not a domain registrar, so you will need to purchase the domain from a well known vendor, such as GoDaddy.","title":"Prerequisites"},{"location":"04-production/setup-ingress-production/#installing-the-nginx-ingress-controller","text":"In this section you will install the community maintained version of the Nginx ingress controller. Please follow below steps to install Nginx using Helm: Add the Ingress Nginx Helm repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update ingress-nginx helm search repo ingress-nginx Install the Nginx Ingress Controller using Helm : helm install ingress-nginx ingress-nginx/ingress-nginx --version 4 .1.3 \\ --namespace ingress-nginx \\ --create-namespace Note To check if the installation was successful, run the helm ls -n ingress-nginx command, and confirm the deployment status. Configure DNS for Nginx Ingress Controller: doctl compute domain create <YOUR_DOMAIN_NAME> Info Please note that this domain matches the domain you purchased in the Prerequisites section. You will use this domain to create additional sub-domains to use with the microservices app you will deploy in this section. You can also use the domain you already created in the Development ingress setup section. Create an A record for your host (make sure to replace the <> placeholders first): doctl compute domain records create <YOUR_DOMAIN_NAME> \\ --record-type \"A\" --record-name <YOUR_RECORD_NAME> \\ --record-data \"<YOUR_PRODUCTION_LB_IP_ADDRESS>\" \\ --record-ttl \"30\" Add the Jetstack Helm repository: helm repo add jetstack https://charts.jetstack.io helm repo update jetstack Install the jetstack/cert-manager chart using Helm : helm install cert-manager jetstack/cert-manager --version 1 .8.0 \\ --namespace cert-manager \\ --create-namespace \\ --set installCRDs = true Note To check if the installation was succesfull you can run the helm ls -n cert-manager and confirm the deployment status. Create a Kubernetes Secret for the DigitalOcean Provider that cert-manager is going to use to perform the DNS-01 challenge using a DigitalOcean API token: DO_API_TOKEN = \"<YOUR_DO_API_TOKEN_HERE>\" kubectl create secret generic \"digitalocean-dns\" \\ --namespace microservices-demo-prod \\ --from-literal = access-token = \" $DO_API_TOKEN \" Note The secret must be created in the same namespace where the Issuer CRD is located - in this case the microservides-demo-prod namespace. Create an issuer resource for cert-manager using kubectl (make sure to replace the <> placeholders first): The issuer manifest file looks like the following: Click to expand issuer manifest file apiVersion : cert-manager.io/v1 kind : Issuer metadata : name : letsencrypt-nginx-wcard namespace : microservices-demo-prod spec : # ACME issuer configuration: # `email` - the email address to be associated with the ACME account (make sure it's a valid one). # `server` - the URL used to access the ACME server\u2019s directory endpoint. # `privateKeySecretRef` - Kubernetes Secret to store the automatically generated ACME account private key. acme : email : <YOUR_EMAIL_ADDRESS> server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-nginx-wcard-private # List of challenge solvers that will be used to solve ACME challenges for the matching domains. solvers : # Use the DigitalOcean DNS API to manage DNS01 challenge records. - dns01 : digitalocean : # Kubernetes secret that contains the DO API token . # Must be in the same namespace as the Issuer CRD. tokenSecretRef : name : digitalocean-dns key : access-token Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-issuer.yaml Info Running kubectl get issuer letsencrypt-nginx-wcard -n microservices-demo-prod should result in the True value being displayed under the READY column. Note If the Issuer object displays a Not Ready state you can describe the object to get additional information using: kubectl describe issuer letsencrypt-nginx-wcard -n microservices-demo-prod to get more information. Create the wildcard certificates resource using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The certificate manifest file looks like the following: Click to expland the certificate resource apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : <YOUR_DOMAIN_NAME> # Cert-Manager will put the resulting Secret in the same Kubernetes namespace as the Certificate. namespace : microservices-demo-prod spec : # Secret name to create, where the private key and certificate should be stored. secretName : <YOUR_DOMAIN_NAME> # What Issuer to use for getting the certificate. issuerRef : name : letsencrypt-nginx-wcard kind : Issuer group : cert-manager.io # Common name to be used on the Certificate. commonName : \"*.<YOUR_DOMAIN_NAME>\" # List of DNS subjectAltNames to be set on the Certificate. dnsNames : - \"<YOUR_DOMAIN_NAME>\" - \"*.<YOUR_DOMAIN_NAME>\" Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/cert-manager-wildcard-certificate.yaml To verify the certificate status run: kubectl get certificate <YOUR_DOMAIN_NAME> -n microservices-demo-prod Info This may take a few minutes to complete. If the Certificate object displays a not ready state you can run: kubectl logs -l app=cert-manager,app.kubernetes.io/component=controller -n cert-manager Add the Ingress Nginx host using kubectl and the provided manifest file (make sure to replace the <> placeholders first): The ingress host manifest file looks like the following: Click to expand the ingress host resource apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-microservices-demo-prod namespace : microservices-demo-prod spec : tls : - hosts : - \"*.<YOUR_DOMAIN_NAME>\" secretName : <YOUR_DOMAIN_NAME> rules : - host : <YOUR_A_RECORD>.<YOUR_DOMAIN_NAME> http : paths : - path : / pathType : Prefix backend : service : name : frontend port : number : 80 ingressClassName : nginx Apply via kubectl: kubectl apply -f docs/04-production/assets/manifests/ingress-host.yaml Open a web browser and point to <YOUR_A_RECORD>.<YOUR_DOMAIN> . You should see the online boutique welcome page. The connection is secure and the certificate is a valid one issued by Let's Encrypt . Next you will install and configure the Prometheus stack for monitoring your DOKS cluster, Loki to fetch and aggregate logs from your cluster's resources and view them in Grafana and configure AlertManager to alert and notify when there is a critical issue in your cluster. You will also configure the events exporter tool to grab Kubernetes events and send and store them in Loki as they are a great way to monitor the health and activity of your K8s clusters.","title":"Installing the Nginx Ingress Controller"},{"location":"05-ci-cd/setup-continuous-deployments/","text":"Introduction After setting up the CI process, the next step is to configure automated (or continuous) deployments for your environments. In some setups, continuous deployments is not desired. In the end, it all drills down to how often you want to deliver release for your project. If the release cycle and cadence of your project is more agile and you want to deliver more releases in a short period of time, then it makes sense to have such an automated setup. In practice, this is not the only reason. You will want some environments such as the development environment to continuously reflect latest code changes of your main application repository branch. Here is where continuous deployments play an important role. Another important aspect is - how do you track each change and what is deployed where? To answer all above questions, a new concept is introduced called GitOps . GitOps is yet another set of methodologies and accompanying practices, allowing automated deployments and easy track of changes for all your application deployments to various environments. It relies on Git as the single source of truth. It means, you rely on the Git history feature to track all changes, and revert the system to a previous working state in case something goes bad. One popular solution used to implement GitOps principles is Argo CD , a free and open source project very well supported by the community. Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Where does Argo CD fit in? Argo CD sits at the very end of your CI process. It waits for changes to happen in your Git repository over it is watching. No need to create and maintain separate GitHub workflows for deploying application components to each environment. Just tell Argo about your GitHub repository, and what Kubernetes cluster to sync with. Then, let Argo CD do the heavy lifting. Do I need to create separate GitHub repositories and/or branches to sync each Kubernetes environment? The short answer is no. No, you don't have to do this. Because you already use Kustomize overlays in your project, you have full control over the deployment process to each environment. No need to create separate branches to target each environment, which is hard to maintain and not recommended. For small projects it is often enough to use a monorepo structure where you have both application code and Kubernetes configuration manifests. In case of bigger projects, it's best to split application code and Kubernetes stuff into separate repositories. It's easier to track application vs Kubernetes changes this way. This guides relies on a monorepo approach, but it should be relatively easy to migrate to a split repo setup because all Kubernetes manifests are kept in the kustomize subfolder. Do I need a separate Argo CD instance per environment or just one connecting all? You can go either one or the other. This guide is using a separate ArgoCD instance per environment. This kind of setup doesn't affect one environment or the other if one ArgoCD instance goes down, or even if one of the clusters is in a degraded state. Only the current environment where Argo operates is affected. This is called a decentralized setup. The only drawback of this configuration is that application specific resources and system specific resources operate in the same DOKS cluster which leads to additional CPU and/or memory usage. Another kind of setup (not covered in this guide) is where you have one dedicated DOKS instance (or a dedicated node pool) to serve this purpose. This is called a centralized setup where one Argo CD instance manages all environments from a single place. Main advantage is user application is now decoupled from system apps, and more resources become available for the app you're developing. Main disadvantage is additional costs for operating a dedicated cluster. Another drawback is possible Argo CD downtime for all environments if HA is not properly configured, or if the cluster is not properly sized to handle multiple Argo projects and applications. Following diagram depicts the Argo CD setup used in this guide for each environment (decentralized setup): It's important to understand Argo CD concepts, so please follow the official getting started guide . To keep it short, you need to know how to operate Argo applications and projects . The application CRD is the most important bit of configuration dealing with connecting various sources such as Git repositories and Kubernetes clusters. On the other hand, Argo CD projects represent a logical way of grouping multiple applications related to each other. This chapter relies on the Argo CD Autopilot project to bootstrap Argo itself, as well as example applications (i.e. microservices-demo ). Why use Argo CD autopilot? Some people argue that the app of apps pattern solves all problems related to bootstrapping Argo apps and projects. This is true, but there are some missing pieces: Argo CD has to be installed first (this is a one time operation usually, so maybe not a big deal). You have to create initial manifest(s) for Argo to consume (including the manifest implementing the app of apps pattern ). Create and layout your GitOps repository. Enforce a set of best practices for your GitOps repository. Commit and push required Argo manifests to your GitOps repository. Tell Argo about your intentions which requires kubectl apply for the involved manifests. The Argo CD autopilot project tries to solve all enumerated steps above. It uses an opinionated way to bootstrap your GitOps environment, and embeds a set of best practices to layout your Argo apps and projects in a GitOps fashion. On top of that, Argo CD is bootstrapped as well on your target cluster with just only one command. When needed, the autopilot CLI can be used to repeat the bootstrap process if your cluster is re-created, without pushing all manifests again to your existing repo (a special flag exists, called --recover ). To summarize, here's what Argo CD autopilot can do for you: Creates a GitOps repository (if doesn't exists) using an opinionated approach for your Argo apps and projects. Bootstraps Argo CD on your DOKS cluster, and keeps itself in sync using same GitOps repo. Helps you create apps and projects which are then automatically synced by your Argo instance (no need to run kubectl apply ). Enforces a set of best practices for all operations. Next, you will learn how to bootstrap Argo CD for each environment using the autopilot CLI. The procedure is basically the same, so once you learn how to do it for one environment, it should be pretty straightforward to perform the same steps for the remaining ones. Prerequisites To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. Also, make sure the initial version ( v1.0.0 ) for the demo application is already pushed to your DOCR as explained in the same chapter. A DOKS cluster set up and running for each environment: Development Environment -> Set up DOKS Staging Environment -> Set up DOKS Production Environment -> Set up DOKS The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Argo CD Autopilot CLI installed for your distribution as explained in the official docs. A GitHub Personal Access Token (or PAT for short) with the repo permissions set. It is required only once by the autopilot CLI to bootstrap Argo CD to your cluster for each environment. Bootstrapping Argo CD for the Development Environment In this section you will deploy Argo CD to your development DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync application changes using the dev overlay folder from your microservices-demo GitHub repository. The Argo CD autopilot project aims to ease the initial bootstrapping process of your Argo instance for each environment. What is nice about this approach is that the Argo installation itself is also synced with your GitHub repo in a GitOps fashion. The autopilot CLI will first deploy Argo CD in your cluster, and then push all required manifests to your GitHub repository. Bootstrap Instructions Note You may need to temporarily disable main branch protection to perform the following steps. Please follow below steps to bootstrap Argo CD, and deploy the microservices-demo app to the development environment : Export the GIT_TOKEN environment variable containing your GitHub personal access token (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-dev cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-dev Bootstrap Argo CD for the development DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/dev , containing all manifests for the Argo CD dev environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD dev instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 15m argocd-dex-server 1/1 1 1 15m argocd-notifications-controller 1/1 1 1 15m argocd-redis 1/1 1 1 15m argocd-repo-server 1/1 1 1 15m argocd-server 1/1 1 1 15m All Argo CD deployments must be healthy and running. Create an Argo project for the dev environment (make sure to replace the <> placeholders first): argocd-autopilot project create dev \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize dev overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/dev\" \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" \\ --project dev \\ --type kustomize Explanations for the above command: --app - the application specifier. Here, autopilot CLI expects the repository URL and path to sync in the following format - https://github.com/<GITHUB_USERNAME>/<REPO_NAME>/<APP_PATH> . Above example uses the kustomize/dev overlay path for the microservices-demo repo because that's what renders the final application manifests for the specific environment. --repo - the repository URL and path where autopilot CLI commits the Argo manifests representing applications, projects, etc. Here, the same repository URL is being used, except the directory path - argocd/dev . In a separate note down below, you will find the main reason why this approach is used. --project - Argo project name to use for the new application (created in the previous step). The given project name reflects the environment name - dev . This may seem confusing at the beginning because you already defined a dev env in the kustomize/dev path from the root directory of your microservices-demo repo. It seems natural to use environment names to define Argo CD projects, but in the end you can pick any name that fits best to describe your project. --type - tells ArgoCD the application type (kustomize or directory). The autopilot CLI is able to infer the application type by looking at the specified repo path, but most of the time it's better to be explicit rather than implicit (or let the tool guess). Note Each environment is nested using the argocd folder from the root directory of your microservices-demo project. This guide is using a monorepo approach to sync all environments, hence the reason. By default, autopilot CLI puts everything in the root directory of your GitHub repo resulting in a mess if no path is specified. Now, check if Argo created the microservices-demo application. First, port-forward the Argo web interface: kubectl port-forward -n argocd svc/argocd-server 8080 :80 Open the Argo CD dashboard in your web browser using this link - localhost:8080 : If everything went well, you should see the dev-microservices-demo app created and synced successfully. Note Argo CD is using this naming convention - <project_name>-<app_name> , hence the dev-microservices-demo name for the application. Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger, hence changes are not propagated instantly. Next, click on the dev-microservices-demo app tile - you should see the online boutique application composition (microservices): Finally, port-forward the frontend service to check the online boutique application status: kubectl port-forward -n microservices-demo-dev svc/frontend 9090 :80 Open a web browser pointing to localhost:9090 - you should see the online boutique application landing page. Tip You should see the previous changes that you made in the Set up continuous integration -> Testing the Online Boutique Application GitHub Workflows chapter applied as well. Understanding GitOps Repository Layout The Argo CD autopilot project is using an opinionated approach to structure your GitOps repository. It does this by creating dedicated folders to store your Argo applications, projects, etc. Following directory structure is created for you automatically in the argocd/dev subfolder (this is the base folder used by Argo to sync your development environment): argocd/dev/ \u251c\u2500\u2500 apps \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 microservices-demo \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 overlays \u2502 \u2514\u2500\u2500 dev \u2502 \u251c\u2500\u2500 config.json \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap \u2502 \u251c\u2500\u2500 argo-cd \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 argo-cd.yaml \u2502 \u251c\u2500\u2500 cluster-resources \u2502 \u2502 \u251c\u2500\u2500 in-cluster \u2502 \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u2502 \u2514\u2500\u2500 argocd-ns.yaml \u2502 \u2502 \u2514\u2500\u2500 in-cluster.json \u2502 \u251c\u2500\u2500 cluster-resources.yaml \u2502 \u2514\u2500\u2500 root.yaml \u2514\u2500\u2500 projects \u251c\u2500\u2500 README.md \u2514\u2500\u2500 dev.yaml Explanations for the above structure: apps - this is the main subfolder where you store all your Argo applications such as the microservice-demo project used in this guide. Each application you create in this subfolder is automatically synced by Argo to your target cluster. You'll also find a README file in this folder with additional explanations (automatically generated by the autopilot CLI). bootstrap - this is where Argo stores all manifests used to bootstrap itself. Usually you don't need to touch this folder unless upgrading Argo to a newer version. projects - this is the main subfolder where you store all your Argo projects. Projects are a way to logically group related Argo applications stored in the apps folder. You'll also find a README file in this folder with additional explanations (automatically generated by the autopilot CLI). How does Argo know how to sync your microservices-demo app for the dev environment? Argo is watching over the argocd/dev path from your repo for changes. The microservices-demo app is stored under the same directory. If you take a look at the argocd/dev/apps/microservices-demo/base/kustomization.yaml manifest, you will see that it points to the kustomize/dev path from the main repo: apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/dev So, whenever you commit something in the kustomize/dev path, Argo will automatically pick up the changes, and sync the microservices-demo application for your dev environment. Managing Argo CD Applications Argo CD autopilot can be used for adding (or bootstrapping) new applications whenever needed. However, when you need to change or upgrade existing apps it is advised to use Git operations to perform changes (via pull requests). In other words, use GitOps practices. Next, you will perform the same steps to bootstrap Argo CD for the staging environment. Bootstrapping Argo CD for the Staging Environment In this section you will deploy Argo CD to your staging DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync application changes using the staging overlay folder from your microservices-demo GitHub repository. Note You may need to temporarily disable main branch protection to perform the following steps. Please follow below steps to bootstrap Argo CD, and deploy the microservices-demo app to the staging environment : Export the GIT_TOKEN environment variable containing your GitHub personal access token, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-staging cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-staging Bootstrap Argo CD for the staging DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/staging , containing all manifests for the Argo CD staging environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD staging instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 10m argocd-dex-server 1/1 1 1 10m argocd-notifications-controller 1/1 1 1 10m argocd-redis 1/1 1 1 10m argocd-repo-server 1/1 1 1 10m argocd-server 1/1 1 1 10m All Argo CD deployments must be healthy and running. Create an Argo project for the staging environment (make sure to replace the <> placeholders first): argocd-autopilot project create staging \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize staging overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/staging\" \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" \\ --project staging \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger, hence changes are not propagated instantly. The provisioned GitOps repository layout structure is similar to the dev environment, the only difference being the name. Of course, you will have environment specific details stored as well. Managing Argo CD applications goes the same way. Next, you will perform the same steps to bootstrap Argo CD for the production environment. Bootstrapping Argo CD for the Production Environment In this section you will deploy Argo CD to your production DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync application changes using the prod overlay folder from your microservices-demo GitHub repository. Note You may need to temporarily disable main branch protection to perform the following steps. Please follow below steps to bootstrap Argo CD, and deploy the microservices-demo app to the production environment : Export the GIT_TOKEN environment variable containing your GitHub personal access token, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-production cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-production Bootstrap Argo CD for the production DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/prod , containing all manifests for the Argo CD production environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD production instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 5m argocd-dex-server 1/1 1 1 5m argocd-notifications-controller 1/1 1 1 5m argocd-redis 1/1 1 1 5m argocd-repo-server 1/1 1 1 5m argocd-server 1/1 1 1 5m All Argo CD deployments must be healthy and running. Create an Argo project for the production environment (make sure to replace the <> placeholders first): argocd-autopilot project create prod \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize prod overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/prod\" \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" \\ --project prod \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger, hence changes are not propagated instantly. The provisioned GitOps repository layout structure is similar to the dev or staging environment, the only difference being the name. Of course, you will have environment specific details stored as well. Managing Argo CD applications goes the same way. Testing the Final Setup As an exercise, go ahead and make a change to one or even more microservices. Then, open a PR and check the associated workflow. Next, approve the PR and merge change into main branch. Check the main branch GitHub workflow as it progresses. When finished, check if Argo propagated the latest changes to your development DOKS cluster. Reverting Bad Application Deployments Up to this point you tested only the happy CI/CD flows for the main application. In reality things may go wrong, hence it's important to know how to deal with this kind of situations as well. Because you're already using GitOps it should be easy to revert changes in case something goes bad. The only place where you need to perform changes is the GitHub repository hosting your application. Moving forward, everything is controlled via pull requests because you already set main branch protection rules. So, the process doesn't get out of control because no one pushes inadvertently to the main branch - this is very important to avoid configuration drifts, and bad things to happen. Still, even with all processes you already have in place, there's is a chance for human errors to slip. Thus, it is very important to know how to handle and recover from this situation. There are two options available: Use the GitHub PR revert feature . It is a feature present in the GitHub user interface which creates a new PR that contains one revert of the merge commit from the original merged pull request. After merging the PR into main branch, Argo CD will pickup changes and deploy the previous images for the application. This approach has a few pros and cons: It is easier to revert back to the original state via single button in the GitHub user interface. It feels natural, just as you would do using the Undo feature from your IDE, or any other desktop application. This aspect falls into the pros category. It will trigger the whole set of CI workflows again. First, the PR workflow, and then the main branch workflow which rebuilds the same set of images basically but using a different commit ID. In the end, you will achieve the final goal and revert things back to their initial state, but the outcome is unnecessary images being built and additional waiting time (things may break in between also). This aspect falls more into the cons category. You have more control over the process, meaning you also revert the whole batch of changes for application code which may be a desired thing or not. This aspect falls more or less into the pros category. Another option is to create a fresh PR, and revert only the kustomize changes to switch to the previous deployment that worked. This approach is more lightweight and doesn't trigger a bunch of GitHub workflows as in the first approach. On the other hand, defective application code stays in place. But, the main advantage is that you revert your application to a working state very quickly, thus it creates minimum application downtime. If choosing the second option, you have time to prepare a set of fixes (or hot-fixes) meanwhile. When everything is ready and you feel confident about your work, go as usual with the CI flow. Open a new PR with your code changes, obtain approval, merge into main branch, Argo CD picks up and updates the application in the development cluster. The first approach is already set up, so if you need to go that route there's nothing new to learn or implement. Following example is based on the second approach. Steps to revert a bad deployment for the development environment (applies to upper environments as well): Identify the problematic commit ID in your application GitHub repository. You should be able to spot it quickly because it contains the following signature: Navigate to the respective commit ID, and see what changed: Create a new PR containing changes with the previous image tag for each affected microservice (the value highlighted using red color in the Git diff shown in the above image). Wait for the Kustomize manifests validation workflow to finish, and if everything is alright approve and merge the PR. After a few moments (3 minutes max), you should see the old version of the microservices-demo application present in your development environment. Restoring a Lost Argo CD Instance If for some reason the DOKS cluster is re-created and the associaged Argo CD instance is lost, then you should be able to recover from this situation using your existing GitOps repository (assuming it is still intact). It's a feature offered by the Argo CD autopilot CLI via a special flag called --recover . Note Don't forget to integrate your new DOKS cluster with DOCR before anything else. If you forget to perform this step, all Kubernetes deployments will fail to pull application images from your registry. Assuming you want to recover the staging environment Argo CD instance, please follow below steps: Export the GIT_TOKEN environment variable containing your GitHub personal access token, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your staging cluster. Below example is using do-nyc1-microservices-demo-staging for demonstration - make sure to change value if yours is different: kubectl config set-context do -nyc1-microservices-demo-staging Run the autopilot bootstrap using the --recover flag this time (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" \\ --recover What the above command does is it will clone your GitOps repository first as specified by the --repo flag. Then, it will install a fresh Argo CD instance using environment specific manifests pointed by the last part of the repository path, i.e. argocd/staging . Environment specific applications and projects should be restored as well. GitOps repository Kustomize manifests should remain untouched. So far, you learned how to configure and enable an automated CI/CD flow for the development environment. Next, you will learn how to create GitHub releases for your application and propagate (or promote) changes to upper environments as well. First to the staging environment, and then after QA approval (may imply project manager decision as well), deploy to production.","title":"Set up continuous deployments"},{"location":"05-ci-cd/setup-continuous-deployments/#introduction","text":"After setting up the CI process, the next step is to configure automated (or continuous) deployments for your environments. In some setups, continuous deployments is not desired. In the end, it all drills down to how often you want to deliver release for your project. If the release cycle and cadence of your project is more agile and you want to deliver more releases in a short period of time, then it makes sense to have such an automated setup. In practice, this is not the only reason. You will want some environments such as the development environment to continuously reflect latest code changes of your main application repository branch. Here is where continuous deployments play an important role. Another important aspect is - how do you track each change and what is deployed where? To answer all above questions, a new concept is introduced called GitOps . GitOps is yet another set of methodologies and accompanying practices, allowing automated deployments and easy track of changes for all your application deployments to various environments. It relies on Git as the single source of truth. It means, you rely on the Git history feature to track all changes, and revert the system to a previous working state in case something goes bad. One popular solution used to implement GitOps principles is Argo CD , a free and open source project very well supported by the community. Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Where does Argo CD fit in? Argo CD sits at the very end of your CI process. It waits for changes to happen in your Git repository over it is watching. No need to create and maintain separate GitHub workflows for deploying application components to each environment. Just tell Argo about your GitHub repository, and what Kubernetes cluster to sync with. Then, let Argo CD do the heavy lifting. Do I need to create separate GitHub repositories and/or branches to sync each Kubernetes environment? The short answer is no. No, you don't have to do this. Because you already use Kustomize overlays in your project, you have full control over the deployment process to each environment. No need to create separate branches to target each environment, which is hard to maintain and not recommended. For small projects it is often enough to use a monorepo structure where you have both application code and Kubernetes configuration manifests. In case of bigger projects, it's best to split application code and Kubernetes stuff into separate repositories. It's easier to track application vs Kubernetes changes this way. This guides relies on a monorepo approach, but it should be relatively easy to migrate to a split repo setup because all Kubernetes manifests are kept in the kustomize subfolder. Do I need a separate Argo CD instance per environment or just one connecting all? You can go either one or the other. This guide is using a separate ArgoCD instance per environment. This kind of setup doesn't affect one environment or the other if one ArgoCD instance goes down, or even if one of the clusters is in a degraded state. Only the current environment where Argo operates is affected. This is called a decentralized setup. The only drawback of this configuration is that application specific resources and system specific resources operate in the same DOKS cluster which leads to additional CPU and/or memory usage. Another kind of setup (not covered in this guide) is where you have one dedicated DOKS instance (or a dedicated node pool) to serve this purpose. This is called a centralized setup where one Argo CD instance manages all environments from a single place. Main advantage is user application is now decoupled from system apps, and more resources become available for the app you're developing. Main disadvantage is additional costs for operating a dedicated cluster. Another drawback is possible Argo CD downtime for all environments if HA is not properly configured, or if the cluster is not properly sized to handle multiple Argo projects and applications. Following diagram depicts the Argo CD setup used in this guide for each environment (decentralized setup): It's important to understand Argo CD concepts, so please follow the official getting started guide . To keep it short, you need to know how to operate Argo applications and projects . The application CRD is the most important bit of configuration dealing with connecting various sources such as Git repositories and Kubernetes clusters. On the other hand, Argo CD projects represent a logical way of grouping multiple applications related to each other. This chapter relies on the Argo CD Autopilot project to bootstrap Argo itself, as well as example applications (i.e. microservices-demo ). Why use Argo CD autopilot? Some people argue that the app of apps pattern solves all problems related to bootstrapping Argo apps and projects. This is true, but there are some missing pieces: Argo CD has to be installed first (this is a one time operation usually, so maybe not a big deal). You have to create initial manifest(s) for Argo to consume (including the manifest implementing the app of apps pattern ). Create and layout your GitOps repository. Enforce a set of best practices for your GitOps repository. Commit and push required Argo manifests to your GitOps repository. Tell Argo about your intentions which requires kubectl apply for the involved manifests. The Argo CD autopilot project tries to solve all enumerated steps above. It uses an opinionated way to bootstrap your GitOps environment, and embeds a set of best practices to layout your Argo apps and projects in a GitOps fashion. On top of that, Argo CD is bootstrapped as well on your target cluster with just only one command. When needed, the autopilot CLI can be used to repeat the bootstrap process if your cluster is re-created, without pushing all manifests again to your existing repo (a special flag exists, called --recover ). To summarize, here's what Argo CD autopilot can do for you: Creates a GitOps repository (if doesn't exists) using an opinionated approach for your Argo apps and projects. Bootstraps Argo CD on your DOKS cluster, and keeps itself in sync using same GitOps repo. Helps you create apps and projects which are then automatically synced by your Argo instance (no need to run kubectl apply ). Enforces a set of best practices for all operations. Next, you will learn how to bootstrap Argo CD for each environment using the autopilot CLI. The procedure is basically the same, so once you learn how to do it for one environment, it should be pretty straightforward to perform the same steps for the remaining ones.","title":"Introduction"},{"location":"05-ci-cd/setup-continuous-deployments/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. Also, make sure the initial version ( v1.0.0 ) for the demo application is already pushed to your DOCR as explained in the same chapter. A DOKS cluster set up and running for each environment: Development Environment -> Set up DOKS Staging Environment -> Set up DOKS Production Environment -> Set up DOKS The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Argo CD Autopilot CLI installed for your distribution as explained in the official docs. A GitHub Personal Access Token (or PAT for short) with the repo permissions set. It is required only once by the autopilot CLI to bootstrap Argo CD to your cluster for each environment.","title":"Prerequisites"},{"location":"05-ci-cd/setup-continuous-deployments/#bootstrapping-argo-cd-for-the-development-environment","text":"In this section you will deploy Argo CD to your development DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync application changes using the dev overlay folder from your microservices-demo GitHub repository. The Argo CD autopilot project aims to ease the initial bootstrapping process of your Argo instance for each environment. What is nice about this approach is that the Argo installation itself is also synced with your GitHub repo in a GitOps fashion. The autopilot CLI will first deploy Argo CD in your cluster, and then push all required manifests to your GitHub repository.","title":"Bootstrapping Argo CD for the Development Environment"},{"location":"05-ci-cd/setup-continuous-deployments/#bootstrap-instructions","text":"Note You may need to temporarily disable main branch protection to perform the following steps. Please follow below steps to bootstrap Argo CD, and deploy the microservices-demo app to the development environment : Export the GIT_TOKEN environment variable containing your GitHub personal access token (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-dev cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-dev Bootstrap Argo CD for the development DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/dev , containing all manifests for the Argo CD dev environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD dev instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 15m argocd-dex-server 1/1 1 1 15m argocd-notifications-controller 1/1 1 1 15m argocd-redis 1/1 1 1 15m argocd-repo-server 1/1 1 1 15m argocd-server 1/1 1 1 15m All Argo CD deployments must be healthy and running. Create an Argo project for the dev environment (make sure to replace the <> placeholders first): argocd-autopilot project create dev \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize dev overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/dev\" \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/dev\" \\ --project dev \\ --type kustomize Explanations for the above command: --app - the application specifier. Here, autopilot CLI expects the repository URL and path to sync in the following format - https://github.com/<GITHUB_USERNAME>/<REPO_NAME>/<APP_PATH> . Above example uses the kustomize/dev overlay path for the microservices-demo repo because that's what renders the final application manifests for the specific environment. --repo - the repository URL and path where autopilot CLI commits the Argo manifests representing applications, projects, etc. Here, the same repository URL is being used, except the directory path - argocd/dev . In a separate note down below, you will find the main reason why this approach is used. --project - Argo project name to use for the new application (created in the previous step). The given project name reflects the environment name - dev . This may seem confusing at the beginning because you already defined a dev env in the kustomize/dev path from the root directory of your microservices-demo repo. It seems natural to use environment names to define Argo CD projects, but in the end you can pick any name that fits best to describe your project. --type - tells ArgoCD the application type (kustomize or directory). The autopilot CLI is able to infer the application type by looking at the specified repo path, but most of the time it's better to be explicit rather than implicit (or let the tool guess). Note Each environment is nested using the argocd folder from the root directory of your microservices-demo project. This guide is using a monorepo approach to sync all environments, hence the reason. By default, autopilot CLI puts everything in the root directory of your GitHub repo resulting in a mess if no path is specified. Now, check if Argo created the microservices-demo application. First, port-forward the Argo web interface: kubectl port-forward -n argocd svc/argocd-server 8080 :80 Open the Argo CD dashboard in your web browser using this link - localhost:8080 : If everything went well, you should see the dev-microservices-demo app created and synced successfully. Note Argo CD is using this naming convention - <project_name>-<app_name> , hence the dev-microservices-demo name for the application. Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger, hence changes are not propagated instantly. Next, click on the dev-microservices-demo app tile - you should see the online boutique application composition (microservices): Finally, port-forward the frontend service to check the online boutique application status: kubectl port-forward -n microservices-demo-dev svc/frontend 9090 :80 Open a web browser pointing to localhost:9090 - you should see the online boutique application landing page. Tip You should see the previous changes that you made in the Set up continuous integration -> Testing the Online Boutique Application GitHub Workflows chapter applied as well.","title":"Bootstrap Instructions"},{"location":"05-ci-cd/setup-continuous-deployments/#understanding-gitops-repository-layout","text":"The Argo CD autopilot project is using an opinionated approach to structure your GitOps repository. It does this by creating dedicated folders to store your Argo applications, projects, etc. Following directory structure is created for you automatically in the argocd/dev subfolder (this is the base folder used by Argo to sync your development environment): argocd/dev/ \u251c\u2500\u2500 apps \u2502 \u251c\u2500\u2500 README.md \u2502 \u2514\u2500\u2500 microservices-demo \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 overlays \u2502 \u2514\u2500\u2500 dev \u2502 \u251c\u2500\u2500 config.json \u2502 \u2514\u2500\u2500 kustomization.yaml \u251c\u2500\u2500 bootstrap \u2502 \u251c\u2500\u2500 argo-cd \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 argo-cd.yaml \u2502 \u251c\u2500\u2500 cluster-resources \u2502 \u2502 \u251c\u2500\u2500 in-cluster \u2502 \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u2502 \u2514\u2500\u2500 argocd-ns.yaml \u2502 \u2502 \u2514\u2500\u2500 in-cluster.json \u2502 \u251c\u2500\u2500 cluster-resources.yaml \u2502 \u2514\u2500\u2500 root.yaml \u2514\u2500\u2500 projects \u251c\u2500\u2500 README.md \u2514\u2500\u2500 dev.yaml Explanations for the above structure: apps - this is the main subfolder where you store all your Argo applications such as the microservice-demo project used in this guide. Each application you create in this subfolder is automatically synced by Argo to your target cluster. You'll also find a README file in this folder with additional explanations (automatically generated by the autopilot CLI). bootstrap - this is where Argo stores all manifests used to bootstrap itself. Usually you don't need to touch this folder unless upgrading Argo to a newer version. projects - this is the main subfolder where you store all your Argo projects. Projects are a way to logically group related Argo applications stored in the apps folder. You'll also find a README file in this folder with additional explanations (automatically generated by the autopilot CLI). How does Argo know how to sync your microservices-demo app for the dev environment? Argo is watching over the argocd/dev path from your repo for changes. The microservices-demo app is stored under the same directory. If you take a look at the argocd/dev/apps/microservices-demo/base/kustomization.yaml manifest, you will see that it points to the kustomize/dev path from the main repo: apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/dev So, whenever you commit something in the kustomize/dev path, Argo will automatically pick up the changes, and sync the microservices-demo application for your dev environment.","title":"Understanding GitOps Repository Layout"},{"location":"05-ci-cd/setup-continuous-deployments/#managing-argo-cd-applications","text":"Argo CD autopilot can be used for adding (or bootstrapping) new applications whenever needed. However, when you need to change or upgrade existing apps it is advised to use Git operations to perform changes (via pull requests). In other words, use GitOps practices. Next, you will perform the same steps to bootstrap Argo CD for the staging environment.","title":"Managing Argo CD Applications"},{"location":"05-ci-cd/setup-continuous-deployments/#bootstrapping-argo-cd-for-the-staging-environment","text":"In this section you will deploy Argo CD to your staging DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync application changes using the staging overlay folder from your microservices-demo GitHub repository. Note You may need to temporarily disable main branch protection to perform the following steps. Please follow below steps to bootstrap Argo CD, and deploy the microservices-demo app to the staging environment : Export the GIT_TOKEN environment variable containing your GitHub personal access token, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-staging cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-staging Bootstrap Argo CD for the staging DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/staging , containing all manifests for the Argo CD staging environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD staging instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 10m argocd-dex-server 1/1 1 1 10m argocd-notifications-controller 1/1 1 1 10m argocd-redis 1/1 1 1 10m argocd-repo-server 1/1 1 1 10m argocd-server 1/1 1 1 10m All Argo CD deployments must be healthy and running. Create an Argo project for the staging environment (make sure to replace the <> placeholders first): argocd-autopilot project create staging \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize staging overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/staging\" \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" \\ --project staging \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger, hence changes are not propagated instantly. The provisioned GitOps repository layout structure is similar to the dev environment, the only difference being the name. Of course, you will have environment specific details stored as well. Managing Argo CD applications goes the same way. Next, you will perform the same steps to bootstrap Argo CD for the production environment.","title":"Bootstrapping Argo CD for the Staging Environment"},{"location":"05-ci-cd/setup-continuous-deployments/#bootstrapping-argo-cd-for-the-production-environment","text":"In this section you will deploy Argo CD to your production DOKS cluster using the autopilot CLI . Then, you will configure Argo to sync application changes using the prod overlay folder from your microservices-demo GitHub repository. Note You may need to temporarily disable main branch protection to perform the following steps. Please follow below steps to bootstrap Argo CD, and deploy the microservices-demo app to the production environment : Export the GIT_TOKEN environment variable containing your GitHub personal access token, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your microservices-demo-production cluster (notice that cluster name is prefixed using the do-<region>- identifier): kubectl config set-context do -nyc1-microservices-demo-production Bootstrap Argo CD for the production DOKS cluster using the autopilot CLI. You need to do this only once on a fresh cluster (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Note Once the process finishes successfully you will receive instructions how to access the Argo CD web interface. Most important, you will get the admin user password - put it safe for later use. Also, a new folder should be present in your Git repository - argocd/prod , containing all manifests for the Argo CD production environment instance. The same folder is synced by Argo to keep itself up to date, thus following GitOps principles. Check if the Argo CD production instance is up and running: kubectl get deployments -n argocd The output looks similar to: NAME READY UP-TO-DATE AVAILABLE AGE argocd-applicationset-controller 1/1 1 1 5m argocd-dex-server 1/1 1 1 5m argocd-notifications-controller 1/1 1 1 5m argocd-redis 1/1 1 1 5m argocd-repo-server 1/1 1 1 5m argocd-server 1/1 1 1 5m All Argo CD deployments must be healthy and running. Create an Argo project for the production environment (make sure to replace the <> placeholders first): argocd-autopilot project create prod \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" Tip It's best practice to organize your Argo applications using projects. For example, you can use Argo projects to define environments, such as in the above example. Finally, create an Argo application for the microservices-demo project using the Kustomize prod overlay (make sure to replace the <> placeholders first): argocd-autopilot app create microservices-demo \\ --app \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/kustomize/prod\" \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/prod\" \\ --project prod \\ --type kustomize To verify the whole setup works, please use the same procedure as you already learned in the Bootstrapping Argo CD for the Development Environment section of this chapter. Note Please bear in mind that Argo CD is using the Git polling method by default which takes around 3 minutes to trigger, hence changes are not propagated instantly. The provisioned GitOps repository layout structure is similar to the dev or staging environment, the only difference being the name. Of course, you will have environment specific details stored as well. Managing Argo CD applications goes the same way.","title":"Bootstrapping Argo CD for the Production Environment"},{"location":"05-ci-cd/setup-continuous-deployments/#testing-the-final-setup","text":"As an exercise, go ahead and make a change to one or even more microservices. Then, open a PR and check the associated workflow. Next, approve the PR and merge change into main branch. Check the main branch GitHub workflow as it progresses. When finished, check if Argo propagated the latest changes to your development DOKS cluster.","title":"Testing the Final Setup"},{"location":"05-ci-cd/setup-continuous-deployments/#reverting-bad-application-deployments","text":"Up to this point you tested only the happy CI/CD flows for the main application. In reality things may go wrong, hence it's important to know how to deal with this kind of situations as well. Because you're already using GitOps it should be easy to revert changes in case something goes bad. The only place where you need to perform changes is the GitHub repository hosting your application. Moving forward, everything is controlled via pull requests because you already set main branch protection rules. So, the process doesn't get out of control because no one pushes inadvertently to the main branch - this is very important to avoid configuration drifts, and bad things to happen. Still, even with all processes you already have in place, there's is a chance for human errors to slip. Thus, it is very important to know how to handle and recover from this situation. There are two options available: Use the GitHub PR revert feature . It is a feature present in the GitHub user interface which creates a new PR that contains one revert of the merge commit from the original merged pull request. After merging the PR into main branch, Argo CD will pickup changes and deploy the previous images for the application. This approach has a few pros and cons: It is easier to revert back to the original state via single button in the GitHub user interface. It feels natural, just as you would do using the Undo feature from your IDE, or any other desktop application. This aspect falls into the pros category. It will trigger the whole set of CI workflows again. First, the PR workflow, and then the main branch workflow which rebuilds the same set of images basically but using a different commit ID. In the end, you will achieve the final goal and revert things back to their initial state, but the outcome is unnecessary images being built and additional waiting time (things may break in between also). This aspect falls more into the cons category. You have more control over the process, meaning you also revert the whole batch of changes for application code which may be a desired thing or not. This aspect falls more or less into the pros category. Another option is to create a fresh PR, and revert only the kustomize changes to switch to the previous deployment that worked. This approach is more lightweight and doesn't trigger a bunch of GitHub workflows as in the first approach. On the other hand, defective application code stays in place. But, the main advantage is that you revert your application to a working state very quickly, thus it creates minimum application downtime. If choosing the second option, you have time to prepare a set of fixes (or hot-fixes) meanwhile. When everything is ready and you feel confident about your work, go as usual with the CI flow. Open a new PR with your code changes, obtain approval, merge into main branch, Argo CD picks up and updates the application in the development cluster. The first approach is already set up, so if you need to go that route there's nothing new to learn or implement. Following example is based on the second approach. Steps to revert a bad deployment for the development environment (applies to upper environments as well): Identify the problematic commit ID in your application GitHub repository. You should be able to spot it quickly because it contains the following signature: Navigate to the respective commit ID, and see what changed: Create a new PR containing changes with the previous image tag for each affected microservice (the value highlighted using red color in the Git diff shown in the above image). Wait for the Kustomize manifests validation workflow to finish, and if everything is alright approve and merge the PR. After a few moments (3 minutes max), you should see the old version of the microservices-demo application present in your development environment.","title":"Reverting Bad Application Deployments"},{"location":"05-ci-cd/setup-continuous-deployments/#restoring-a-lost-argo-cd-instance","text":"If for some reason the DOKS cluster is re-created and the associaged Argo CD instance is lost, then you should be able to recover from this situation using your existing GitOps repository (assuming it is still intact). It's a feature offered by the Argo CD autopilot CLI via a special flag called --recover . Note Don't forget to integrate your new DOKS cluster with DOCR before anything else. If you forget to perform this step, all Kubernetes deployments will fail to pull application images from your registry. Assuming you want to recover the staging environment Argo CD instance, please follow below steps: Export the GIT_TOKEN environment variable containing your GitHub personal access token, if not already (make sure to replace the <> placeholders first): export GIT_TOKEN = <YOUR_GITHUB_PERSONAL_ACCESS_TOKEN_HERE> Switch Kubernetes context to your staging cluster. Below example is using do-nyc1-microservices-demo-staging for demonstration - make sure to change value if yours is different: kubectl config set-context do -nyc1-microservices-demo-staging Run the autopilot bootstrap using the --recover flag this time (make sure to replace the <> placeholders first): argocd-autopilot repo bootstrap \\ --repo \"https://github.com/<YOUR_GITHUB_USERNAME>/microservices-demo/argocd/staging\" \\ --recover What the above command does is it will clone your GitOps repository first as specified by the --repo flag. Then, it will install a fresh Argo CD instance using environment specific manifests pointed by the last part of the repository path, i.e. argocd/staging . Environment specific applications and projects should be restored as well. GitOps repository Kustomize manifests should remain untouched. So far, you learned how to configure and enable an automated CI/CD flow for the development environment. Next, you will learn how to create GitHub releases for your application and propagate (or promote) changes to upper environments as well. First to the staging environment, and then after QA approval (may imply project manager decision as well), deploy to production.","title":"Restoring a Lost Argo CD Instance"},{"location":"05-ci-cd/setup-continuous-integration/","text":"Introduction Continuous integration, or CI for short, is the ongoing process of continuously integrating code into the main or active development branch of a Git repository. Usually, each developer works on a piece of code dealing with a specific functionality. In the end, all pieces merge together via the CI process, thus creating the final product. One typical Git flow is depicted below: gitGraph commit commit branch feature1 checkout feature1 commit commit checkout main branch feature2 checkout feature2 commit commit commit checkout main merge feature1 id: \"PR merge feature1\" tag: \"NEW TAG - v1.0.0\" type: HIGHLIGHT commit checkout feature2 merge main checkout main merge feature2 id: \"PR merge feature2\" tag: \"NEW TAG - v1.0.1\" type: HIGHLIGHT commit The Git flow used in this guide is a simple one - no release branches, no feature branches, etc. It's just the main branch and release tags. Of course, each developer will work in the end on a feature, fixing a bug, etc, so a dedicated branch gets created. But, in the end everything is merged into the main branch via pull requests. There is a list of reviewers which is automatically assigned, as well as automated workflows to automatically validate or invalidate PRs. Then, when a certain amount of bug fixes and/or proposed features are merged in, the owner (or the project manager) of the application repository creates a release. This is a separate process and requires manual intervention from designated team members. In the release process, a new version and tag is created (using semantic versioning), as well as the accompanying changelog (or release notes). GitHub is the most popular solution for collaborative work, sharing open source ideas, and storing application code. It is more than just a frontend for the most reliable source code management tool - Git . It also empowers issue tracking, project management, and most important of all - CI automation. GitHub offers actions and workflows to help you achieve this goal, hence it's a very good candidate. In this section, you will learn how to leverage the power of GitHub actions to implement the CI process required by the online boutique sample application. Prerequisites To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. A development DOKS cluster set up and running as explained in the Development Environment -> Set up DOKS section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Main branch protection is enabled and configured for your repository. Also number of reviewers should be set to at least one. Main branch changes should be allowed only via pull requests . Your DigitalOcean authentication token stored as a GitHub secret named DIGITALOCEAN_ACCESS_TOKEN in the microservices-demo repository. Follow this guide to learn more about creating GitHub secrets. Online Boutique Application CI Flows Configuration The CI process used in this guide is comprised of two phases, each with a workflow associated: Developers open a new pull request (or PR for short). Then, automated tests run via a dedicated GitHub workflow, and validate or invalidate the PR. Also, an additional manual review step is required by setting the number of approvals in the main branch protection rules . A typical pull request flow is depicted below: graph TD A(New PR) --> B(Run Automated Tests) B --> C{Tests Passing?} C -- No --> D(Reject PR) C -- Yes --> E{PR Review Passed?} E -- No --> D E -- Yes ---> F(Merge PR) If the PR gets approved and code is merged to main branch, a second workflow runs which builds project assets, pushes them to a registry, and deploys the application to the development environment. Note In practice, it's safe to have automated deployments for the development environment. This way, developers or other team members can see immediate results, and test application behavior on the real environment (besides remote development with Tilt ). A typical main branch flow is depicted below: graph TD A(Close PR) --> B(Merge to Main Branch) B --> C(Run Integration Tests) C --> D{Tests Passing?} D -- No --> E(Reject Build) D -- Yes --> F(Build & Tag DEV Images) F --> G(Push DEV Images to Registry) G --> H(Deploy to DEV Environment) Next, you will learn how to implement the CI workflows used in this guide for the online boutique sample application . Configuring Pull Requests Workflow On each pull request a dedicated workflow is triggered responsible with running automated tests, part of PR validation step. Next, manual review is requested by the developer opening the PR. Reviewers can be assigned also automatically . Follow below steps to configure and enable main branch PR validation workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-pr-ci.yaml \\ -o .github/workflows/online-boutique-pr-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-pr-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-pr-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique PR CI workflow file name : Online Boutique PR CI on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on main branch PR events pull_request : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows for PRs # If disabled, leads to memory exhaustion on the DOKS dev cluster concurrency : pr-ci-dev # Global environment variables env : DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" CLUSTER_NAME : \"microservices-demo-dev\" REGION : \"nyc1\" K8S_NAMESPACE : \"microservices-demo-${{ github.event.pull_request.number }}\" PROJECT_NAME : \"online-boutique\" jobs : # Run unit tests in parallel using below matrix to cut down time # Unit tests are standalone and should not raise conflicts # Each microservice is written in a specific language, hence it's added to the matrix unit-tests : runs-on : ubuntu-latest strategy : matrix : include : - project_name : cartservice project_language : csharp - project_name : checkoutservice project_language : golang - project_name : currencyservice project_language : javascript - project_name : emailservice project_language : python - project_name : frontend project_language : golang - project_name : paymentservice project_language : javascript - project_name : productcatalogservice project_language : golang - project_name : recommendationservice project_language : python - project_name : shippingservice project_language : golang steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Set up Go env if : ${{ matrix.project_language == 'golang' }} uses : actions/setup-go@v3 with : go-version : \"1.19\" - name : Go Unit Tests if : ${{ matrix.project_language == 'golang' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} go test ) - name : Set up C# env if : ${{ matrix.project_language == 'csharp' }} uses : actions/setup-dotnet@v2 with : dotnet-version : \"6.0\" include-prerelease : true - name : C# Unit Tests if : ${{ matrix.project_language == 'csharp' }} timeout-minutes : 5 run : dotnet test src/${{ matrix.project_name }}/ - name : Set up NodeJS env if : ${{ matrix.project_language == 'javascript' }} uses : actions/setup-node@v3 with : node-version : 18 - name : Javascript Unit Tests if : ${{ matrix.project_language == 'javascript' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} npm install npm run test ) - name : Set up Python env if : ${{ matrix.project_language == 'python' }} uses : actions/setup-python@v3 with : python-version : \"3.7\" - name : Python Unit Tests if : ${{ matrix.project_language == 'python' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} pip install -r requirements.txt pytest ) # Run deployment tests (smoke tests) # You can add integration tests as well # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : needs : unit-tests runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 1200 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kubectl : \"1.25.2\" kustomize : \"4.5.7\" tilt : \"0.30.9\" - name : Configure kubectl for DOKS with short-lived credentials run : doctl kubernetes cluster kubeconfig save ${{ env.CLUSTER_NAME }} --expiry-seconds 1200 - name : Deploy microservices to DOKS timeout-minutes : 10 run : | # Bring all microservices up using Tilt and wait for all deployments cp tilt-resources/dev/tilt_config.json . tilt ci -- \\ --allowed_contexts \"do-${{ env.REGION }}-${{ env.CLUSTER_NAME }}\" \\ --default_registry \"${{ env.DOCR_ENDPOINT }}\" \\ --environment \"dev\" \\ --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Build loadgenerator image uses : docker/build-push-action@v3 with : context : \"src/loadgenerator\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/loadgenerator\" - name : Smoke tests timeout-minutes : 10 run : | # Prepare load generator # Inject workflow custom docker image sed -i \"s#<LOAD_GENERATOR_IMAGE>#${{ env.DOCR_ENDPOINT }}/loadgenerator#g\" loadgenerator.yaml # Deploy load generator kubectl apply -f loadgenerator.yaml -n ${{ env.K8S_NAMESPACE }} # Wait for load generator deployment to be ready kubectl wait --for=condition=available --timeout=60s deployment/loadgenerator -n ${{ env.K8S_NAMESPACE }} || { # Show why load generator failed to start echo \"[INFO] Load generator pod events\" kubectl describe pod -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -10 exit 1 } # Run smoke tests REQUEST_COUNT=\"0\" while [[ \"$REQUEST_COUNT\" -lt \"50\" ]]; do sleep 5 REQUEST_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $2}') done # ensure there are no errors hitting endpoints ERROR_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $3}' | sed \"s/[(][^)]*[)]//g\") if [[ \"$ERROR_COUNT\" -gt \"0\" ]]; then # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 exit 1 fi # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 working-directory : \"src/loadgenerator\" - name : Clean up Tilt microservices environment if : ${{ always() }} run : | # Remove all microservices and the namespace created by Tilt tilt down --delete-namespaces -- --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Clean up Tilt docker images from registry if : ${{ always() }} run : | # Remove Tilt docker images from registry for tilt_repo in $(docker images --format \"{{.Repository}}:{{.Tag}}\" | grep \"tilt-.*[a-z|0-9]\"); do repo_and_tag=\"${tilt_repo##*/}\" repo=\"${repo_and_tag%%:*}\" tag=\"${repo_and_tag##*:}\" echo \"[INFO] Deleting tag $tag from repo $repo ...\" doctl registry repository delete-tag \"$repo\" \"$tag\" -f done echo \"[INFO] Remember to run the DOCR garbage collector from time to time!\" Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" CLUSTER_NAME : \"<YOUR_DEV_DOKS_CLUSTER_NAME_HERE>\" REGION : \"<YOUR_DEV_DOKS_CLUSTER_REGION_HERE>\" K8S_NAMESPACE : \"<YOUR_PROJECT_CUSTOM_NAMESPACE_HERE>-${{ github.event.pull_request.number }}\" In order to run deployment tests, the example CI workflow provided in this guide creates a dedicated Kubernetes namespace to deploy microservices on your target DOKS development cluster. It is suffixed using the PR number which is unique across PRs. When it finishes, it will try to clean up all associated resources such as the provisioned Kubernetes namespace, and docker images from the registry. For docker registry, you still need to run garbage collection (not triggered automatically in the workflow because it puts the DOCR in read-only mode, hence other PR workflows will fail at the docker clean up step). Explanation for the above configuration: on.pull_request - triggers the Online Boutique PR CI workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique PR CI workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths-ignore - list of repository paths used for filtering. The Online Boutique PR CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. jobs - defines list of job to run inside the pipeline such as unit tests, deployment tests, etc. strategy.matrix - use a matrix build type. Runs the unit tests in parallel for each project type combination. This approach is a perfect match for projects using multiple components, such as microservices. It also cuts down the time required to build and test each component. Each element from the matrix sets the project name, and the language being used. steps - list of steps to execute as part of the workflow jobs. For each project component (or microservice), the following list of actions is executed: Code checkout, via actions/checkout@v3 . Specific tools are installed based on project language ( actions/setup-dotnet@v2 , actions/setup-go@v3 , etc). Unit tests are executed for each project. For deployment tests, the whole suite of microservices is deployed to the DEV environment in a dedicated namespace (unique across PRs) using Tilt. The dev profile is being used. Then, load tests are performed via the smoke tests step. Finally, clean up is performed. The microservices setup allocated for the PR is destroyed, including associated Docker images pushed to the registry. Following diagram shows the Online Boutique PR CI workflow composition: graph LR A(PR Workflow Start) --> B(Unit Tests Matrix) B --> C(Cart Service <br> Tests) B --> D(Checkout Service <br> Tests) B --> E(Currency Service <br> Tests) B --> F(Email Service <br> Tests) B --> G(Frontend Tests) B --> H(Payment Service <br> Tests) B --> I(Product Catalog Service <br> Tests) B --> J(Recommendation Service <br> Tests) B --> K(Shipping Service <br> Tests) C & D & E & F & G & H & I & J & K --> L(Deployment Tests) L --> M(Env Clean Up) Next, you learn how to configure and enable the main branch workflow that gets triggered whenever changes are pushed to the main branch after each PR merge. Configuring Main Branch Workflow Whenever a PR is closed and code is merged into the main branch, a dedicated GitHub workflow is triggered. Main purpose of the main branch workflow is to test the whole application as a whole after merging various features via integration tests. Then, it will build and tag images using latest commit id, push them to registry, and finally deploy everything to the development environment. Usually you want the development environment to continuously reflect the latest changes from the development (or main) branch. Note There's no point to run unit tests again after each PR merge. Unit testing deals with changes for the affected components only. This part it is already taken care in the PR workflow. What makes more sense is to run integration tests as part of the main branch CI flow. This way, you check if application functionality is impacted after merging features in. Adding a GitHub badge in the main project README is another great benefit, giving visual feedback for the latest build state - passing or failing. Follow below steps to configure and enable the main branch workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-main-ci.yaml \\ -o .github/workflows/online-boutique-main-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-main-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-main-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique CI Main workflow file name : Online Boutique CI Main on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on push to main events push : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows # Changes should be delivered one a time as code is merged into the main branch concurrency : main-ci-dev # Global environment variables env : CI_COMMIT_AUTHOR : \"GitHub CI Actions\" CI_COMMIT_AUTHOR_EMAIL : \"gh-ci-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" jobs : # There's no point to run unit tests again after PR merge # Unit testing deals with changes for the affected components, # and that it is already taken care in the PR workflow # What it makes sense, is to run integration tests, # to see if the whole application is impacted after merging the changes # # Run deployment tests (integration tests) # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Run integration tests run : | echo \"[INFO] Not implemented yet!\" # Build and push project images in parallel using a matrix strategy # Cuts down build time build-and-push-images : needs : deployment-tests runs-on : ubuntu-latest strategy : matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout code uses : actions/checkout@v3 - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 600 - name : Build and push image uses : docker/build-push-action@v3 with : # cartservice is an exception - Dockerfile is placed in src/cartservice/src subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.sha }}\" # Kustomize image field for each microservice present in the `src/` dir # Finally, commit changes to main branch and let ArgoCD take over afterwards apply-kustomize-changes : needs : build-and-push-images runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kustomize : \"4.5.7\" - name : Kustomize dev environment images run : | for microservice in src/*/; do microservice=\"$(basename $microservice)\" if [[ \"$microservice\" == \"loadgenerator\" ]]; then continue fi ( cd kustomize/dev kustomize edit set image $microservice=${{ env.DOCR_ENDPOINT }}/${microservice}:${{ github.sha }} ) done - name : Commit Kustomize manifests for dev env run : | git config --global user.name \"${{ env.CI_COMMIT_AUTHOR }}\" git config --global user.email \"${{ env.CI_COMMIT_AUTHOR_EMAIL }}\" git add kustomize/dev/ git commit -m \"[CI] Bump docker images tag to ${{ github.sha }}\" - name : Push changes uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note The deployment-tests job from the Online Boutique CI Main workflow example contains only a placeholder for integration tests. This part is implementation specific, and it is left for the user to configure. Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : CI_COMMIT_AUTHOR : \"CI GitHub Actions\" CI_COMMIT_AUTHOR_EMAIL : \"gh-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" Explanation for the above configuration: on.push - triggers the Online Boutique Main CI workflow on push events only. on.push.branches - triggers the Online Boutique Main CI workflow whenever a push event is detected for the specified list of branches. In this case only main branch is desired. on.push.paths-ignore - list of repository paths used for filtering. The Online Boutique Main CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. jobs - defines list of job to run inside the pipeline such as integration tests, build and push docker images, apply Kustomize changes, etc. steps - list of steps implementing workflow jobs logic. Following diagram shows the Online Boutique Main CI workflow composition: graph TD A(Main Branch Workflow Start) --> B(Deployment Tests) B --> C{Tests Passing?} C -- No --> D(Reject build) C -- Yes --> E(Build and Push Images) E --> F(Kustomize Dev Env Images Tag) F --> G(Commit Kustomize Changes) G -. Trigger .-> H(Argo CD Dev Env Sync) style H stroke-dasharray: 5 5 Note The final step from above flow chart - Argo CD Dev Env Sync is discussed and implemented in the next chapter - Continous Deployments . Next, you learn how to configure and enable the Kustomize manifests validation workflow that gets triggered whenever changes are requested via pull requests affecting the kustomize/** path in your GitHub repository. Infrastructure Manifests Validation GitHub Workflow So far you created CI workflows for main application code verification. What about Kubernetes manifests or infrastructure configuration files validation? This guide is using a monorepo structure, hence infrastructure configuration files live in the same repository as application code. This part is handled via a separate GitHub workflow called Online Boutique PR Kustomize Validation . Main role is to perform static checks (or linting) for all Kustomize manifests used in each overlay. It represents an additional step to validate or invalidate pull requests handling infra changes for each environment via Kustomize. Following diagram shows the Online Boutique PR Kustomize Validation workflow logic: graph TD A(New Kustomize PR) --> B(Run Validation Tests) B --> C{Tests Passing?} C -- No --> D(Reject PR) C -- Yes --> E{PR Review Passed?} E -- No --> D E -- Yes ---> F(Merge PR) Follow below steps to enable the Kustomize checks workflow for PRs: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-pr-kustomize-validation.yaml workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-pr-kustomize-validation.yaml \\ -o .github/workflows/online-boutique-pr-kustomize-validation.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-pr-kustomize-validation.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-pr-kustomize-validation.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique PR Kustomize Validation workflow file name : Online Boutique PR Kustomize Validation on : workflow_dispatch : pull_request : branches : - main paths : - \"kustomize/**\" env : KUBECONFORM_VERSION : \"0.5.0\" KUBERNETES_VERSION : \"1.24.4\" jobs : job : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up kubeconform run : | ( cd /tmp wget \"https://github.com/yannh/kubeconform/releases/download/v${{ env.KUBECONFORM_VERSION }}/kubeconform-linux-amd64.tar.gz\" tar xvf kubeconform-linux-amd64.tar.gz chmod u+x ./kubeconform ) - name : Kustomize linting using kubeconform run : | # Test each overlay using kubeconform for kustomize_overlay in kustomize/*/; do kustomize build \"$kustomize_overlay\" | \\ /tmp/kubeconform -kubernetes-version \"${{env.KUBERNETES_VERSION}}\" -summary -verbose done Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note Depending on your setup, you may want to adjust the following environment variable at the top of your workflow file: env : KUBERNETES_VERSION : \"1.24.4\" Above workflow will validate each Kubernetes manifest using Kubeconform after rendering via Kustomize. Kubeconform is a tool for validating Kubernetes YAML files using the OpenAPI specifications. Next, you will test each workflow to check if it meets the required functionality. Testing the Online Boutique Application GitHub Workflows In this section you will test each workflow functionality for each event - pull requests and main branch code merge. Testing the Pull Requests Workflow Follow below steps to test the PR workflow for the online boutique application: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Checkout to a new branch to add a new feature: git checkout -b features/musical_instruments Open the header template file for the frontend microservice using a text editor of your choice, preferably with HTML lint support. For example, you can use VS Code : code src/frontend/templates/header.html Change the Cymbal Shops string from title section to something different (e.g. Musical Instruments ): < title > {{ if $.is_cymbal_brand }} Musical Instruments {{ else }} Online Boutique {{ end }} </ title > Tip You can quickly spin up a local environment using Tilt to test the changes first, as explained in the local development using Tilt section. You will get live updates for each change which helps in the iterative development process. Save changes, commit and push new branch to remote: git commit -am \"New feature - musical instruments.\" git push origin features/musical_instruments Navigate to your GitHub repository page, and open a new pull request against main branch using the features/musical_instruments branch as the base. At this point, the Online Boutique PR CI should kick in. Branch merging should be blocked until the workflow finishes successfully, and at least one approval is present (explained in the prerequisites section): Next, navigate to the Actions tab of your GitHub repository and inspect PR workflow progress: If everything goes as planned, the workflow should pass and the PR can be merged. This is the happy flow. You should also check what happens if introducing a change that breaks things - change one of the unit tests so that it doesn't compile successfully. Next, you will check the main branch workflow and see if applies required kustomize changes for the development environment. Testing the Main Branch Workflow The second workflow should automatically start after merging code into main branch via the pull request created in the previous step. Navigate to the actions tab of your GitHub repository to see it in action: Note Automated deployments to the development environment cluster are disabled for now until configuring continuous deployments via ArgoCD in the next chapter. So, nothing gets deployed yet to the dev environment. You should also see a new commit added in the Git history of your repo. It shows kustomize changes for each image that was built and tagged using latest PR merge commit id: Testing the Infra Manifests GitHub Workflow This goes the same way as with the previous workflows testing, the only thing that's different is that it triggers only for changes present in the kustomize/ folder from your GitHub repository. You should be able to test it quickly by changing something in one of the Kustomize overlays of the online boutique project, and then create a new PR. Wait for the validation workflow to star, and check the progress. If everything goes well you should be able to merge the new PR. Tip You should also test what happens if you introduce changes breaking Kustomize functionality, such as a misspelled key name in one of the YAML manifests from the kustomize/ folder. The validation workflow should fail, and the associated PR rejected. Additional Best Practices Going further, you should be able to enhance the CI process even more by following some additional best practices, such as: Run tests only for affected microservices. Should improve workflow run time even more. Strategy matrix used to speed up actions inside the workflow such as running unit tests and building docker images is static. It is possible to create a strategy matrix with dynamic values. For example, the tilt_config.json file from the dev profile already contains the list of project microservices, so the matrix can read the values directly from the list. Use artifacts caching inside workflows whenever possible. Decouple application code and Kubernetes configuration by using a dedicated Git repository. Application code should not be dependent on Kubernetes configuration, hence it make sense to stay in a different repository. Also, whenever changes are required on the Kubernetes side, you don't need to open PRs and merge Kubernetes specific commits into the application repository. Next, ArgoCD should pickup the changes and deploy the application automatically to your development environment DOKS cluster. In order for this part to work you need to set it up in the following section.","title":"Set up continuous integration"},{"location":"05-ci-cd/setup-continuous-integration/#introduction","text":"Continuous integration, or CI for short, is the ongoing process of continuously integrating code into the main or active development branch of a Git repository. Usually, each developer works on a piece of code dealing with a specific functionality. In the end, all pieces merge together via the CI process, thus creating the final product. One typical Git flow is depicted below: gitGraph commit commit branch feature1 checkout feature1 commit commit checkout main branch feature2 checkout feature2 commit commit commit checkout main merge feature1 id: \"PR merge feature1\" tag: \"NEW TAG - v1.0.0\" type: HIGHLIGHT commit checkout feature2 merge main checkout main merge feature2 id: \"PR merge feature2\" tag: \"NEW TAG - v1.0.1\" type: HIGHLIGHT commit The Git flow used in this guide is a simple one - no release branches, no feature branches, etc. It's just the main branch and release tags. Of course, each developer will work in the end on a feature, fixing a bug, etc, so a dedicated branch gets created. But, in the end everything is merged into the main branch via pull requests. There is a list of reviewers which is automatically assigned, as well as automated workflows to automatically validate or invalidate PRs. Then, when a certain amount of bug fixes and/or proposed features are merged in, the owner (or the project manager) of the application repository creates a release. This is a separate process and requires manual intervention from designated team members. In the release process, a new version and tag is created (using semantic versioning), as well as the accompanying changelog (or release notes). GitHub is the most popular solution for collaborative work, sharing open source ideas, and storing application code. It is more than just a frontend for the most reliable source code management tool - Git . It also empowers issue tracking, project management, and most important of all - CI automation. GitHub offers actions and workflows to help you achieve this goal, hence it's a very good candidate. In this section, you will learn how to leverage the power of GitHub actions to implement the CI process required by the online boutique sample application.","title":"Introduction"},{"location":"05-ci-cd/setup-continuous-integration/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. A development DOKS cluster set up and running as explained in the Development Environment -> Set up DOKS section. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Main branch protection is enabled and configured for your repository. Also number of reviewers should be set to at least one. Main branch changes should be allowed only via pull requests . Your DigitalOcean authentication token stored as a GitHub secret named DIGITALOCEAN_ACCESS_TOKEN in the microservices-demo repository. Follow this guide to learn more about creating GitHub secrets.","title":"Prerequisites"},{"location":"05-ci-cd/setup-continuous-integration/#online-boutique-application-ci-flows-configuration","text":"The CI process used in this guide is comprised of two phases, each with a workflow associated: Developers open a new pull request (or PR for short). Then, automated tests run via a dedicated GitHub workflow, and validate or invalidate the PR. Also, an additional manual review step is required by setting the number of approvals in the main branch protection rules . A typical pull request flow is depicted below: graph TD A(New PR) --> B(Run Automated Tests) B --> C{Tests Passing?} C -- No --> D(Reject PR) C -- Yes --> E{PR Review Passed?} E -- No --> D E -- Yes ---> F(Merge PR) If the PR gets approved and code is merged to main branch, a second workflow runs which builds project assets, pushes them to a registry, and deploys the application to the development environment. Note In practice, it's safe to have automated deployments for the development environment. This way, developers or other team members can see immediate results, and test application behavior on the real environment (besides remote development with Tilt ). A typical main branch flow is depicted below: graph TD A(Close PR) --> B(Merge to Main Branch) B --> C(Run Integration Tests) C --> D{Tests Passing?} D -- No --> E(Reject Build) D -- Yes --> F(Build & Tag DEV Images) F --> G(Push DEV Images to Registry) G --> H(Deploy to DEV Environment) Next, you will learn how to implement the CI workflows used in this guide for the online boutique sample application .","title":"Online Boutique Application CI Flows Configuration"},{"location":"05-ci-cd/setup-continuous-integration/#configuring-pull-requests-workflow","text":"On each pull request a dedicated workflow is triggered responsible with running automated tests, part of PR validation step. Next, manual review is requested by the developer opening the PR. Reviewers can be assigned also automatically . Follow below steps to configure and enable main branch PR validation workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-pr-ci.yaml \\ -o .github/workflows/online-boutique-pr-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-pr-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-pr-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique PR CI workflow file name : Online Boutique PR CI on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on main branch PR events pull_request : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows for PRs # If disabled, leads to memory exhaustion on the DOKS dev cluster concurrency : pr-ci-dev # Global environment variables env : DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" CLUSTER_NAME : \"microservices-demo-dev\" REGION : \"nyc1\" K8S_NAMESPACE : \"microservices-demo-${{ github.event.pull_request.number }}\" PROJECT_NAME : \"online-boutique\" jobs : # Run unit tests in parallel using below matrix to cut down time # Unit tests are standalone and should not raise conflicts # Each microservice is written in a specific language, hence it's added to the matrix unit-tests : runs-on : ubuntu-latest strategy : matrix : include : - project_name : cartservice project_language : csharp - project_name : checkoutservice project_language : golang - project_name : currencyservice project_language : javascript - project_name : emailservice project_language : python - project_name : frontend project_language : golang - project_name : paymentservice project_language : javascript - project_name : productcatalogservice project_language : golang - project_name : recommendationservice project_language : python - project_name : shippingservice project_language : golang steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Set up Go env if : ${{ matrix.project_language == 'golang' }} uses : actions/setup-go@v3 with : go-version : \"1.19\" - name : Go Unit Tests if : ${{ matrix.project_language == 'golang' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} go test ) - name : Set up C# env if : ${{ matrix.project_language == 'csharp' }} uses : actions/setup-dotnet@v2 with : dotnet-version : \"6.0\" include-prerelease : true - name : C# Unit Tests if : ${{ matrix.project_language == 'csharp' }} timeout-minutes : 5 run : dotnet test src/${{ matrix.project_name }}/ - name : Set up NodeJS env if : ${{ matrix.project_language == 'javascript' }} uses : actions/setup-node@v3 with : node-version : 18 - name : Javascript Unit Tests if : ${{ matrix.project_language == 'javascript' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} npm install npm run test ) - name : Set up Python env if : ${{ matrix.project_language == 'python' }} uses : actions/setup-python@v3 with : python-version : \"3.7\" - name : Python Unit Tests if : ${{ matrix.project_language == 'python' }} timeout-minutes : 5 run : | ( cd src/${{ matrix.project_name }} pip install -r requirements.txt pytest ) # Run deployment tests (smoke tests) # You can add integration tests as well # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : needs : unit-tests runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 with : ref : ${{github.event.pull_request.head.ref}} repository : ${{github.event.pull_request.head.repo.full_name}} - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 1200 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kubectl : \"1.25.2\" kustomize : \"4.5.7\" tilt : \"0.30.9\" - name : Configure kubectl for DOKS with short-lived credentials run : doctl kubernetes cluster kubeconfig save ${{ env.CLUSTER_NAME }} --expiry-seconds 1200 - name : Deploy microservices to DOKS timeout-minutes : 10 run : | # Bring all microservices up using Tilt and wait for all deployments cp tilt-resources/dev/tilt_config.json . tilt ci -- \\ --allowed_contexts \"do-${{ env.REGION }}-${{ env.CLUSTER_NAME }}\" \\ --default_registry \"${{ env.DOCR_ENDPOINT }}\" \\ --environment \"dev\" \\ --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Build loadgenerator image uses : docker/build-push-action@v3 with : context : \"src/loadgenerator\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/loadgenerator\" - name : Smoke tests timeout-minutes : 10 run : | # Prepare load generator # Inject workflow custom docker image sed -i \"s#<LOAD_GENERATOR_IMAGE>#${{ env.DOCR_ENDPOINT }}/loadgenerator#g\" loadgenerator.yaml # Deploy load generator kubectl apply -f loadgenerator.yaml -n ${{ env.K8S_NAMESPACE }} # Wait for load generator deployment to be ready kubectl wait --for=condition=available --timeout=60s deployment/loadgenerator -n ${{ env.K8S_NAMESPACE }} || { # Show why load generator failed to start echo \"[INFO] Load generator pod events\" kubectl describe pod -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -10 exit 1 } # Run smoke tests REQUEST_COUNT=\"0\" while [[ \"$REQUEST_COUNT\" -lt \"50\" ]]; do sleep 5 REQUEST_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $2}') done # ensure there are no errors hitting endpoints ERROR_COUNT=$(kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | grep Aggregated | awk '{print $3}' | sed \"s/[(][^)]*[)]//g\") if [[ \"$ERROR_COUNT\" -gt \"0\" ]]; then # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 exit 1 fi # Print final results echo \"[INFO] Load generator results\" kubectl logs -l app=loadgenerator -n ${{ env.K8S_NAMESPACE }} | tail -20 working-directory : \"src/loadgenerator\" - name : Clean up Tilt microservices environment if : ${{ always() }} run : | # Remove all microservices and the namespace created by Tilt tilt down --delete-namespaces -- --namespace \"${{ env.K8S_NAMESPACE }}\" - name : Clean up Tilt docker images from registry if : ${{ always() }} run : | # Remove Tilt docker images from registry for tilt_repo in $(docker images --format \"{{.Repository}}:{{.Tag}}\" | grep \"tilt-.*[a-z|0-9]\"); do repo_and_tag=\"${tilt_repo##*/}\" repo=\"${repo_and_tag%%:*}\" tag=\"${repo_and_tag##*:}\" echo \"[INFO] Deleting tag $tag from repo $repo ...\" doctl registry repository delete-tag \"$repo\" \"$tag\" -f done echo \"[INFO] Remember to run the DOCR garbage collector from time to time!\" Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" CLUSTER_NAME : \"<YOUR_DEV_DOKS_CLUSTER_NAME_HERE>\" REGION : \"<YOUR_DEV_DOKS_CLUSTER_REGION_HERE>\" K8S_NAMESPACE : \"<YOUR_PROJECT_CUSTOM_NAMESPACE_HERE>-${{ github.event.pull_request.number }}\" In order to run deployment tests, the example CI workflow provided in this guide creates a dedicated Kubernetes namespace to deploy microservices on your target DOKS development cluster. It is suffixed using the PR number which is unique across PRs. When it finishes, it will try to clean up all associated resources such as the provisioned Kubernetes namespace, and docker images from the registry. For docker registry, you still need to run garbage collection (not triggered automatically in the workflow because it puts the DOCR in read-only mode, hence other PR workflows will fail at the docker clean up step). Explanation for the above configuration: on.pull_request - triggers the Online Boutique PR CI workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique PR CI workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths-ignore - list of repository paths used for filtering. The Online Boutique PR CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. jobs - defines list of job to run inside the pipeline such as unit tests, deployment tests, etc. strategy.matrix - use a matrix build type. Runs the unit tests in parallel for each project type combination. This approach is a perfect match for projects using multiple components, such as microservices. It also cuts down the time required to build and test each component. Each element from the matrix sets the project name, and the language being used. steps - list of steps to execute as part of the workflow jobs. For each project component (or microservice), the following list of actions is executed: Code checkout, via actions/checkout@v3 . Specific tools are installed based on project language ( actions/setup-dotnet@v2 , actions/setup-go@v3 , etc). Unit tests are executed for each project. For deployment tests, the whole suite of microservices is deployed to the DEV environment in a dedicated namespace (unique across PRs) using Tilt. The dev profile is being used. Then, load tests are performed via the smoke tests step. Finally, clean up is performed. The microservices setup allocated for the PR is destroyed, including associated Docker images pushed to the registry. Following diagram shows the Online Boutique PR CI workflow composition: graph LR A(PR Workflow Start) --> B(Unit Tests Matrix) B --> C(Cart Service <br> Tests) B --> D(Checkout Service <br> Tests) B --> E(Currency Service <br> Tests) B --> F(Email Service <br> Tests) B --> G(Frontend Tests) B --> H(Payment Service <br> Tests) B --> I(Product Catalog Service <br> Tests) B --> J(Recommendation Service <br> Tests) B --> K(Shipping Service <br> Tests) C & D & E & F & G & H & I & J & K --> L(Deployment Tests) L --> M(Env Clean Up) Next, you learn how to configure and enable the main branch workflow that gets triggered whenever changes are pushed to the main branch after each PR merge.","title":"Configuring Pull Requests Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#configuring-main-branch-workflow","text":"Whenever a PR is closed and code is merged into the main branch, a dedicated GitHub workflow is triggered. Main purpose of the main branch workflow is to test the whole application as a whole after merging various features via integration tests. Then, it will build and tag images using latest commit id, push them to registry, and finally deploy everything to the development environment. Usually you want the development environment to continuously reflect the latest changes from the development (or main) branch. Note There's no point to run unit tests again after each PR merge. Unit testing deals with changes for the affected components only. This part it is already taken care in the PR workflow. What makes more sense is to run integration tests as part of the main branch CI flow. This way, you check if application functionality is impacted after merging features in. Adding a GitHub badge in the main project README is another great benefit, giving visual feedback for the latest build state - passing or failing. Follow below steps to configure and enable the main branch workflow provided in this guide: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the CI workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-main-ci.yaml \\ -o .github/workflows/online-boutique-main-ci.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-main-ci.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-main-ci.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique CI Main workflow file name : Online Boutique CI Main on : # Used for testing only (can be disabled afterwards) workflow_dispatch : # Uncomment below lines to enable this workflow on push to main events push : branches : - main paths-ignore : - \"**/README.md\" - \"kustomize/**\" - \"argocd/**\" - \".github/workflows/*\" # Do not allow concurrent workflows # Changes should be delivered one a time as code is merged into the main branch concurrency : main-ci-dev # Global environment variables env : CI_COMMIT_AUTHOR : \"GitHub CI Actions\" CI_COMMIT_AUTHOR_EMAIL : \"gh-ci-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" jobs : # There's no point to run unit tests again after PR merge # Unit testing deals with changes for the affected components, # and that it is already taken care in the PR workflow # What it makes sense, is to run integration tests, # to see if the whole application is impacted after merging the changes # # Run deployment tests (integration tests) # Please bear in mind that more tests means increased workflow run time # With workflow concurrency disabled, means more waiting time for other PRs in the queue deployment-tests : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Run integration tests run : | echo \"[INFO] Not implemented yet!\" # Build and push project images in parallel using a matrix strategy # Cuts down build time build-and-push-images : needs : deployment-tests runs-on : ubuntu-latest strategy : matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout code uses : actions/checkout@v3 - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 600 - name : Build and push image uses : docker/build-push-action@v3 with : # cartservice is an exception - Dockerfile is placed in src/cartservice/src subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.sha }}\" # Kustomize image field for each microservice present in the `src/` dir # Finally, commit changes to main branch and let ArgoCD take over afterwards apply-kustomize-changes : needs : build-and-push-images runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kustomize : \"4.5.7\" - name : Kustomize dev environment images run : | for microservice in src/*/; do microservice=\"$(basename $microservice)\" if [[ \"$microservice\" == \"loadgenerator\" ]]; then continue fi ( cd kustomize/dev kustomize edit set image $microservice=${{ env.DOCR_ENDPOINT }}/${microservice}:${{ github.sha }} ) done - name : Commit Kustomize manifests for dev env run : | git config --global user.name \"${{ env.CI_COMMIT_AUTHOR }}\" git config --global user.email \"${{ env.CI_COMMIT_AUTHOR_EMAIL }}\" git add kustomize/dev/ git commit -m \"[CI] Bump docker images tag to ${{ github.sha }}\" - name : Push changes uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note The deployment-tests job from the Online Boutique CI Main workflow example contains only a placeholder for integration tests. This part is implementation specific, and it is left for the user to configure. Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : CI_COMMIT_AUTHOR : \"CI GitHub Actions\" CI_COMMIT_AUTHOR_EMAIL : \"gh-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" Explanation for the above configuration: on.push - triggers the Online Boutique Main CI workflow on push events only. on.push.branches - triggers the Online Boutique Main CI workflow whenever a push event is detected for the specified list of branches. In this case only main branch is desired. on.push.paths-ignore - list of repository paths used for filtering. The Online Boutique Main CI workflow is triggered only on paths not present in the paths-ignore list. Basically, everything is included (microservices as well), except for Kustomize logic, README files, and workflows logic. Kustomize configuration is dealt in a separate workflow, because it's not related to microservices application logic. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. jobs - defines list of job to run inside the pipeline such as integration tests, build and push docker images, apply Kustomize changes, etc. steps - list of steps implementing workflow jobs logic. Following diagram shows the Online Boutique Main CI workflow composition: graph TD A(Main Branch Workflow Start) --> B(Deployment Tests) B --> C{Tests Passing?} C -- No --> D(Reject build) C -- Yes --> E(Build and Push Images) E --> F(Kustomize Dev Env Images Tag) F --> G(Commit Kustomize Changes) G -. Trigger .-> H(Argo CD Dev Env Sync) style H stroke-dasharray: 5 5 Note The final step from above flow chart - Argo CD Dev Env Sync is discussed and implemented in the next chapter - Continous Deployments . Next, you learn how to configure and enable the Kustomize manifests validation workflow that gets triggered whenever changes are requested via pull requests affecting the kustomize/** path in your GitHub repository.","title":"Configuring Main Branch Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#infrastructure-manifests-validation-github-workflow","text":"So far you created CI workflows for main application code verification. What about Kubernetes manifests or infrastructure configuration files validation? This guide is using a monorepo structure, hence infrastructure configuration files live in the same repository as application code. This part is handled via a separate GitHub workflow called Online Boutique PR Kustomize Validation . Main role is to perform static checks (or linting) for all Kustomize manifests used in each overlay. It represents an additional step to validate or invalidate pull requests handling infra changes for each environment via Kustomize. Following diagram shows the Online Boutique PR Kustomize Validation workflow logic: graph TD A(New Kustomize PR) --> B(Run Validation Tests) B --> C{Tests Passing?} C -- No --> D(Reject PR) C -- Yes --> E{PR Review Passed?} E -- No --> D E -- Yes ---> F(Merge PR) Follow below steps to enable the Kustomize checks workflow for PRs: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-pr-kustomize-validation.yaml workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-pr-kustomize-validation.yaml \\ -o .github/workflows/online-boutique-pr-kustomize-validation.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-pr-kustomize-validation.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-pr-kustomize-validation.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique PR Kustomize Validation workflow file name : Online Boutique PR Kustomize Validation on : workflow_dispatch : pull_request : branches : - main paths : - \"kustomize/**\" env : KUBECONFORM_VERSION : \"0.5.0\" KUBERNETES_VERSION : \"1.24.4\" jobs : job : runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up kubeconform run : | ( cd /tmp wget \"https://github.com/yannh/kubeconform/releases/download/v${{ env.KUBECONFORM_VERSION }}/kubeconform-linux-amd64.tar.gz\" tar xvf kubeconform-linux-amd64.tar.gz chmod u+x ./kubeconform ) - name : Kustomize linting using kubeconform run : | # Test each overlay using kubeconform for kustomize_overlay in kustomize/*/; do kustomize build \"$kustomize_overlay\" | \\ /tmp/kubeconform -kubernetes-version \"${{env.KUBERNETES_VERSION}}\" -summary -verbose done Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note Depending on your setup, you may want to adjust the following environment variable at the top of your workflow file: env : KUBERNETES_VERSION : \"1.24.4\" Above workflow will validate each Kubernetes manifest using Kubeconform after rendering via Kustomize. Kubeconform is a tool for validating Kubernetes YAML files using the OpenAPI specifications. Next, you will test each workflow to check if it meets the required functionality.","title":"Infrastructure Manifests Validation GitHub Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#testing-the-online-boutique-application-github-workflows","text":"In this section you will test each workflow functionality for each event - pull requests and main branch code merge.","title":"Testing the Online Boutique Application GitHub Workflows"},{"location":"05-ci-cd/setup-continuous-integration/#testing-the-pull-requests-workflow","text":"Follow below steps to test the PR workflow for the online boutique application: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Checkout to a new branch to add a new feature: git checkout -b features/musical_instruments Open the header template file for the frontend microservice using a text editor of your choice, preferably with HTML lint support. For example, you can use VS Code : code src/frontend/templates/header.html Change the Cymbal Shops string from title section to something different (e.g. Musical Instruments ): < title > {{ if $.is_cymbal_brand }} Musical Instruments {{ else }} Online Boutique {{ end }} </ title > Tip You can quickly spin up a local environment using Tilt to test the changes first, as explained in the local development using Tilt section. You will get live updates for each change which helps in the iterative development process. Save changes, commit and push new branch to remote: git commit -am \"New feature - musical instruments.\" git push origin features/musical_instruments Navigate to your GitHub repository page, and open a new pull request against main branch using the features/musical_instruments branch as the base. At this point, the Online Boutique PR CI should kick in. Branch merging should be blocked until the workflow finishes successfully, and at least one approval is present (explained in the prerequisites section): Next, navigate to the Actions tab of your GitHub repository and inspect PR workflow progress: If everything goes as planned, the workflow should pass and the PR can be merged. This is the happy flow. You should also check what happens if introducing a change that breaks things - change one of the unit tests so that it doesn't compile successfully. Next, you will check the main branch workflow and see if applies required kustomize changes for the development environment.","title":"Testing the Pull Requests Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#testing-the-main-branch-workflow","text":"The second workflow should automatically start after merging code into main branch via the pull request created in the previous step. Navigate to the actions tab of your GitHub repository to see it in action: Note Automated deployments to the development environment cluster are disabled for now until configuring continuous deployments via ArgoCD in the next chapter. So, nothing gets deployed yet to the dev environment. You should also see a new commit added in the Git history of your repo. It shows kustomize changes for each image that was built and tagged using latest PR merge commit id:","title":"Testing the Main Branch Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#testing-the-infra-manifests-github-workflow","text":"This goes the same way as with the previous workflows testing, the only thing that's different is that it triggers only for changes present in the kustomize/ folder from your GitHub repository. You should be able to test it quickly by changing something in one of the Kustomize overlays of the online boutique project, and then create a new PR. Wait for the validation workflow to star, and check the progress. If everything goes well you should be able to merge the new PR. Tip You should also test what happens if you introduce changes breaking Kustomize functionality, such as a misspelled key name in one of the YAML manifests from the kustomize/ folder. The validation workflow should fail, and the associated PR rejected.","title":"Testing the Infra Manifests GitHub Workflow"},{"location":"05-ci-cd/setup-continuous-integration/#additional-best-practices","text":"Going further, you should be able to enhance the CI process even more by following some additional best practices, such as: Run tests only for affected microservices. Should improve workflow run time even more. Strategy matrix used to speed up actions inside the workflow such as running unit tests and building docker images is static. It is possible to create a strategy matrix with dynamic values. For example, the tilt_config.json file from the dev profile already contains the list of project microservices, so the matrix can read the values directly from the list. Use artifacts caching inside workflows whenever possible. Decouple application code and Kubernetes configuration by using a dedicated Git repository. Application code should not be dependent on Kubernetes configuration, hence it make sense to stay in a different repository. Also, whenever changes are required on the Kubernetes side, you don't need to open PRs and merge Kubernetes specific commits into the application repository. Next, ArgoCD should pickup the changes and deploy the application automatically to your development environment DOKS cluster. In order for this part to work you need to set it up in the following section.","title":"Additional Best Practices"},{"location":"06-promote-releases/promote-releases-to-upper-envs/","text":"Introduction In general, it's not best practice to immediately deploy to production after each new release. You will want to deploy to staging environment first, then have QA team check and approve the release. If everything is ok, then you will move forward and deploy to production. Promoting a new release to upper environments should happen in a controlled manner via pull requests. This approach ensures atomic changes for each environment. In case something goes wrong, you will revert last PR changes causing issues. Promote Releases to Staging Environment You already have automated deployments enabled for the staging environment from previous chapter . In other words, releases are automatically promoted to the staging environment. This approach is safe in general and preferred because the staging environment is used by QA teams testing your product. On the other hand, the same environment is shared with your clients (or customers) so that they can test or preview new features before releasing to production. Following diagram shows the high level overview of the process: graph TD A(New Application Release) -- \"Promote <br/> #40;Automated Process#41;\" --> B(Staging Environment) B --> C(QA Testing) C --> D{Release Approved?} D -- No --> E(Reject Release) D -- Yes --> F(Valid Release) Once the new release is promoted to staging, the next logical step is for QA team to kick in and test the new release. If everything looks good, then you can move forward to production environment. Promote Releases to Production Environment If new release version received QA approval then the next step is to create a new pull request addressing changes for the production environment. Basically, you will change the prod overlay kustomization.yaml manifest file to reflect new release version of application images. Optionally (but recommended), a designated team member (usually the project manager) gives the final approval as part of the PR review process. Following diagram shows the high level overview of the process: graph TD A(New Application Release) --> B{\"Valid Release? <br/> #40;QA Approved#41;\"} B -- No --> C(Reject Release) B -- Yes --> D(Promote to Prod PR) D --> E{Project Manager <br/> Approval} E -- Yes --> F(Deploy to Production Environment) E -- No --> C Prerequisites: A new version release for your project. You already released a new version for the online boutique application in previous chapter, tagged with v1.0.1 . Release artifacts already published to DOCR via the designated GitHub workflow. This step is already accomplished for v1.0.1 , as mentioned above. New application version already tested and approved by QA team for staging environment. Steps: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change version number within newTag line for each application defined in the kustomization.yaml manifest file from prod overlay. For example, if new application version to promote is v1.0.1 , then the images section looks similar to: Click to expand the kustomize/prod/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.1 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.1 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.1 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.1 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.1 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.1 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.1 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.1 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.1 ... Note Above example is using registry.digitalocean.com/microservices-demo for the DOCR endpoint. Don't forget to adjust according to your setup. Save the kustomization.yaml manifest, commit changes to your working branch. Push new branch to remote, and create a new PR addressing Kustomize changes for the prod overlay. Next, the Kustomize validation GitHub workflow is automatically triggered. If all checks pass, approve the PR, and merge code to main branch. After a few moments (3 minutes max), ArgoCD picks changes and deploys new application version to prod environment. Finally, check if Argo CD synced the new application version to the production environment. After port-forwarding the Argo web console, you should see new application version deployed to your production environment automatically. Rollback Bad Releases A very important aspect of every CD system is the ability to easily rollback an application to a previous working version. On the other hand, you will want to perform the rollback in a timely manner as well to avoid unhappy clients, especially in production environments. Below you will find two methods that you can use. Both should work out of the box and yield the same result, the most notable difference being the speed at which you will perform the actual rollback. GitHub PR Revert Feature In case something goes bad, you should be able to immediately revert the PR causing the culprit for the respective environment. Using PRs ensures atomic changes and rollbacks. Next, Argo CD picks the changes automatically and rolls back to previous application version. Essentially, you follow the same procedure as explained in the reverting bad application deployments section from the CD chapter. The big downside of this approach is that it is slower - you have to revert the previous PR, create a new one, wait for validation workflows to pass, manual review, and finally merge changes to main branch. Next, a faster approach is presented offered by Argo CD itself which suits best for production environments where speed matters. Argo CD Application Rollback Feature Another approach is to use the Argo CD application rollback feature. You should be able to revert to any previous git commit ID. Using this approach you will perform faster application rollbacks. Warning Unfortunately there's an open issue on Argo CD project GitHub repository impeding the rollback feature from working properly. Nevertheless, below steps should give you a quick overview of how this operation is accomplished. Follow below steps to learn how to use this feature: Make sure kubectl context is set to point to your production environment cluster. Below example is using do-nyc1-microservices-demo-production for demonstration - make sure to change value if yours is different: kubectl config set-context do -nyc1-microservices-demo-production Port forward the Argo CD web interface: kubectl port-forward -n argocd svc/argocd-server 8080 :80 Open the Argo CD dashboard in your web browser using localhost:8080 . Locate the argocd/prod-microservices-demo application tile, and click on it. You will be redirected to the prod-microservices-demo application details page. Now, click on the HISTORY AND ROLLBACK button located at the top of the page: You should see a list of tiles representing deployment history activity for the prod-microservices-demo application. You need to scroll down to the second tile from the list, containing the previous release commit ID. Then, click on the three dots button from the right side - a Rollback button pops up: Click on the Rollback button. A dialog box message pops up informing you that the auto-sync feature needs to be turned off in order to finish the rollback operation. This is expected, otherwise Argo will immediately trigger an update if a change is detected in your GitOps repository and undo your rollback operation. Click on the OK button, and wait for the process to finish. Just bear in mind that during the rollback process changes are performed only on the Argo CD server - your Git repository remains untouched. As a consequence, the affected Argo application is not in sync anymore with latest state of your Git repository. This is not a bad thing after all because it gives you time to fix the application. The affected Argo application will be synced again automatically after enabling auto-sync again, and a new release is made containing required fixes.","title":"Promote releases to upper environments"},{"location":"06-promote-releases/promote-releases-to-upper-envs/#introduction","text":"In general, it's not best practice to immediately deploy to production after each new release. You will want to deploy to staging environment first, then have QA team check and approve the release. If everything is ok, then you will move forward and deploy to production. Promoting a new release to upper environments should happen in a controlled manner via pull requests. This approach ensures atomic changes for each environment. In case something goes wrong, you will revert last PR changes causing issues.","title":"Introduction"},{"location":"06-promote-releases/promote-releases-to-upper-envs/#promote-releases-to-staging-environment","text":"You already have automated deployments enabled for the staging environment from previous chapter . In other words, releases are automatically promoted to the staging environment. This approach is safe in general and preferred because the staging environment is used by QA teams testing your product. On the other hand, the same environment is shared with your clients (or customers) so that they can test or preview new features before releasing to production. Following diagram shows the high level overview of the process: graph TD A(New Application Release) -- \"Promote <br/> #40;Automated Process#41;\" --> B(Staging Environment) B --> C(QA Testing) C --> D{Release Approved?} D -- No --> E(Reject Release) D -- Yes --> F(Valid Release) Once the new release is promoted to staging, the next logical step is for QA team to kick in and test the new release. If everything looks good, then you can move forward to production environment.","title":"Promote Releases to Staging Environment"},{"location":"06-promote-releases/promote-releases-to-upper-envs/#promote-releases-to-production-environment","text":"If new release version received QA approval then the next step is to create a new pull request addressing changes for the production environment. Basically, you will change the prod overlay kustomization.yaml manifest file to reflect new release version of application images. Optionally (but recommended), a designated team member (usually the project manager) gives the final approval as part of the PR review process. Following diagram shows the high level overview of the process: graph TD A(New Application Release) --> B{\"Valid Release? <br/> #40;QA Approved#41;\"} B -- No --> C(Reject Release) B -- Yes --> D(Promote to Prod PR) D --> E{Project Manager <br/> Approval} E -- Yes --> F(Deploy to Production Environment) E -- No --> C Prerequisites: A new version release for your project. You already released a new version for the online boutique application in previous chapter, tagged with v1.0.1 . Release artifacts already published to DOCR via the designated GitHub workflow. This step is already accomplished for v1.0.1 , as mentioned above. New application version already tested and approved by QA team for staging environment. Steps: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change version number within newTag line for each application defined in the kustomization.yaml manifest file from prod overlay. For example, if new application version to promote is v1.0.1 , then the images section looks similar to: Click to expand the kustomize/prod/kustomization.yaml images section ... images : - name : cartservice newName : registry.digitalocean.com/microservices-demo/cartservice newTag : v1.0.1 - name : checkoutservice newName : registry.digitalocean.com/microservices-demo/checkoutservice newTag : v1.0.1 - name : currencyservice newName : registry.digitalocean.com/microservices-demo/currencyservice newTag : v1.0.1 - name : emailservice newName : registry.digitalocean.com/microservices-demo/emailservice newTag : v1.0.1 - name : frontend newName : registry.digitalocean.com/microservices-demo/frontend newTag : v1.0.1 - name : paymentservice newName : registry.digitalocean.com/microservices-demo/paymentservice newTag : v1.0.1 - name : productcatalogservice newName : registry.digitalocean.com/microservices-demo/productcatalogservice newTag : v1.0.1 - name : recommendationservice newName : registry.digitalocean.com/microservices-demo/recommendationservice newTag : v1.0.1 - name : shippingservice newName : registry.digitalocean.com/microservices-demo/shippingservice newTag : v1.0.1 ... Note Above example is using registry.digitalocean.com/microservices-demo for the DOCR endpoint. Don't forget to adjust according to your setup. Save the kustomization.yaml manifest, commit changes to your working branch. Push new branch to remote, and create a new PR addressing Kustomize changes for the prod overlay. Next, the Kustomize validation GitHub workflow is automatically triggered. If all checks pass, approve the PR, and merge code to main branch. After a few moments (3 minutes max), ArgoCD picks changes and deploys new application version to prod environment. Finally, check if Argo CD synced the new application version to the production environment. After port-forwarding the Argo web console, you should see new application version deployed to your production environment automatically.","title":"Promote Releases to Production Environment"},{"location":"06-promote-releases/promote-releases-to-upper-envs/#rollback-bad-releases","text":"A very important aspect of every CD system is the ability to easily rollback an application to a previous working version. On the other hand, you will want to perform the rollback in a timely manner as well to avoid unhappy clients, especially in production environments. Below you will find two methods that you can use. Both should work out of the box and yield the same result, the most notable difference being the speed at which you will perform the actual rollback.","title":"Rollback Bad Releases"},{"location":"06-promote-releases/promote-releases-to-upper-envs/#github-pr-revert-feature","text":"In case something goes bad, you should be able to immediately revert the PR causing the culprit for the respective environment. Using PRs ensures atomic changes and rollbacks. Next, Argo CD picks the changes automatically and rolls back to previous application version. Essentially, you follow the same procedure as explained in the reverting bad application deployments section from the CD chapter. The big downside of this approach is that it is slower - you have to revert the previous PR, create a new one, wait for validation workflows to pass, manual review, and finally merge changes to main branch. Next, a faster approach is presented offered by Argo CD itself which suits best for production environments where speed matters.","title":"GitHub PR Revert Feature"},{"location":"06-promote-releases/promote-releases-to-upper-envs/#argo-cd-application-rollback-feature","text":"Another approach is to use the Argo CD application rollback feature. You should be able to revert to any previous git commit ID. Using this approach you will perform faster application rollbacks. Warning Unfortunately there's an open issue on Argo CD project GitHub repository impeding the rollback feature from working properly. Nevertheless, below steps should give you a quick overview of how this operation is accomplished. Follow below steps to learn how to use this feature: Make sure kubectl context is set to point to your production environment cluster. Below example is using do-nyc1-microservices-demo-production for demonstration - make sure to change value if yours is different: kubectl config set-context do -nyc1-microservices-demo-production Port forward the Argo CD web interface: kubectl port-forward -n argocd svc/argocd-server 8080 :80 Open the Argo CD dashboard in your web browser using localhost:8080 . Locate the argocd/prod-microservices-demo application tile, and click on it. You will be redirected to the prod-microservices-demo application details page. Now, click on the HISTORY AND ROLLBACK button located at the top of the page: You should see a list of tiles representing deployment history activity for the prod-microservices-demo application. You need to scroll down to the second tile from the list, containing the previous release commit ID. Then, click on the three dots button from the right side - a Rollback button pops up: Click on the Rollback button. A dialog box message pops up informing you that the auto-sync feature needs to be turned off in order to finish the rollback operation. This is expected, otherwise Argo will immediately trigger an update if a change is detected in your GitOps repository and undo your rollback operation. Click on the OK button, and wait for the process to finish. Just bear in mind that during the rollback process changes are performed only on the Argo CD server - your Git repository remains untouched. As a consequence, the affected Argo application is not in sync anymore with latest state of your Git repository. This is not a bad thing after all because it gives you time to fix the application. The affected Argo application will be synced again automatically after enabling auto-sync again, and a new release is made containing required fixes.","title":"Argo CD Application Rollback Feature"},{"location":"06-promote-releases/setup-release-process/","text":"Introduction Releasing a new version for a project is a separate process, and it is usually triggered manually by a release engineer (or project owner). GitHub offers the possibility to create and manage releases via the web interface in a very straightforward manner. GitHub helps you in the release process with creating new tags, generating changelogs, and packing application assets for the specific version. Next, after a new version of the application is released, a dedicated GitHub workflow runs which builds and tags application artifacts. Optionally, trigger a rollout for the new application version in the staging environment. Depending on your setup (as well as internal policies), the staging environment is set to automatically receive new application versions or not. In general, it's safe to automatically deploy a new release to the staging environment. The staging environment acts as a buffer between the development environment and the final production stage. This environment is used by QA teams to perform additional testing, and in some cases is shared with your customers to check pre-releases or new features before releasing the final product. Note This guide enables automated deployments for staging environment on each release. Prerequisites To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. A DOKS cluster set up and running for the staging environment . The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Your DigitalOcean authentication token stored as a GitHub secret named DIGITALOCEAN_ACCESS_TOKEN in the microservices-demo repository. Follow this guide to learn more about creating GitHub secrets. Managing Application Releases The release process is pretty straightforward, and follows below rules: Semantic versioning is used to distinguish between each release. First version starts at v1.0.0 . On each release, application version is increased by a single value . For example if previous application version was v1.0.0 , new version will be v1.0.1 , and so on. A corresponding git tag is created on each release. Git tags help you identify changes introduced by a specific version in time. Put it other way, you get a snapshot for a set of application changes in time. Also, in case something goes wrong you should be able to rollback easily to previous application version. A dedicated workflow runs on each git tag event. Main purpose is to build and push project images tagged with the release version. Optionally, it can push new application version to the staging environment. Note This guide (and associated examples) uses a simple release process which doesn't cover release candidates, beta releases, hotfixes or other advanced strategies. Releasing a new version for the online boutique sample application consists of: Create a new GitHub release using the web interface: Tag the new release based on semantic versioning (e.g. v1.0.0). Generate release changelog. Automatically trigger the release GitHub workflow which does the following: Checks the latest release tag and corresponding version. Builds and tags images for application components. Pushes release artifacts to registry (docker images). Optionally, push new release version to staging environment. The release process flow is depicted below: graph TD A(Create New GitHub Release) -- Trigger --> B(Release GitHub Workflow) B --> C(Release Validation Tests) C --> D{Tests Passing?} D -- No --> E(Reject release) D -- Yes --> F(Build and Push Release Images) F --> G(Kustomize Staging Env Images Tag) G --> H(Commit Kustomize Changes) H -. Trigger .-> I(Argo CD Staging Sync) style I stroke-dasharray: 5 5 Next, you will learn how to configure and enable the GitHub workflow which automatically triggers on each application release. Setting Up the Release Process GitHub Workflow Before jumping to a real world example, the GitHub workflow associated with the release process needs to be configured. The workflow is automatically triggered whenever a new Git tag is created using the following naming convention - v[MAJOR].[MINOR].[PATCH] . Follow below steps to perform enable the release workflow: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-release.yaml workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-release.yaml \\ -o .github/workflows/online-boutique-release.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-release.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-release.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Release workflow file name : Online Boutique Release on : workflow_dispatch : push : # Trigger on push events to any tag matching semantic versioning tags : - 'v[0-9]+\\.[0-9]+\\.[0-9]+' env : RELEASE_COMMIT_AUTHOR : \"GitHub Release Actions\" RELEASE_COMMIT_AUTHOR_EMAIL : \"gh-release-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" jobs : validation-tests : runs-on : ubuntu-latest steps : - run : echo \"[INFO] Not implemented yet!\" build-and-push-release-images : needs : validation-tests runs-on : ubuntu-latest strategy : matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout code uses : actions/checkout@v3 - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 600 - name : Build and push image uses : docker/build-push-action@v3 with : # cartservice is an exception - Dockerfile is placed in src/cartservice/src subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.ref_name }}\" # Kustomize image field for each microservice present in the `src/` dir # Finally, commit changes to main branch and let ArgoCD take over afterwards apply-kustomize-changes : needs : build-and-push-release-images runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kustomize : \"4.5.7\" - name : Kustomize staging environment images run : | for microservice in src/*/; do microservice=\"$(basename $microservice)\" if [[ \"$microservice\" == \"loadgenerator\" ]]; then continue fi ( cd kustomize/staging/ kustomize edit set image $microservice=${{ env.DOCR_ENDPOINT }}/${microservice}:${{ github.ref_name }} ) done - name : Commit Kustomize manifests for staging env run : | git config --global user.name \"${{ env.RELEASE_COMMIT_AUTHOR }}\" git config --global user.email \"${{ env.RELEASE_COMMIT_AUTHOR_EMAIL }}\" git add kustomize/staging/ git commit -m \"[Release] Bump docker images tag to ${{ github.ref_name }}\" - name : Push changes uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note Depending on your setup, you may want to change the following environment variables at the top of your workflow file: env : RELEASE_COMMIT_AUTHOR : \"GitHub Release Actions\" RELEASE_COMMIT_AUTHOR_EMAIL : \"gh-release-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" Explanation for the above configuration: on.push.tags - triggers the Online Boutique Release workflow whenever a push event is detected for the specified list of tags. In this case only tags matching the following regex expression - v[0-9]+\\.[0-9]+\\.[0-9]+ are desired. The regex expression matches any tag name corresponding to the v[MAJOR].[MINOR].[PATCH] convention - e.g. v1.0.0 , v1.0.1 , and so on. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. jobs - defines list of job to run inside the pipeline such as validation tests, build and push release docker images, apply Kustomize changes, etc. steps - list of steps implementing workflow jobs logic. Next, you will learn how to create a new release version for the online boutique sample application used in this guide. Releasing a New Version for the Online Boutique Application In this section, you will learn how to create a new GitHub release using the web interface. GitHub provides a neat UI experience and helps you create and manage releases for your application using a straightforward process. Tip It's best practice to announce and enforce code freeze for your repository before each release. This procedure ensures that no changes are pushed to your repository during the release process. You can automate this behavior by setting pull/triage/push/maintain/admin permissions for your GitHub repository via the REST API . Bellow snippet allows collaborators to pull code only, thus dissallowing code push to your repository (make sure to replace the <> placeholders first): cURL GitHub CLI curl \\ -X PUT \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer <YOUR_TOKEN>\" \\ https://api.github.com/repos/<OWNER>/<REPO>/collaborators/<USERNAME> \\ -d '{\"permission\":\"pull\"}' gh api \\ --method PUT \\ -H \"Accept: application/vnd.github+json\" \\ /repos/<OWNER>/<REPO>/collaborators/<USERNAME> \\ -f permission = 'pull' Find out more by visiting the Collaborators REST API page from the official GitHub documentation website. Follow below steps to create and tag a new release for the sample application used in this guide: Prepare a few major changes for the application, open and merge required pull requests to commit all changes to your microservices-demo repository. Open a web browser, and navigate to your Github repository hosting the microservices-demo sample application. Open the release wizard by clicking the Create a new release link on the right side: Complete the Choose a tag input field using v1.0.1 for new tag name. Then, click on the Generate release notes button, followed by the Publish release green button down below: Next, the Release Process GitHub Workflow configured earlier is triggered. Navigate to the actions tab to see it progressing: If everything goes well, the release process GitHub workflow should finish successfully, and push application images to your DOCR tagged using the v1.0.1 release version: Next, a new commit should be present in your microservices-demo repo with the following signature: Navigate to the respective commit ID, and see what changed - you should see new release version set for all staging env docker images: Finally, check if Argo CD synced the new application version to the staging environment. After port-forwarding the Argo web console, you should see new application version deployed to your staging environment automatically. Reverting a Bad Release Reverting a bad release goes the same way as you learned in the Set up continuous deployments -> Reverting bad application deployments chapter. Next, you will learn how to promote releases to the production environment. This is a step performed manually for obvious reasons, discussed in more detail in the respective chapter.","title":"Set up release process"},{"location":"06-promote-releases/setup-release-process/#introduction","text":"Releasing a new version for a project is a separate process, and it is usually triggered manually by a release engineer (or project owner). GitHub offers the possibility to create and manage releases via the web interface in a very straightforward manner. GitHub helps you in the release process with creating new tags, generating changelogs, and packing application assets for the specific version. Next, after a new version of the application is released, a dedicated GitHub workflow runs which builds and tags application artifacts. Optionally, trigger a rollout for the new application version in the staging environment. Depending on your setup (as well as internal policies), the staging environment is set to automatically receive new application versions or not. In general, it's safe to automatically deploy a new release to the staging environment. The staging environment acts as a buffer between the development environment and the final production stage. This environment is used by QA teams to perform additional testing, and in some cases is shared with your customers to check pre-releases or new features before releasing the final product. Note This guide enables automated deployments for staging environment on each release.","title":"Introduction"},{"location":"06-promote-releases/setup-release-process/#prerequisites","text":"To complete this section you will need: A container registry already set up as explained in the Getting Started -> Set up DOCR section. A DOKS cluster set up and running for the staging environment . The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. Your DigitalOcean authentication token stored as a GitHub secret named DIGITALOCEAN_ACCESS_TOKEN in the microservices-demo repository. Follow this guide to learn more about creating GitHub secrets.","title":"Prerequisites"},{"location":"06-promote-releases/setup-release-process/#managing-application-releases","text":"The release process is pretty straightforward, and follows below rules: Semantic versioning is used to distinguish between each release. First version starts at v1.0.0 . On each release, application version is increased by a single value . For example if previous application version was v1.0.0 , new version will be v1.0.1 , and so on. A corresponding git tag is created on each release. Git tags help you identify changes introduced by a specific version in time. Put it other way, you get a snapshot for a set of application changes in time. Also, in case something goes wrong you should be able to rollback easily to previous application version. A dedicated workflow runs on each git tag event. Main purpose is to build and push project images tagged with the release version. Optionally, it can push new application version to the staging environment. Note This guide (and associated examples) uses a simple release process which doesn't cover release candidates, beta releases, hotfixes or other advanced strategies. Releasing a new version for the online boutique sample application consists of: Create a new GitHub release using the web interface: Tag the new release based on semantic versioning (e.g. v1.0.0). Generate release changelog. Automatically trigger the release GitHub workflow which does the following: Checks the latest release tag and corresponding version. Builds and tags images for application components. Pushes release artifacts to registry (docker images). Optionally, push new release version to staging environment. The release process flow is depicted below: graph TD A(Create New GitHub Release) -- Trigger --> B(Release GitHub Workflow) B --> C(Release Validation Tests) C --> D{Tests Passing?} D -- No --> E(Reject release) D -- Yes --> F(Build and Push Release Images) F --> G(Kustomize Staging Env Images Tag) G --> H(Commit Kustomize Changes) H -. Trigger .-> I(Argo CD Staging Sync) style I stroke-dasharray: 5 5 Next, you will learn how to configure and enable the GitHub workflow which automatically triggers on each application release.","title":"Managing Application Releases"},{"location":"06-promote-releases/setup-release-process/#setting-up-the-release-process-github-workflow","text":"Before jumping to a real world example, the GitHub workflow associated with the release process needs to be configured. The workflow is automatically triggered whenever a new Git tag is created using the following naming convention - v[MAJOR].[MINOR].[PATCH] . Follow below steps to perform enable the release workflow: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-release.yaml workflow file from the kubernetes-sample-apps repo: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-release.yaml \\ -o .github/workflows/online-boutique-release.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-release.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-release.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Release workflow file name : Online Boutique Release on : workflow_dispatch : push : # Trigger on push events to any tag matching semantic versioning tags : - 'v[0-9]+\\.[0-9]+\\.[0-9]+' env : RELEASE_COMMIT_AUTHOR : \"GitHub Release Actions\" RELEASE_COMMIT_AUTHOR_EMAIL : \"gh-release-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" jobs : validation-tests : runs-on : ubuntu-latest steps : - run : echo \"[INFO] Not implemented yet!\" build-and-push-release-images : needs : validation-tests runs-on : ubuntu-latest strategy : matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout code uses : actions/checkout@v3 - name : Install doctl uses : digitalocean/action-doctl@v2 with : token : ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }} - name : Log in to DOCR with short-lived credentials run : doctl registry login --expiry-seconds 600 - name : Build and push image uses : docker/build-push-action@v3 with : # cartservice is an exception - Dockerfile is placed in src/cartservice/src subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : true tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.ref_name }}\" # Kustomize image field for each microservice present in the `src/` dir # Finally, commit changes to main branch and let ArgoCD take over afterwards apply-kustomize-changes : needs : build-and-push-release-images runs-on : ubuntu-latest steps : - name : Checkout code uses : actions/checkout@v3 - name : Set up K8S tools uses : yokawasa/action-setup-kube-tools@v0.8.2 with : kustomize : \"4.5.7\" - name : Kustomize staging environment images run : | for microservice in src/*/; do microservice=\"$(basename $microservice)\" if [[ \"$microservice\" == \"loadgenerator\" ]]; then continue fi ( cd kustomize/staging/ kustomize edit set image $microservice=${{ env.DOCR_ENDPOINT }}/${microservice}:${{ github.ref_name }} ) done - name : Commit Kustomize manifests for staging env run : | git config --global user.name \"${{ env.RELEASE_COMMIT_AUTHOR }}\" git config --global user.email \"${{ env.RELEASE_COMMIT_AUTHOR_EMAIL }}\" git add kustomize/staging/ git commit -m \"[Release] Bump docker images tag to ${{ github.ref_name }}\" - name : Push changes uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} Save the workflow file, commit, and push changes to your git repository main branch ( you may need to temporarily disable main branch protection first ). Note Depending on your setup, you may want to change the following environment variables at the top of your workflow file: env : RELEASE_COMMIT_AUTHOR : \"GitHub Release Actions\" RELEASE_COMMIT_AUTHOR_EMAIL : \"gh-release-actions@noreply.github.com\" DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" Explanation for the above configuration: on.push.tags - triggers the Online Boutique Release workflow whenever a push event is detected for the specified list of tags. In this case only tags matching the following regex expression - v[0-9]+\\.[0-9]+\\.[0-9]+ are desired. The regex expression matches any tag name corresponding to the v[MAJOR].[MINOR].[PATCH] convention - e.g. v1.0.0 , v1.0.1 , and so on. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. jobs - defines list of job to run inside the pipeline such as validation tests, build and push release docker images, apply Kustomize changes, etc. steps - list of steps implementing workflow jobs logic. Next, you will learn how to create a new release version for the online boutique sample application used in this guide.","title":"Setting Up the Release Process GitHub Workflow"},{"location":"06-promote-releases/setup-release-process/#releasing-a-new-version-for-the-online-boutique-application","text":"In this section, you will learn how to create a new GitHub release using the web interface. GitHub provides a neat UI experience and helps you create and manage releases for your application using a straightforward process. Tip It's best practice to announce and enforce code freeze for your repository before each release. This procedure ensures that no changes are pushed to your repository during the release process. You can automate this behavior by setting pull/triage/push/maintain/admin permissions for your GitHub repository via the REST API . Bellow snippet allows collaborators to pull code only, thus dissallowing code push to your repository (make sure to replace the <> placeholders first): cURL GitHub CLI curl \\ -X PUT \\ -H \"Accept: application/vnd.github+json\" \\ -H \"Authorization: Bearer <YOUR_TOKEN>\" \\ https://api.github.com/repos/<OWNER>/<REPO>/collaborators/<USERNAME> \\ -d '{\"permission\":\"pull\"}' gh api \\ --method PUT \\ -H \"Accept: application/vnd.github+json\" \\ /repos/<OWNER>/<REPO>/collaborators/<USERNAME> \\ -f permission = 'pull' Find out more by visiting the Collaborators REST API page from the official GitHub documentation website. Follow below steps to create and tag a new release for the sample application used in this guide: Prepare a few major changes for the application, open and merge required pull requests to commit all changes to your microservices-demo repository. Open a web browser, and navigate to your Github repository hosting the microservices-demo sample application. Open the release wizard by clicking the Create a new release link on the right side: Complete the Choose a tag input field using v1.0.1 for new tag name. Then, click on the Generate release notes button, followed by the Publish release green button down below: Next, the Release Process GitHub Workflow configured earlier is triggered. Navigate to the actions tab to see it progressing: If everything goes well, the release process GitHub workflow should finish successfully, and push application images to your DOCR tagged using the v1.0.1 release version: Next, a new commit should be present in your microservices-demo repo with the following signature: Navigate to the respective commit ID, and see what changed - you should see new release version set for all staging env docker images: Finally, check if Argo CD synced the new application version to the staging environment. After port-forwarding the Argo web console, you should see new application version deployed to your staging environment automatically.","title":"Releasing a New Version for the Online Boutique Application"},{"location":"06-promote-releases/setup-release-process/#reverting-a-bad-release","text":"Reverting a bad release goes the same way as you learned in the Set up continuous deployments -> Reverting bad application deployments chapter. Next, you will learn how to promote releases to the production environment. This is a step performed manually for obvious reasons, discussed in more detail in the respective chapter.","title":"Reverting a Bad Release"},{"location":"07-security/software-supply-chain/","text":"Introduction Securing the Kubernetes software supply chain is a challenging task because of the multiple layers and components involved. In the end, it doesn't have to be so. By following a set of best practices and recommendations the process gets easier to implement and manage afterwards. Below is a set of best practices that will help you achieve a more secure supply chain: Early adoption of security scanning tools right within your IDE. Usually, you get this kind of support via extensions (or plugins) that can be installed right in your preferred IDE. It's best to adopt security best practices in the early stages of development thus preventing breaches at a later time when costs get really high. Use security gates with CI pipelines. Basically, you enable security tools to perform static code analysis for each pull request targeting your GitHub repository. Container registry periodic scans. New vulnerabilities are reported each day so you need to make sure that your registry is scanned on a regular basis and get notified in due time. Even though project images are automatically scanned in the CI process and pass all security checks at that point in time, it doesn't mean they are still safe today. What's important to understand is that any external dependency you source in your project is subject to new threats. Docker base images, 3rd party libraries, etc you fetch and use in your projects add to the list of vulnerabilities. Post-deployment and Kubernetes cluster periodic scans. Same idea as with registry scans. You already deployed application workloads to your cluster but because new vulnerabilities are discovered each day, yet again your system becomes sensible to attacks pretty soon. Enabling continuous scans for Kubernetes workloads (Pods, Deployments, etc), worker nodes, etc, decreases the overall risk. Kubernetes is a multi-tenant system - what if someone bypasses all security checks presented earlier? This is another reason why continuous scanning of your Kubernetes cluster is important. Because of the multitude of vulnerabilities found each day, this process should be approached systematically. On the other hand, each team or organization decides the level of acceptance considered to be safe for each project internally. Usually, you take care of vulnerabilities reported as high/critical first. Later on, at a fixed interval or even earlier depending on the severity level of newly disclosed vulnerabilities you repeat the process, and so on. There are several tools available to help you along the way two of the most valuable today being Kubescape and Snyk . In this guide, Snyk is picked because it provides more options and it's more flexible. Either tool you choose the basic idea is the same. Why Snyk? Snyk provides very good support for the following: IDE integration (e.g. Visual Studio Code ). User application code base scanning. IAC manifests scanning (Kubernetes, Terraform, etc). Third party libraries and open source projects scanning. Application Docker images scanning. Docker registry integration for periodic scanning (including private registries such as DOCR ). CI pipelines integration via the CLI binary, or GitHub Actions . GitHub repository integration for continuous scans of your application repository. Dedicated web portal to investigate reported issues and take appropriate actions to remediate the situation. All of the above is available in the free version. For small startups and business the free tier should be more than enough. For an in-depth explanations and comparison, please visit DigitalOcean's tutorials dedicated to both Kubescape and Snyk . Also, the official documentation for each tool is very valuable and provides more insight. Prerequisites To complete this section you will need: A free Snyk cloud account account used to periodically publish scan results for your Kubernetes cluster to a nice dashboard. Also, the Snyk web interface helps you with investigations and risk analysis. Please follow How to Create a Snyk Account documentation page. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. A GitHub secret named SNYK_TOKEN which holds your Snyk user account ID. Run snyk config get api in a terminal to get your ID. If that doesn't work, you should be able to retrieve the token from your user account settings page. The Snyk Code option enabled in your Snyk Org Setting. A Slack workspace you own, and a dedicated Slack app to get notified of vulnerability scan issues reported by Snyk. Configuring Snyk for IDEs It's very important to discover security issues right from the early stages of development. Integrating vulnerabilities scanning tool with your preferred IDE is a good start. Doing so, you will get security reports right in your IDE for each project you're working on. Fixing security issues in the development stage avoids propagation and later problems in production systems. Installing Snyk IDE Extension This guide is focusing on Visual Studio Code integration because it is the most popular IDE nowadays. Nevertheless, Snyk offers support for other IDEs as well, such as: Eclipse . JetBrains . Visual Studio . Generic integration via language server protocol for any IDE that supports it. Follow below steps to integrate Snyk with your existing Visual Studio Code installation: Launch Visual Studio Code, and navigate to the extensions tab: Type snyk in the search box. Matched extensions should be present: Install Snyk extension from the list: After completing above steps, a Snyk button should appear in the left menu bar: From there you should connect your free Snyk account with Visual Studio Code and benefit from the features offered by the extension. Next, you will discover how easy it is to scan an existing project and get detailed information about reported vulnerabilities. Scanning Online Boutique Demo Application The Snyk extension is able to scan your local projects and report found issues right within your IDE of your choice. This guide is relying on Visual Studio Code but you should get similar experience with other supported IDEs as well. To test Snyk extension features inside Visual Studio Code, launch the IDE and open your microservices-demo project (should be already cloned on your local machine). Then, follow below steps: Navigate to the Snyk extension menu by clicking the icon from the left menu bar with a dog picture on it. You will be presented with a message asking you to enable Snyk code scanning. This needs to be performed only once for the current project: Next, you will be redirected to your Snyk account settings. A new web browser window is opened. Tick the checkbox, and save changes: Go back to your VS Code instance - you should see Snyk scanning your project for vulnerabilities. When it finishes, the following report should be presented to you: If everything looks like above then you have successfully scanned your first project using Snyk right from your IDE. Next, click on each reported issue to see the root cause and get additional details: For each reported issue you should get the severity level printed, and additional explanations why it's considered an issue alongside with a possible fix, if it's known. Severity levels are denoted with an uppercase letter, such as H for high severity, M for medium severity, and so on. Please visit the official documentation to learn more about the Snyk IDE extension and all available features. Configuring Snyk for CI Pipelines Snyk offers very good support in terms of CI pipelines automation via the dedicated CLI . GitHub pipelines support is also available via Snyk actions . Before going into the implementation steps it's important to understand how Snyk is architected and used in practice. Without going too much into the inner details, you should know the following: Snyk is able to scan your application code for vulnerabilities. This feature is provided by the Snyk Code component. Snyk is able to scan open source projects and 3rd party dependencies . This feature is provided by the Snyk Open Source component. Snyk knows how to scan Infrastructure as Code (IAC) manifests such as Kubernetes , Terraform , etc. This feature is provided by the Snyk Infrastructure as Code component. Snyk knows to scan containers (e.g. Docker), as well as Dockerfiles . This feature is provided by the Snyk Container component. Each feature from the above list is available via the free CLI tool offered by Snyk. This guide is focused more on the CLI integration with GitHub pipelines because of the scripting flexibility that it offers. You already configured a CI pipeline for your project in the CI chapter available in the Kubernetes adoption journey guide. Whenever a pull request is opened for your repository, a CI GitHub workflow is automatically triggered for validation. You will learn how to integrate Snyk with existing CI workflows to benefit from all features offered by the scanning tool. On each scan, the Snyk CLI will generate a report that you can visualize using the web portal offered for free by Snyk. The tool is also able to generate SARIF reports hence the final results will be available right in the Security tab of your GitHub repository. Other supported formats are JSON and HTML . Note Only the IAC manifests scanner is able to upload results to the Snyk web portal at this time of writing. Instead of embedding Snyk logic directly into CI pipelines, you will configure three separate Snyk workflows running in parallel with existing ones. Each Snyk workflow deals with a specific task such as scanning and detecting vulnerabilities for application source code, IAC manifests, and Docker containers. In the end, the net effect is the same - Snyk acts as a security gate, thus validating or invalidating a pull request based on the scan results. This approach has the following benefits: Snyk workflows run in parallel with other flows, such as pull requests CI workflows . If you need to fix something that is Snyk related, only the specific workflow is triggered and not the whole PR flow. Waiting time is reduced - some workflows take more time to finish such as PR workflows which runs automated tests. By decoupling Snyk logic, you avoid adding more complexity to the pull requests workflow thus easing maintenance. Next, you will learn how to configure each Snyk workflow in turn to scan application source code with 3rd party dependencies, IAC manifests and Docker containers. Application Source Code Scanning Snyk uses various heuristics under the hood to perform static analysis of your application source code, as well as 3rd party dependencies. It creates a snapshot for your application source code current state, and runs the analysis. When finished you'll get the scan results printed to standard output in your terminal of choice. Snyk is also able to export scan results in other formats such as GitHub SARIF , JSON , etc. Each reported issue has a severity level associated as well such as critical , high , medium , and low . Usually you will take into account issues marked as critical and high first. Also, some issues may not affect your application even though they're reported. In such case, you have the option to exclude or ignore issues . How do you pick the severity threshold? Snyk marks your pipeline as failed if the found issues have a severity level equal to or higher than the threshold you set. As an example, if you set the threshold to high , then Snyk takes into account all issues reported as high and critical . If the threshold is set to medium , then it will take into account all issues reported as medium , high and critical . In other words, if you lower the bar it gets more and more strict. Follow below steps to configure the Snyk workflow used for application source code scanning: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-snyk-source-code-scan.yaml workflow file from the kubernetes-sample-apps repository: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-snyk-source-code-scan.yaml \\ -o .github/workflows/online-boutique-snyk-source-code-scan.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-snyk-source-code-scan.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-snyk-source-code-scan.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Snyk Source Code Scan workflow file name : Online Boutique Snyk Source Code Scan on : pull_request : branches : - main paths : - \"src/**\" # Below configuration is used for manual workflow dispatch workflow_dispatch : inputs : snyk_fail_threshold : description : | Sets fail threshold for Snyk (low | medium | high | critical) required : true default : \"high\" env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" jobs : source-code-security-check : runs-on : ubuntu-latest strategy : fail-fast : false matrix : include : - project_name : cartservice project_language : dotnet - project_name : checkoutservice project_language : golang - project_name : currencyservice project_language : node - project_name : emailservice project_language : python - project_name : frontend project_language : golang - project_name : paymentservice project_language : node - project_name : productcatalogservice project_language : golang - project_name : recommendationservice project_language : python - project_name : shippingservice project_language : golang steps : - name : Checkout uses : actions/checkout@v3 with : ref : ${{ github.event.pull_request.head.ref }} repository : ${{ github.event.pull_request.head.repo.full_name }} - name : Install Snyk uses : snyk/actions/setup@master - name : Setup dotnet if : ${{ matrix.project_language == 'dotnet' }} uses : actions/setup-dotnet@v3 with : dotnet-version : '6.0' - name : Restore dotnet project dependencies if : ${{ matrix.project_language == 'dotnet' }} run : dotnet restore working-directory : \"src/${{ matrix.project_name }}\" - name : Set up Python env if : ${{ matrix.project_language == 'python' }} uses : actions/setup-python@v3 with : python-version : \"3.7\" - name : Install Python project dependencies if : ${{ matrix.project_language == 'python' }} run : pip install -r requirements.txt working-directory : \"src/${{ matrix.project_name }}\" - name : Check application source code for vulnerabilities run : | # Cartservice is an exception regarding project layout # It uses a nested src subfolder - `src/cartservice/src` if [[ \"${{ matrix.project_name }}\" == \"cartservice\" ]]; then cd src/ fi snyk code test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : \"src/${{ matrix.project_name }}\" - name : Check 3rd party source code for vulnerabilities run : | # Cartservice is an exception regarding project layout # It uses a nested src subfolder - `src/cartservice/src` if [[ \"${{ matrix.project_name }}\" == \"cartservice\" ]]; then cd src/ fi snyk test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --project-name=${{ env.PROJECT_NAME }} \\ --target-reference=${{ matrix.project_name }} env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : \"src/${{ matrix.project_name }}\" Save the workflow file, commit, and push changes to your GitHub repository. Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" Explanations for the Online Boutique Snyk Source Code Scan workflow configuration: on.pull_request - triggers the Online Boutique Snyk Source Code Scan workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique Snyk Source Code Scan workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths - list of repository paths used for filtering. The Online Boutique Snyk Source Code Scan workflow is triggered whenever a change happens for the specified paths. It has to trigger whenever a change happens in the src folder and below (hence the src/** expression), where all microservices source code is stored. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. strategy.matrix - use a matrix build type. Snyk scans run in parallel for each microservices. This approach is a perfect match for projects using multiple components, such as microservices. It also cuts down the time required to test each component for vulnerabilities. Each element from the matrix sets the project name, and the language being used. steps - list of steps to execute as part of the workflow jobs. For each project component (or microservice), the following list of actions is executed: Code checkout, via actions/checkout@v3 . Specific tools are installed based on project language ( actions/setup-dotnet@v2 , actions/setup-python@v3 , etc). Snyk requires extra steps for Python and DotNET projects such as installing required dependencies before starting the actual scan. Application source code and 3rd party dependencies are scanned via snyk code test and snyk test commands. The severity threshold is picked either from the designated workflow environment variable, or from the designated input field if it is manually triggered. The manual trigger option is left as an alternative for testing the workflow - it is not triggered under normal circumstances. Note The Online Boutique Snyk Source Code Scan GitHub workflow matrix strategy has the fail-fast flag set to false . This is desired in this case because you will want to scan all microservices and wait for the final results to fix all issues, if any. Following diagram shows a high level overview of Online Boutique Snyk Source Code Scan workflow composition: graph LR A(Source Code Scan Workflow <br> Start) --> B(Snyk Scan Matrix) B --> C(Cart Service <br> Source Code Scan) B --> D(Checkout Service <br> Source Code Scan) B --> E(Currency Service <br> Source Code Scan) B --> F(Email Service <br> Source Code Scan) B --> G(Frontend Source Code Scan) B --> H(Payment Service <br> Source Code Scan) B --> I(Product Catalog Service <br> Source Code Scan) B --> J(Recommendation Service <br> Source Code Scan) B --> K(Shipping Service <br> Source Code Scan) C & D & E & F & G & H & I & J & K --> L(Source Code Scan Workflow <br> End) Next, you will configure the Snyk GitHub workflow that automatically triggers whenever a change is detected for Kustomize manifests, aka trigger IAC scans. IAC Manifests Scanning Besides application source code you will also have configuration data present in your repository if using a monorepo structure. Configuration data usually represents Kubernetes manifests, Terraform HCL files, etc. Adoption journey guide deals with Kustomize manifests defining Kubernetes manifests for each environment where the online boutique application runs. Kustomize and associated Kubernetes manifests are part of the supply chain as well hence it makes sense to create a dedicated workflow to keep an eye on this part. Argo CD generated manifests from the CD chapter is also part of the supply chain so it's best practice to include it as well in the automated scan process (already done via the GitHub workflow presented below). Follow below steps to configure the Snyk workflow used for IAC manifests scanning (Kustomize + Argo CD): Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-snyk-iac-scan.yaml workflow file from the kubernetes-sample-apps repository: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-snyk-iac-scan.yaml \\ -o .github/workflows/online-boutique-snyk-iac-scan.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-snyk-iac-scan.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-snyk-iac-scan.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Snyk IAC Scan workflow file name : Online Boutique Snyk IAC Scan on : pull_request : branches : - main paths : - \"argocd/**\" - \"kustomize/**\" # Below configuration is used for manual workflow dispatch workflow_dispatch : inputs : snyk_fail_threshold : description : | Sets fail threshold for Snyk (low | medium | high | critical) required : true default : \"high\" env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" jobs : iac-security-check : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v3 with : ref : ${{ github.event.pull_request.head.ref }} repository : ${{ github.event.pull_request.head.repo.full_name }} - name : Install Snyk uses : snyk/actions/setup@master - name : Check for Kubernetes manifests vulnerabilities run : | snyk iac test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --target-name=${{ env.PROJECT_NAME }} \\ --target-reference=\"kustomize-PR#${{ github.event.pull_request.number }}\" \\ --report env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : kustomize - name : Check for Argo CD manifests vulnerabilities run : | snyk iac test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --target-name=${{ env.PROJECT_NAME }} \\ --target-reference=\"argocd-PR#${{ github.event.pull_request.number }}\" \\ --report env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : argocd Save the workflow file, commit, and push changes to your GitHub repository. Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" Explanations for the Online Boutique Snyk IAC Scan workflow configuration: on.pull_request - triggers the Online Boutique Snyk IAC Scan workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique Snyk IAC Scan workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths - list of repository paths used for filtering. The Online Boutique Snyk IAC Scan workflow is triggered whenever a change happens for the specified paths. It has to trigger whenever a change happens in the kustomize folder and below (hence the kustomize/** expression), or for argocd folder in the same manner. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. steps - list of steps to execute as part of the workflow jobs. The following list of actions is executed: Code checkout, via actions/checkout@v3 . Kustomize and Argo CD source manifests are scanned via snyk iac test command. The severity threshold is picked either from the designated workflow environment variable, or from the designated input field if it is manually triggered. The manual trigger option is left as an alternative for testing the workflow - it is not triggered under normal circumstances. You also have the possibility to upload scan result reports to the Snyk web portal via the --report flag, but it is not mandatory. Above workkflow has this feature enabled because it helps to identify issues better. Please bear in mind that this feature is available only for IAC scans at this time of writing. Following diagram shows a high level overview of Online Boutique Snyk IAC Scan workflow composition: graph LR A(Source IAC Scan Workflow <br> Start) --> B(Kustomize Manifests Scan) B --> C(Argo CD Manifests Scan) C --> L(Source Code Scan Workflow <br> End) Next, you will configure the Snyk GitHub workflow that deals with scanning and detecting vulnerabilities for application Docker containers. Docker Containers Scanning The final piece from the supply chain security covered in this guide is related to application containers scanning. Docker is the most popular solution used today so it makes sense to cover this part as well. Follow below steps to configure the Snyk workflow used for application Docker containers scanning (including Dockerfiles ): Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-snyk-docker-scan.yaml workflow file from the kubernetes-sample-apps repository: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-snyk-docker-scan.yaml \\ -o .github/workflows/online-boutique-snyk-docker-scan.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-snyk-docker-scan.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-snyk-docker-scan.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Snyk Docker Scan workflow file name : Online Boutique Snyk Docker Scan on : pull_request : branches : - main paths : - \"src/**/Dockerfile\" # Below configuration is used for manual workflow dispatch workflow_dispatch : inputs : snyk_fail_threshold : description : | Sets fail threshold for Snyk (low | medium | high | critical) required : true default : \"high\" env : DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" jobs : container-security-check : runs-on : ubuntu-latest strategy : fail-fast : false matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout uses : actions/checkout@v3 with : ref : ${{ github.event.pull_request.head.ref }} repository : ${{ github.event.pull_request.head.repo.full_name }} - name : Install Snyk uses : snyk/actions/setup@master - name : Build app image for Snyk container scanning uses : docker/build-push-action@v3 with : # Cartservice is an exception - Dockerfile is placed in `src/cartservice/src` subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : false tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.event.pull_request.head.sha }}\" - name : Check application container vulnerabilities run : | # Cartservice is an exception regarding project layout # It uses a nested src subfolder - `src/cartservice/src` if [[ \"${{ matrix.project }}\" == \"cartservice\" ]]; then cd src/ fi snyk container test \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.event.pull_request.head.sha }}\" \\ --file=Dockerfile \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --target-name=${{ env.PROJECT_NAME }} \\ --target-reference=${{ matrix.project }} env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : \"src/${{ matrix.project }}\" Save the workflow file, commit, and push changes to your GitHub repository. Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" Explanations for the Online Boutique Snyk Docker Scan workflow configuration: on.pull_request - triggers the Online Boutique Snyk Docker Scan workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique Snyk Docker Scan workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths - list of repository paths used for filtering. The Online Boutique Snyk Docker Scan workflow is triggered whenever a change happens for the specified paths. It has to trigger whenever a change happens in the Dockerfile of each microservice from the src folder (hence the src/**/Dockerfile expression). env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. steps - list of steps to execute as part of the workflow jobs. The following list of actions is executed: Code checkout, via actions/checkout@v3 . Each microservice container is built and scanned used snyk container test command. The container scan process uses each microservice Dockerfile as a reference. The severity threshold is picked either from the designated workflow environment variable, or from the designated input field if it is manually triggered. The manual trigger option is left as an alternative for testing the workflow - it is not triggered under normal circumstances. Following diagram shows a high level overview of Online Boutique Snyk Docker Scan workflow composition: graph LR A(Container Scan Workflow <br> Start) --> B(Snyk Scan Matrix) B --> C(Cart Service <br> Container Scan) B --> D(Checkout Service <br> Container Scan) B --> E(Currency Service <br> Container Scan) B --> F(Email Service <br> Container Scan) B --> G(Frontend Container Scan) B --> H(Payment Service <br> Container Scan) B --> I(Product Catalog Service <br> Container Scan) B --> J(Recommendation Service <br> Container Scan) B --> K(Shipping Service <br> Container Scan) C & D & E & F & G & H & I & J & K --> L(Container Scan Workflow <br> End) Next, you will discover how to benefit from the Snyk workflows created so far, and then how to investigate and fix some of the reported security issues. Testing the Final Setup To test each Snyk workflow a few PRs are required first, each dealing with a specific area of interest. As seen in previous sections, each Snyk workflow is responsible with scanning a specific area of the Kubernetes supply chain: Application source code and 3rd party artifacts (such as external libraries). Kubernetes manifests (Kustomize and Argo CD). Docker containers and associated Dockerfiles. Next, each area of interest is touched briefly with the accompanying examples and explanations. Inspecting and Fixing Application Source Code Vulnerabilities Testing the Snyk workflow responsible with detecting application source code vulnerabilities implies the following: Create a PR addressing source code changes for one (or multiple) application components. For example, change some of the header of footer HTML templates from the frontend component. Verify PR checks window - should show both main PR and Snyk application source code scan workflows running in parallel: Watch closely the Snyk application source code scan workflow as it progresses: After the Snyk application source code scan workflow finishes you should see if and how each microservice is affected. Below example shows the list of issues reported for the frontend microservice: You will receive some explanations and hints regarding the line of code that imposes a security risk, as well as why it is considered an issue and a possible fix (if it is known). Next, it is the responsibility of the developer to consult with the team and management, and decide if the highlighted issue o(r set of issues) is really a threat. There are situations when it's safe to ignore some of the reported issues and add them to the exceptions list. After a decision was made, you either mark the issue(s) as an exception, or proceed and create a set of fixes for the current PR. Push changes, and if everything goes well the workflow checks should pass. Tip To avoid many iterations and thus reducing wait time for each PR, you should benefit from the IDE integration that Snyk offers. This is the first and most important best practice explained in this chapter. Inspecting and Fixing IAC Vulnerabilities The process of inspecting and fixing IAC manifests is similar to previous section except that in this case only Kubernetes manifests are scanned. Whenever you open a PR containing changes for the kustomize and/or argocd subfolder, the Snyk IAC scan workflow is automatically triggered. You also get a report in the Snyk web portal that looks like below (after navigating to the projects page): In the above example only a low severity issue is reported. Nevertheless, after expanding kustomize-PR#<YOUR_PR_NUMBER> reports, click on the base/redis.yaml manifest. You should see the following low severity issue reported: Inspecting and Fixing Docker Containers Vulnerabilities Whenever a PR is created containing changes for each microservice Dockerfile, then the associated Online Boutique Snyk Docker Scan workflow is triggered. Below is a sample screenshot about how it looks in practice: By clicking on one of the scans, you will get additional details about discovered issues: The process of addressing Docker issues goes the same way as with the previously discussed ones. Configuring Snyk for Docker Registry Scanning Snyk also offers support for various integrations such as DigitalOcean Container Registry, aka DOCR. Integrating your DOCR with Snyk offers the possibility to continuously monitor and get notified about newly disclosed vulnerabilities for each container you use to deploy your applications. This is different from continuously scanning and validating project artifacts on each pull request. The pull request is validated and it's approved at a specific point in time, but this doesn't mean the deployed artifacts are still safe in the near future. Follow below steps to enable this feature: Log in to your Snyk portal, and navigate to the Integrations menu from the left. Scroll down the page untill you see the Container registries section: Click on the DigitalOcean tile. In the next page, enter your DO API token, and hit save. Finally, select your project images you want to monitor continuously via Snyk: After clicking the Add Selected Images button at the top you should be ready to go. From now on you will receive periodic emails informing you about newly disclosed vulnerabilities for the selected application containers. Continuous Monitoring of Applications Just as with Docker, Snyk offers another feature for continuously monitoring your applications before each deployment. The Snyk CLI offers this feature via the snyk monitor command. It uploads a snapshot of your app to the Snyk portal. Then, whenever a new vulnerability is disclosed, you will get notified via email. This approach has a major benefit over existing PR checks because new vulnerabilities are disclosed periodically, so you will be always up to date with your project. You will find more information about this feature here . Configuring Slack Notifications You can set up Snyk to send Slack alerts about new vulnerabilities discovered in your projects, and about new upgrades or patches that have become available. To set it up, you will need to generate a Slack webhook. You can either do this via Incoming WebHooks or by creating your own Slack App . Once you have generated your Slack Webhook URL, go to your Manage organization settings, enter the URL, and click the Connect button: Additional Best Practices and Resources Although the workflows presented in this guide should give you a good starting point, they may be not so perfect. For example, when working on microservices based projects, each developer works on microservice or on a set of microservices. The workflows presented in this chapter don't have builtin logic to deal with scanning only microservices that actually change between each iteration for the main project. The Snyk workflows presented here trigger and scan all the microservices at once each time. If the company or the team behind the main project decides to allocate a developer per microservice then it makes sense to isolate and trigger each Snyk scan per microservice based on which one has changed. There are two possible ways of achieving this: Leave the Snyk application source code scan workflow as is and use a GitHub action that detects which files changed from the src folder. Then, for each job in the matrix representing each microservice, add a condition to skip it or not based on the set of files that changed. Split the Snyk workflow in multiple workflows for each microservice. Then, filter using the paths field from the on.pull_request event trigger. Learn more by reading the following additional resources: DigitalOcean Supply Chain Security Guides Kubernetes Security Best Practices Article from Snyk More about Snyk Security Levels Vulnerability Assessment Snyk Targets and Projects Snyk for IDEs Discover more Snyk Integrations Snyk Portal Users and Group Management Fixing and Prioritizing Issues Reported by Snyk Snyk Github Action Used in this Guide","title":"Securing the software supply chain"},{"location":"07-security/software-supply-chain/#introduction","text":"Securing the Kubernetes software supply chain is a challenging task because of the multiple layers and components involved. In the end, it doesn't have to be so. By following a set of best practices and recommendations the process gets easier to implement and manage afterwards. Below is a set of best practices that will help you achieve a more secure supply chain: Early adoption of security scanning tools right within your IDE. Usually, you get this kind of support via extensions (or plugins) that can be installed right in your preferred IDE. It's best to adopt security best practices in the early stages of development thus preventing breaches at a later time when costs get really high. Use security gates with CI pipelines. Basically, you enable security tools to perform static code analysis for each pull request targeting your GitHub repository. Container registry periodic scans. New vulnerabilities are reported each day so you need to make sure that your registry is scanned on a regular basis and get notified in due time. Even though project images are automatically scanned in the CI process and pass all security checks at that point in time, it doesn't mean they are still safe today. What's important to understand is that any external dependency you source in your project is subject to new threats. Docker base images, 3rd party libraries, etc you fetch and use in your projects add to the list of vulnerabilities. Post-deployment and Kubernetes cluster periodic scans. Same idea as with registry scans. You already deployed application workloads to your cluster but because new vulnerabilities are discovered each day, yet again your system becomes sensible to attacks pretty soon. Enabling continuous scans for Kubernetes workloads (Pods, Deployments, etc), worker nodes, etc, decreases the overall risk. Kubernetes is a multi-tenant system - what if someone bypasses all security checks presented earlier? This is another reason why continuous scanning of your Kubernetes cluster is important. Because of the multitude of vulnerabilities found each day, this process should be approached systematically. On the other hand, each team or organization decides the level of acceptance considered to be safe for each project internally. Usually, you take care of vulnerabilities reported as high/critical first. Later on, at a fixed interval or even earlier depending on the severity level of newly disclosed vulnerabilities you repeat the process, and so on. There are several tools available to help you along the way two of the most valuable today being Kubescape and Snyk . In this guide, Snyk is picked because it provides more options and it's more flexible. Either tool you choose the basic idea is the same. Why Snyk? Snyk provides very good support for the following: IDE integration (e.g. Visual Studio Code ). User application code base scanning. IAC manifests scanning (Kubernetes, Terraform, etc). Third party libraries and open source projects scanning. Application Docker images scanning. Docker registry integration for periodic scanning (including private registries such as DOCR ). CI pipelines integration via the CLI binary, or GitHub Actions . GitHub repository integration for continuous scans of your application repository. Dedicated web portal to investigate reported issues and take appropriate actions to remediate the situation. All of the above is available in the free version. For small startups and business the free tier should be more than enough. For an in-depth explanations and comparison, please visit DigitalOcean's tutorials dedicated to both Kubescape and Snyk . Also, the official documentation for each tool is very valuable and provides more insight.","title":"Introduction"},{"location":"07-security/software-supply-chain/#prerequisites","text":"To complete this section you will need: A free Snyk cloud account account used to periodically publish scan results for your Kubernetes cluster to a nice dashboard. Also, the Snyk web interface helps you with investigations and risk analysis. Please follow How to Create a Snyk Account documentation page. The microservices-demo GitHub repository already prepared as explained in the Preparing demo application GitHub repository section. A GitHub secret named SNYK_TOKEN which holds your Snyk user account ID. Run snyk config get api in a terminal to get your ID. If that doesn't work, you should be able to retrieve the token from your user account settings page. The Snyk Code option enabled in your Snyk Org Setting. A Slack workspace you own, and a dedicated Slack app to get notified of vulnerability scan issues reported by Snyk.","title":"Prerequisites"},{"location":"07-security/software-supply-chain/#configuring-snyk-for-ides","text":"It's very important to discover security issues right from the early stages of development. Integrating vulnerabilities scanning tool with your preferred IDE is a good start. Doing so, you will get security reports right in your IDE for each project you're working on. Fixing security issues in the development stage avoids propagation and later problems in production systems.","title":"Configuring Snyk for IDEs"},{"location":"07-security/software-supply-chain/#installing-snyk-ide-extension","text":"This guide is focusing on Visual Studio Code integration because it is the most popular IDE nowadays. Nevertheless, Snyk offers support for other IDEs as well, such as: Eclipse . JetBrains . Visual Studio . Generic integration via language server protocol for any IDE that supports it. Follow below steps to integrate Snyk with your existing Visual Studio Code installation: Launch Visual Studio Code, and navigate to the extensions tab: Type snyk in the search box. Matched extensions should be present: Install Snyk extension from the list: After completing above steps, a Snyk button should appear in the left menu bar: From there you should connect your free Snyk account with Visual Studio Code and benefit from the features offered by the extension. Next, you will discover how easy it is to scan an existing project and get detailed information about reported vulnerabilities.","title":"Installing Snyk IDE Extension"},{"location":"07-security/software-supply-chain/#scanning-online-boutique-demo-application","text":"The Snyk extension is able to scan your local projects and report found issues right within your IDE of your choice. This guide is relying on Visual Studio Code but you should get similar experience with other supported IDEs as well. To test Snyk extension features inside Visual Studio Code, launch the IDE and open your microservices-demo project (should be already cloned on your local machine). Then, follow below steps: Navigate to the Snyk extension menu by clicking the icon from the left menu bar with a dog picture on it. You will be presented with a message asking you to enable Snyk code scanning. This needs to be performed only once for the current project: Next, you will be redirected to your Snyk account settings. A new web browser window is opened. Tick the checkbox, and save changes: Go back to your VS Code instance - you should see Snyk scanning your project for vulnerabilities. When it finishes, the following report should be presented to you: If everything looks like above then you have successfully scanned your first project using Snyk right from your IDE. Next, click on each reported issue to see the root cause and get additional details: For each reported issue you should get the severity level printed, and additional explanations why it's considered an issue alongside with a possible fix, if it's known. Severity levels are denoted with an uppercase letter, such as H for high severity, M for medium severity, and so on. Please visit the official documentation to learn more about the Snyk IDE extension and all available features.","title":"Scanning Online Boutique Demo Application"},{"location":"07-security/software-supply-chain/#configuring-snyk-for-ci-pipelines","text":"Snyk offers very good support in terms of CI pipelines automation via the dedicated CLI . GitHub pipelines support is also available via Snyk actions . Before going into the implementation steps it's important to understand how Snyk is architected and used in practice. Without going too much into the inner details, you should know the following: Snyk is able to scan your application code for vulnerabilities. This feature is provided by the Snyk Code component. Snyk is able to scan open source projects and 3rd party dependencies . This feature is provided by the Snyk Open Source component. Snyk knows how to scan Infrastructure as Code (IAC) manifests such as Kubernetes , Terraform , etc. This feature is provided by the Snyk Infrastructure as Code component. Snyk knows to scan containers (e.g. Docker), as well as Dockerfiles . This feature is provided by the Snyk Container component. Each feature from the above list is available via the free CLI tool offered by Snyk. This guide is focused more on the CLI integration with GitHub pipelines because of the scripting flexibility that it offers. You already configured a CI pipeline for your project in the CI chapter available in the Kubernetes adoption journey guide. Whenever a pull request is opened for your repository, a CI GitHub workflow is automatically triggered for validation. You will learn how to integrate Snyk with existing CI workflows to benefit from all features offered by the scanning tool. On each scan, the Snyk CLI will generate a report that you can visualize using the web portal offered for free by Snyk. The tool is also able to generate SARIF reports hence the final results will be available right in the Security tab of your GitHub repository. Other supported formats are JSON and HTML . Note Only the IAC manifests scanner is able to upload results to the Snyk web portal at this time of writing. Instead of embedding Snyk logic directly into CI pipelines, you will configure three separate Snyk workflows running in parallel with existing ones. Each Snyk workflow deals with a specific task such as scanning and detecting vulnerabilities for application source code, IAC manifests, and Docker containers. In the end, the net effect is the same - Snyk acts as a security gate, thus validating or invalidating a pull request based on the scan results. This approach has the following benefits: Snyk workflows run in parallel with other flows, such as pull requests CI workflows . If you need to fix something that is Snyk related, only the specific workflow is triggered and not the whole PR flow. Waiting time is reduced - some workflows take more time to finish such as PR workflows which runs automated tests. By decoupling Snyk logic, you avoid adding more complexity to the pull requests workflow thus easing maintenance. Next, you will learn how to configure each Snyk workflow in turn to scan application source code with 3rd party dependencies, IAC manifests and Docker containers.","title":"Configuring Snyk for CI Pipelines"},{"location":"07-security/software-supply-chain/#application-source-code-scanning","text":"Snyk uses various heuristics under the hood to perform static analysis of your application source code, as well as 3rd party dependencies. It creates a snapshot for your application source code current state, and runs the analysis. When finished you'll get the scan results printed to standard output in your terminal of choice. Snyk is also able to export scan results in other formats such as GitHub SARIF , JSON , etc. Each reported issue has a severity level associated as well such as critical , high , medium , and low . Usually you will take into account issues marked as critical and high first. Also, some issues may not affect your application even though they're reported. In such case, you have the option to exclude or ignore issues . How do you pick the severity threshold? Snyk marks your pipeline as failed if the found issues have a severity level equal to or higher than the threshold you set. As an example, if you set the threshold to high , then Snyk takes into account all issues reported as high and critical . If the threshold is set to medium , then it will take into account all issues reported as medium , high and critical . In other words, if you lower the bar it gets more and more strict. Follow below steps to configure the Snyk workflow used for application source code scanning: Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-snyk-source-code-scan.yaml workflow file from the kubernetes-sample-apps repository: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-snyk-source-code-scan.yaml \\ -o .github/workflows/online-boutique-snyk-source-code-scan.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-snyk-source-code-scan.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-snyk-source-code-scan.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Snyk Source Code Scan workflow file name : Online Boutique Snyk Source Code Scan on : pull_request : branches : - main paths : - \"src/**\" # Below configuration is used for manual workflow dispatch workflow_dispatch : inputs : snyk_fail_threshold : description : | Sets fail threshold for Snyk (low | medium | high | critical) required : true default : \"high\" env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" jobs : source-code-security-check : runs-on : ubuntu-latest strategy : fail-fast : false matrix : include : - project_name : cartservice project_language : dotnet - project_name : checkoutservice project_language : golang - project_name : currencyservice project_language : node - project_name : emailservice project_language : python - project_name : frontend project_language : golang - project_name : paymentservice project_language : node - project_name : productcatalogservice project_language : golang - project_name : recommendationservice project_language : python - project_name : shippingservice project_language : golang steps : - name : Checkout uses : actions/checkout@v3 with : ref : ${{ github.event.pull_request.head.ref }} repository : ${{ github.event.pull_request.head.repo.full_name }} - name : Install Snyk uses : snyk/actions/setup@master - name : Setup dotnet if : ${{ matrix.project_language == 'dotnet' }} uses : actions/setup-dotnet@v3 with : dotnet-version : '6.0' - name : Restore dotnet project dependencies if : ${{ matrix.project_language == 'dotnet' }} run : dotnet restore working-directory : \"src/${{ matrix.project_name }}\" - name : Set up Python env if : ${{ matrix.project_language == 'python' }} uses : actions/setup-python@v3 with : python-version : \"3.7\" - name : Install Python project dependencies if : ${{ matrix.project_language == 'python' }} run : pip install -r requirements.txt working-directory : \"src/${{ matrix.project_name }}\" - name : Check application source code for vulnerabilities run : | # Cartservice is an exception regarding project layout # It uses a nested src subfolder - `src/cartservice/src` if [[ \"${{ matrix.project_name }}\" == \"cartservice\" ]]; then cd src/ fi snyk code test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : \"src/${{ matrix.project_name }}\" - name : Check 3rd party source code for vulnerabilities run : | # Cartservice is an exception regarding project layout # It uses a nested src subfolder - `src/cartservice/src` if [[ \"${{ matrix.project_name }}\" == \"cartservice\" ]]; then cd src/ fi snyk test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --project-name=${{ env.PROJECT_NAME }} \\ --target-reference=${{ matrix.project_name }} env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : \"src/${{ matrix.project_name }}\" Save the workflow file, commit, and push changes to your GitHub repository. Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" Explanations for the Online Boutique Snyk Source Code Scan workflow configuration: on.pull_request - triggers the Online Boutique Snyk Source Code Scan workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique Snyk Source Code Scan workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths - list of repository paths used for filtering. The Online Boutique Snyk Source Code Scan workflow is triggered whenever a change happens for the specified paths. It has to trigger whenever a change happens in the src folder and below (hence the src/** expression), where all microservices source code is stored. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. strategy.matrix - use a matrix build type. Snyk scans run in parallel for each microservices. This approach is a perfect match for projects using multiple components, such as microservices. It also cuts down the time required to test each component for vulnerabilities. Each element from the matrix sets the project name, and the language being used. steps - list of steps to execute as part of the workflow jobs. For each project component (or microservice), the following list of actions is executed: Code checkout, via actions/checkout@v3 . Specific tools are installed based on project language ( actions/setup-dotnet@v2 , actions/setup-python@v3 , etc). Snyk requires extra steps for Python and DotNET projects such as installing required dependencies before starting the actual scan. Application source code and 3rd party dependencies are scanned via snyk code test and snyk test commands. The severity threshold is picked either from the designated workflow environment variable, or from the designated input field if it is manually triggered. The manual trigger option is left as an alternative for testing the workflow - it is not triggered under normal circumstances. Note The Online Boutique Snyk Source Code Scan GitHub workflow matrix strategy has the fail-fast flag set to false . This is desired in this case because you will want to scan all microservices and wait for the final results to fix all issues, if any. Following diagram shows a high level overview of Online Boutique Snyk Source Code Scan workflow composition: graph LR A(Source Code Scan Workflow <br> Start) --> B(Snyk Scan Matrix) B --> C(Cart Service <br> Source Code Scan) B --> D(Checkout Service <br> Source Code Scan) B --> E(Currency Service <br> Source Code Scan) B --> F(Email Service <br> Source Code Scan) B --> G(Frontend Source Code Scan) B --> H(Payment Service <br> Source Code Scan) B --> I(Product Catalog Service <br> Source Code Scan) B --> J(Recommendation Service <br> Source Code Scan) B --> K(Shipping Service <br> Source Code Scan) C & D & E & F & G & H & I & J & K --> L(Source Code Scan Workflow <br> End) Next, you will configure the Snyk GitHub workflow that automatically triggers whenever a change is detected for Kustomize manifests, aka trigger IAC scans.","title":"Application Source Code Scanning"},{"location":"07-security/software-supply-chain/#iac-manifests-scanning","text":"Besides application source code you will also have configuration data present in your repository if using a monorepo structure. Configuration data usually represents Kubernetes manifests, Terraform HCL files, etc. Adoption journey guide deals with Kustomize manifests defining Kubernetes manifests for each environment where the online boutique application runs. Kustomize and associated Kubernetes manifests are part of the supply chain as well hence it makes sense to create a dedicated workflow to keep an eye on this part. Argo CD generated manifests from the CD chapter is also part of the supply chain so it's best practice to include it as well in the automated scan process (already done via the GitHub workflow presented below). Follow below steps to configure the Snyk workflow used for IAC manifests scanning (Kustomize + Argo CD): Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-snyk-iac-scan.yaml workflow file from the kubernetes-sample-apps repository: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-snyk-iac-scan.yaml \\ -o .github/workflows/online-boutique-snyk-iac-scan.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-snyk-iac-scan.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-snyk-iac-scan.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Snyk IAC Scan workflow file name : Online Boutique Snyk IAC Scan on : pull_request : branches : - main paths : - \"argocd/**\" - \"kustomize/**\" # Below configuration is used for manual workflow dispatch workflow_dispatch : inputs : snyk_fail_threshold : description : | Sets fail threshold for Snyk (low | medium | high | critical) required : true default : \"high\" env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" jobs : iac-security-check : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v3 with : ref : ${{ github.event.pull_request.head.ref }} repository : ${{ github.event.pull_request.head.repo.full_name }} - name : Install Snyk uses : snyk/actions/setup@master - name : Check for Kubernetes manifests vulnerabilities run : | snyk iac test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --target-name=${{ env.PROJECT_NAME }} \\ --target-reference=\"kustomize-PR#${{ github.event.pull_request.number }}\" \\ --report env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : kustomize - name : Check for Argo CD manifests vulnerabilities run : | snyk iac test \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --target-name=${{ env.PROJECT_NAME }} \\ --target-reference=\"argocd-PR#${{ github.event.pull_request.number }}\" \\ --report env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : argocd Save the workflow file, commit, and push changes to your GitHub repository. Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" Explanations for the Online Boutique Snyk IAC Scan workflow configuration: on.pull_request - triggers the Online Boutique Snyk IAC Scan workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique Snyk IAC Scan workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths - list of repository paths used for filtering. The Online Boutique Snyk IAC Scan workflow is triggered whenever a change happens for the specified paths. It has to trigger whenever a change happens in the kustomize folder and below (hence the kustomize/** expression), or for argocd folder in the same manner. env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. steps - list of steps to execute as part of the workflow jobs. The following list of actions is executed: Code checkout, via actions/checkout@v3 . Kustomize and Argo CD source manifests are scanned via snyk iac test command. The severity threshold is picked either from the designated workflow environment variable, or from the designated input field if it is manually triggered. The manual trigger option is left as an alternative for testing the workflow - it is not triggered under normal circumstances. You also have the possibility to upload scan result reports to the Snyk web portal via the --report flag, but it is not mandatory. Above workkflow has this feature enabled because it helps to identify issues better. Please bear in mind that this feature is available only for IAC scans at this time of writing. Following diagram shows a high level overview of Online Boutique Snyk IAC Scan workflow composition: graph LR A(Source IAC Scan Workflow <br> Start) --> B(Kustomize Manifests Scan) B --> C(Argo CD Manifests Scan) C --> L(Source Code Scan Workflow <br> End) Next, you will configure the Snyk GitHub workflow that deals with scanning and detecting vulnerabilities for application Docker containers.","title":"IAC Manifests Scanning"},{"location":"07-security/software-supply-chain/#docker-containers-scanning","text":"The final piece from the supply chain security covered in this guide is related to application containers scanning. Docker is the most popular solution used today so it makes sense to cover this part as well. Follow below steps to configure the Snyk workflow used for application Docker containers scanning (including Dockerfiles ): Clone the microservices-demo repository on your local machine, if not already (make sure to replace the <> placeholders accordingly): git clone https://github.com/<YOUR_GITHUB_ACCOUNT_USERNAME>/microservices-demo.git Change directory to your local copy: cd microservices-demo Fetch the online-boutique-snyk-docker-scan.yaml workflow file from the kubernetes-sample-apps repository: curl https://raw.githubusercontent.com/digitalocean/kubernetes-sample-apps/master/.github/workflows/online-boutique-snyk-docker-scan.yaml \\ -o .github/workflows/online-boutique-snyk-docker-scan.yaml \\ --create-dirs Edit the .github/workflows/online-boutique-snyk-docker-scan.yaml file using a text editor of your choice, preferably with YAML lint support. For example, you can use VS Code : code .github/workflows/online-boutique-snyk-docker-scan.yaml Uncomment the lines prefixed with a hash mark at the top of the workflow file. The resulting file should look like: Click to expand the Online Boutique Snyk Docker Scan workflow file name : Online Boutique Snyk Docker Scan on : pull_request : branches : - main paths : - \"src/**/Dockerfile\" # Below configuration is used for manual workflow dispatch workflow_dispatch : inputs : snyk_fail_threshold : description : | Sets fail threshold for Snyk (low | medium | high | critical) required : true default : \"high\" env : DOCR_ENDPOINT : \"registry.digitalocean.com/microservices-demo\" PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" jobs : container-security-check : runs-on : ubuntu-latest strategy : fail-fast : false matrix : project : - cartservice - checkoutservice - currencyservice - emailservice - frontend - paymentservice - productcatalogservice - recommendationservice - shippingservice steps : - name : Checkout uses : actions/checkout@v3 with : ref : ${{ github.event.pull_request.head.ref }} repository : ${{ github.event.pull_request.head.repo.full_name }} - name : Install Snyk uses : snyk/actions/setup@master - name : Build app image for Snyk container scanning uses : docker/build-push-action@v3 with : # Cartservice is an exception - Dockerfile is placed in `src/cartservice/src` subfolder context : \"src/${{ matrix.project }}/${{ matrix.project == 'cartservice' && 'src' || ''}}\" push : false tags : \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.event.pull_request.head.sha }}\" - name : Check application container vulnerabilities run : | # Cartservice is an exception regarding project layout # It uses a nested src subfolder - `src/cartservice/src` if [[ \"${{ matrix.project }}\" == \"cartservice\" ]]; then cd src/ fi snyk container test \"${{ env.DOCR_ENDPOINT }}/${{ matrix.project }}:${{ github.event.pull_request.head.sha }}\" \\ --file=Dockerfile \\ --severity-threshold=${{ github.event.inputs.snyk_fail_threshold || env.SNYK_FAIL_THRESHOLD }} \\ --target-name=${{ env.PROJECT_NAME }} \\ --target-reference=${{ matrix.project }} env : SNYK_TOKEN : ${{ secrets.SNYK_TOKEN }} working-directory : \"src/${{ matrix.project }}\" Save the workflow file, commit, and push changes to your GitHub repository. Note Depending on your setup, you may want to adjust the following environment variables at the top of your workflow file: env : DOCR_ENDPOINT : \"registry.digitalocean.com/<YOUR_REGISTRY_NAME_HERE>\" PROJECT_NAME : \"online-boutique\" SNYK_FAIL_THRESHOLD : \"high\" Explanations for the Online Boutique Snyk Docker Scan workflow configuration: on.pull_request - triggers the Online Boutique Snyk Docker Scan workflow on pull request events only. on.pull_request.branches - triggers the Online Boutique Snyk Docker Scan workflow whenever a pull request event is detected for the specified list of branches. In this case only main branch is desired. on.pull_requests.paths - list of repository paths used for filtering. The Online Boutique Snyk Docker Scan workflow is triggered whenever a change happens for the specified paths. It has to trigger whenever a change happens in the Dockerfile of each microservice from the src folder (hence the src/**/Dockerfile expression). env - sets environment variables to use for the whole pipeline. Usually, environment variables control workflow logic. steps - list of steps to execute as part of the workflow jobs. The following list of actions is executed: Code checkout, via actions/checkout@v3 . Each microservice container is built and scanned used snyk container test command. The container scan process uses each microservice Dockerfile as a reference. The severity threshold is picked either from the designated workflow environment variable, or from the designated input field if it is manually triggered. The manual trigger option is left as an alternative for testing the workflow - it is not triggered under normal circumstances. Following diagram shows a high level overview of Online Boutique Snyk Docker Scan workflow composition: graph LR A(Container Scan Workflow <br> Start) --> B(Snyk Scan Matrix) B --> C(Cart Service <br> Container Scan) B --> D(Checkout Service <br> Container Scan) B --> E(Currency Service <br> Container Scan) B --> F(Email Service <br> Container Scan) B --> G(Frontend Container Scan) B --> H(Payment Service <br> Container Scan) B --> I(Product Catalog Service <br> Container Scan) B --> J(Recommendation Service <br> Container Scan) B --> K(Shipping Service <br> Container Scan) C & D & E & F & G & H & I & J & K --> L(Container Scan Workflow <br> End) Next, you will discover how to benefit from the Snyk workflows created so far, and then how to investigate and fix some of the reported security issues.","title":"Docker Containers Scanning"},{"location":"07-security/software-supply-chain/#testing-the-final-setup","text":"To test each Snyk workflow a few PRs are required first, each dealing with a specific area of interest. As seen in previous sections, each Snyk workflow is responsible with scanning a specific area of the Kubernetes supply chain: Application source code and 3rd party artifacts (such as external libraries). Kubernetes manifests (Kustomize and Argo CD). Docker containers and associated Dockerfiles. Next, each area of interest is touched briefly with the accompanying examples and explanations.","title":"Testing the Final Setup"},{"location":"07-security/software-supply-chain/#inspecting-and-fixing-application-source-code-vulnerabilities","text":"Testing the Snyk workflow responsible with detecting application source code vulnerabilities implies the following: Create a PR addressing source code changes for one (or multiple) application components. For example, change some of the header of footer HTML templates from the frontend component. Verify PR checks window - should show both main PR and Snyk application source code scan workflows running in parallel: Watch closely the Snyk application source code scan workflow as it progresses: After the Snyk application source code scan workflow finishes you should see if and how each microservice is affected. Below example shows the list of issues reported for the frontend microservice: You will receive some explanations and hints regarding the line of code that imposes a security risk, as well as why it is considered an issue and a possible fix (if it is known). Next, it is the responsibility of the developer to consult with the team and management, and decide if the highlighted issue o(r set of issues) is really a threat. There are situations when it's safe to ignore some of the reported issues and add them to the exceptions list. After a decision was made, you either mark the issue(s) as an exception, or proceed and create a set of fixes for the current PR. Push changes, and if everything goes well the workflow checks should pass. Tip To avoid many iterations and thus reducing wait time for each PR, you should benefit from the IDE integration that Snyk offers. This is the first and most important best practice explained in this chapter.","title":"Inspecting and Fixing Application Source Code Vulnerabilities"},{"location":"07-security/software-supply-chain/#inspecting-and-fixing-iac-vulnerabilities","text":"The process of inspecting and fixing IAC manifests is similar to previous section except that in this case only Kubernetes manifests are scanned. Whenever you open a PR containing changes for the kustomize and/or argocd subfolder, the Snyk IAC scan workflow is automatically triggered. You also get a report in the Snyk web portal that looks like below (after navigating to the projects page): In the above example only a low severity issue is reported. Nevertheless, after expanding kustomize-PR#<YOUR_PR_NUMBER> reports, click on the base/redis.yaml manifest. You should see the following low severity issue reported:","title":"Inspecting and Fixing IAC Vulnerabilities"},{"location":"07-security/software-supply-chain/#inspecting-and-fixing-docker-containers-vulnerabilities","text":"Whenever a PR is created containing changes for each microservice Dockerfile, then the associated Online Boutique Snyk Docker Scan workflow is triggered. Below is a sample screenshot about how it looks in practice: By clicking on one of the scans, you will get additional details about discovered issues: The process of addressing Docker issues goes the same way as with the previously discussed ones.","title":"Inspecting and Fixing Docker Containers Vulnerabilities"},{"location":"07-security/software-supply-chain/#configuring-snyk-for-docker-registry-scanning","text":"Snyk also offers support for various integrations such as DigitalOcean Container Registry, aka DOCR. Integrating your DOCR with Snyk offers the possibility to continuously monitor and get notified about newly disclosed vulnerabilities for each container you use to deploy your applications. This is different from continuously scanning and validating project artifacts on each pull request. The pull request is validated and it's approved at a specific point in time, but this doesn't mean the deployed artifacts are still safe in the near future. Follow below steps to enable this feature: Log in to your Snyk portal, and navigate to the Integrations menu from the left. Scroll down the page untill you see the Container registries section: Click on the DigitalOcean tile. In the next page, enter your DO API token, and hit save. Finally, select your project images you want to monitor continuously via Snyk: After clicking the Add Selected Images button at the top you should be ready to go. From now on you will receive periodic emails informing you about newly disclosed vulnerabilities for the selected application containers.","title":"Configuring Snyk for Docker Registry Scanning"},{"location":"07-security/software-supply-chain/#continuous-monitoring-of-applications","text":"Just as with Docker, Snyk offers another feature for continuously monitoring your applications before each deployment. The Snyk CLI offers this feature via the snyk monitor command. It uploads a snapshot of your app to the Snyk portal. Then, whenever a new vulnerability is disclosed, you will get notified via email. This approach has a major benefit over existing PR checks because new vulnerabilities are disclosed periodically, so you will be always up to date with your project. You will find more information about this feature here .","title":"Continuous Monitoring of Applications"},{"location":"07-security/software-supply-chain/#configuring-slack-notifications","text":"You can set up Snyk to send Slack alerts about new vulnerabilities discovered in your projects, and about new upgrades or patches that have become available. To set it up, you will need to generate a Slack webhook. You can either do this via Incoming WebHooks or by creating your own Slack App . Once you have generated your Slack Webhook URL, go to your Manage organization settings, enter the URL, and click the Connect button:","title":"Configuring Slack Notifications"},{"location":"07-security/software-supply-chain/#additional-best-practices-and-resources","text":"Although the workflows presented in this guide should give you a good starting point, they may be not so perfect. For example, when working on microservices based projects, each developer works on microservice or on a set of microservices. The workflows presented in this chapter don't have builtin logic to deal with scanning only microservices that actually change between each iteration for the main project. The Snyk workflows presented here trigger and scan all the microservices at once each time. If the company or the team behind the main project decides to allocate a developer per microservice then it makes sense to isolate and trigger each Snyk scan per microservice based on which one has changed. There are two possible ways of achieving this: Leave the Snyk application source code scan workflow as is and use a GitHub action that detects which files changed from the src folder. Then, for each job in the matrix representing each microservice, add a condition to skip it or not based on the set of files that changed. Split the Snyk workflow in multiple workflows for each microservice. Then, filter using the paths field from the on.pull_request event trigger. Learn more by reading the following additional resources: DigitalOcean Supply Chain Security Guides Kubernetes Security Best Practices Article from Snyk More about Snyk Security Levels Vulnerability Assessment Snyk Targets and Projects Snyk for IDEs Discover more Snyk Integrations Snyk Portal Users and Group Management Fixing and Prioritizing Issues Reported by Snyk Snyk Github Action Used in this Guide","title":"Additional Best Practices and Resources"}]}